17/04/04 11:10:40 WARN SparkContext: Support for Java 7 is deprecated as of Spark 2.0.0
17/04/04 11:10:40 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time, about=, type=DEFAULT, always=false, sampleName=Ops)
17/04/04 11:10:40 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time, about=, type=DEFAULT, always=false, sampleName=Ops)
17/04/04 11:10:40 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(value=[GetGroups], valueName=Time, about=, type=DEFAULT, always=false, sampleName=Ops)
17/04/04 11:10:40 DEBUG MetricsSystemImpl: UgiMetrics, User and group related metrics
17/04/04 11:10:40 DEBUG Shell: setsid exited with exit code 0
17/04/04 11:10:40 DEBUG Groups:  Creating new Groups object
17/04/04 11:10:40 DEBUG NativeCodeLoader: Trying to load the custom-built native-hadoop library...
17/04/04 11:10:40 DEBUG NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
17/04/04 11:10:40 DEBUG NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
17/04/04 11:10:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/04/04 11:10:40 DEBUG PerformanceAdvisory: Falling back to shell based
17/04/04 11:10:40 DEBUG JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
17/04/04 11:10:40 DEBUG Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
17/04/04 11:10:40 DEBUG UserGroupInformation: hadoop login
17/04/04 11:10:40 DEBUG UserGroupInformation: hadoop login commit
17/04/04 11:10:40 DEBUG UserGroupInformation: using local user:UnixPrincipal: hadoop
17/04/04 11:10:40 DEBUG UserGroupInformation: Using user: "UnixPrincipal: hadoop" with name hadoop
17/04/04 11:10:40 DEBUG UserGroupInformation: User entry: "hadoop"
17/04/04 11:10:40 DEBUG UserGroupInformation: UGI loginUser:hadoop (auth:SIMPLE)
17/04/04 11:10:40 WARN SparkConf: Detected deprecated memory fraction settings: [spark.shuffle.memoryFraction, spark.storage.memoryFraction, spark.storage.unrollFraction]. As of Spark 1.6, execution and storage memory management are unified. All memory fractions used in the old model are now deprecated and no longer read. If you wish to use the old memory management, you may explicitly enable `spark.memory.useLegacyMode` (not recommended).
17/04/04 11:10:40 WARN SparkConf: 
SPARK_CLASSPATH was detected (set to '/home/hadoop/bryantchang/platforms/spark2.0/lib/mysql-connector-java-5.1.40-bin.jar:').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with --driver-class-path to augment the driver classpath
 - spark.executor.extraClassPath to augment the executor classpath
        
17/04/04 11:10:40 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/home/hadoop/bryantchang/platforms/spark2.0/lib/mysql-connector-java-5.1.40-bin.jar:' as a work-around.
17/04/04 11:10:40 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/home/hadoop/bryantchang/platforms/spark2.0/lib/mysql-connector-java-5.1.40-bin.jar:' as a work-around.
17/04/04 11:10:40 DEBUG InternalLoggerFactory: Using SLF4J as the default logging framework
17/04/04 11:10:40 DEBUG PlatformDependent0: java.nio.Buffer.address: available
17/04/04 11:10:40 DEBUG PlatformDependent0: sun.misc.Unsafe.theUnsafe: available
17/04/04 11:10:40 DEBUG PlatformDependent0: sun.misc.Unsafe.copyMemory: available
17/04/04 11:10:40 DEBUG PlatformDependent0: direct buffer constructor: available
17/04/04 11:10:40 DEBUG PlatformDependent0: java.nio.Bits.unaligned: available, true
17/04/04 11:10:40 DEBUG PlatformDependent0: java.nio.DirectByteBuffer.<init>(long, int): available
17/04/04 11:10:40 DEBUG Cleaner0: java.nio.ByteBuffer.cleaner(): available
17/04/04 11:10:40 DEBUG PlatformDependent: Java version: 7
17/04/04 11:10:40 DEBUG PlatformDependent: -Dio.netty.noUnsafe: false
17/04/04 11:10:40 DEBUG PlatformDependent: sun.misc.Unsafe: available
17/04/04 11:10:40 DEBUG PlatformDependent: -Dio.netty.noJavassist: false
17/04/04 11:10:40 DEBUG PlatformDependent: Javassist: available
17/04/04 11:10:40 DEBUG PlatformDependent: -Dio.netty.tmpdir: /tmp (java.io.tmpdir)
17/04/04 11:10:40 DEBUG PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)
17/04/04 11:10:40 DEBUG PlatformDependent: -Dio.netty.noPreferDirect: false
17/04/04 11:10:40 DEBUG PlatformDependent: io.netty.maxDirectMemory: 1342177280 bytes
17/04/04 11:10:40 DEBUG JavassistTypeParameterMatcherGenerator: Generated: io.netty.util.internal.__matchers__.org.apache.spark.network.protocol.MessageMatcher
17/04/04 11:10:40 DEBUG JavassistTypeParameterMatcherGenerator: Generated: io.netty.util.internal.__matchers__.io.netty.buffer.ByteBufMatcher
17/04/04 11:10:40 DEBUG MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 8
17/04/04 11:10:40 DEBUG NioEventLoop: -Dio.netty.noKeySetOptimization: false
17/04/04 11:10:40 DEBUG NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512
17/04/04 11:10:40 DEBUG PlatformDependent: org.jctools-core.MpscChunkedArrayQueue: available
17/04/04 11:10:40 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 8
17/04/04 11:10:40 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 8
17/04/04 11:10:40 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192
17/04/04 11:10:40 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 11
17/04/04 11:10:40 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 16777216
17/04/04 11:10:40 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.tinyCacheSize: 512
17/04/04 11:10:40 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256
17/04/04 11:10:40 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64
17/04/04 11:10:40 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768
17/04/04 11:10:40 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192
17/04/04 11:10:40 DEBUG ThreadLocalRandom: -Dio.netty.initialSeedUniquifier: 0x29373d4ae718bcc0 (took 0 ms)
17/04/04 11:10:40 DEBUG ByteBufUtil: -Dio.netty.allocator.type: unpooled
17/04/04 11:10:40 DEBUG ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 65536
17/04/04 11:10:40 DEBUG ByteBufUtil: -Dio.netty.maxThreadLocalCharBufferSize: 16384
17/04/04 11:10:40 DEBUG NetUtil: Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%1)
17/04/04 11:10:40 DEBUG NetUtil: /proc/sys/net/core/somaxconn: 128
17/04/04 11:10:41 DEBUG AbstractByteBuf: -Dio.netty.buffer.bytebuf.checkAccessible: true
17/04/04 11:10:41 DEBUG ResourceLeakDetector: -Dio.netty.leakDetection.level: simple
17/04/04 11:10:41 DEBUG ResourceLeakDetector: -Dio.netty.leakDetection.maxRecords: 4
17/04/04 11:10:41 DEBUG ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: io.netty.util.ResourceLeakDetector@171ac096
17/04/04 11:10:41 DEBUG Recycler: -Dio.netty.recycler.maxCapacity.default: 32768
17/04/04 11:10:41 DEBUG Recycler: -Dio.netty.recycler.maxSharedCapacityFactor: 2
17/04/04 11:10:41 DEBUG Recycler: -Dio.netty.recycler.linkCapacity: 16
17/04/04 11:10:41 DEBUG Recycler: -Dio.netty.recycler.ratio: 8
17/04/04 11:10:41 DEBUG BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false
17/04/04 11:10:41 DEBUG BlockReaderLocal: dfs.client.read.shortcircuit = false
17/04/04 11:10:41 DEBUG BlockReaderLocal: dfs.client.domain.socket.data.traffic = false
17/04/04 11:10:41 DEBUG BlockReaderLocal: dfs.domain.socket.path = 
17/04/04 11:10:41 DEBUG RetryUtils: multipleLinearRandomRetry = null
17/04/04 11:10:41 DEBUG Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcRequestWrapper, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@18762d21
17/04/04 11:10:41 DEBUG Client: getting client out of cache: org.apache.hadoop.ipc.Client@2209ad9a
17/04/04 11:10:41 DEBUG PerformanceAdvisory: Both short-circuit local reads and UNIX domain socket are disabled.
17/04/04 11:10:41 DEBUG DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
17/04/04 11:10:41 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:10:41 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:10:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:10:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #0
17/04/04 11:10:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #0
17/04/04 11:10:41 DEBUG ProtobufRpcEngine: Call: getFileInfo took 26ms
17/04/04 11:10:41 DEBUG DFSClient: /eventLogs/app-20170404111041-0045.lz4.inprogress: masked=rw-r--r--
17/04/04 11:10:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #1
17/04/04 11:10:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #1
17/04/04 11:10:41 DEBUG ProtobufRpcEngine: Call: create took 24ms
17/04/04 11:10:41 DEBUG DFSClient: computePacketChunkSize: src=/eventLogs/app-20170404111041-0045.lz4.inprogress, chunkSize=516, chunksPerPacket=126, packetSize=65016
17/04/04 11:10:41 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 started
17/04/04 11:10:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #2
17/04/04 11:10:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #2
17/04/04 11:10:41 DEBUG ProtobufRpcEngine: Call: setPermission took 3ms
17/04/04 11:10:41 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=0 lastFlushOffset=0 createNewBlock=false
17/04/04 11:10:41 DEBUG DFSClient: Waiting for ack for: -1
17/04/04 11:10:41 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=0 lastFlushOffset=0 createNewBlock=false
17/04/04 11:10:41 DEBUG DFSClient: Waiting for ack for: -1
17/04/04 11:10:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #3
17/04/04 11:10:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #3
17/04/04 11:10:42 DEBUG ProtobufRpcEngine: Call: getFileInfo took 1ms
17/04/04 11:10:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #4
17/04/04 11:10:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #4
17/04/04 11:10:42 DEBUG ProtobufRpcEngine: Call: getListing took 1ms
17/04/04 11:10:42 DEBUG FileInputFormat: Time taken to get FileStatuses: 15
17/04/04 11:10:42 INFO FileInputFormat: Total input paths to process : 10
17/04/04 11:10:42 DEBUG FileInputFormat: Total # of splits generated by getSplits: 12, TimeTaken: 19
17/04/04 11:10:42 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=0 lastFlushOffset=0 createNewBlock=false
17/04/04 11:10:42 DEBUG DFSClient: Waiting for ack for: -1
[Stage 0:>                                                         (0 + 0) / 10]17/04/04 11:10:48 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=0, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0
17/04/04 11:10:48 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=6325 lastFlushOffset=0 createNewBlock=false
17/04/04 11:10:48 DEBUG DFSClient: Queued packet 0
17/04/04 11:10:48 DEBUG DFSClient: Waiting for ack for: 0
17/04/04 11:10:48 DEBUG DFSClient: Allocating new block
17/04/04 11:10:48 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #5
17/04/04 11:10:48 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #5
17/04/04 11:10:48 DEBUG ProtobufRpcEngine: Call: addBlock took 22ms
17/04/04 11:10:48 DEBUG DFSClient: pipeline = DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]
17/04/04 11:10:48 DEBUG DFSClient: Connecting to datanode 172.21.15.173:50010
17/04/04 11:10:48 DEBUG DFSClient: Send buf size 124928
17/04/04 11:10:48 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #6
17/04/04 11:10:48 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #6
17/04/04 11:10:48 DEBUG ProtobufRpcEngine: Call: getServerDefaults took 1ms
17/04/04 11:10:48 DEBUG SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = /172.21.15.173, datanodeId = DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]
17/04/04 11:10:48 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 6325
17/04/04 11:10:48 DEBUG DFSClient: DFSClient seqno: 0 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:10:48 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #7
17/04/04 11:10:48 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #7
17/04/04 11:10:48 DEBUG ProtobufRpcEngine: Call: fsync took 5ms
17/04/04 11:10:48 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=1, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=6144
17/04/04 11:10:48 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=6325 lastFlushOffset=6325 createNewBlock=false
17/04/04 11:10:48 DEBUG DFSClient: Waiting for ack for: 0
[Stage 0:>                                                         (0 + 4) / 10][Stage 0:=================>                                        (3 + 4) / 10][Stage 0:=======================>                                  (4 + 4) / 10][Stage 0:=============================>                            (5 + 4) / 10][Stage 0:==================================>                       (6 + 4) / 10][Stage 0:==============================================>           (8 + 2) / 10]17/04/04 11:10:58 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:10:58 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 0:====================================================>     (9 + 1) / 10]                                                                                17/04/04 11:10:59 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=2, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=6144
17/04/04 11:10:59 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=10467 lastFlushOffset=6325 createNewBlock=false
17/04/04 11:10:59 DEBUG DFSClient: Queued packet 2
17/04/04 11:10:59 DEBUG DFSClient: Waiting for ack for: 2
17/04/04 11:10:59 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 2 offsetInBlock: 6144 lastPacketInBlock: false lastByteOffsetInBlock: 10467
17/04/04 11:10:59 DEBUG DFSClient: DFSClient seqno: 2 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:10:59 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=3, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=10240
17/04/04 11:10:59 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=10467 lastFlushOffset=10467 createNewBlock=false
17/04/04 11:10:59 DEBUG DFSClient: Waiting for ack for: 2
17/04/04 11:10:59 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=4, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=10240
17/04/04 11:10:59 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=10467 lastFlushOffset=10467 createNewBlock=false
17/04/04 11:10:59 DEBUG DFSClient: Waiting for ack for: 2
[Stage 1:>                                                         (0 + 4) / 10][Stage 1:=================>                                        (3 + 4) / 10][Stage 1:=======================>                                  (4 + 4) / 10][Stage 1:========================================>                 (7 + 3) / 10][Stage 1:==============================================>           (8 + 2) / 10]17/04/04 11:11:01 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=5, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=10240
17/04/04 11:11:01 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=14898 lastFlushOffset=10467 createNewBlock=false
17/04/04 11:11:01 DEBUG DFSClient: Queued packet 5
17/04/04 11:11:01 DEBUG DFSClient: Waiting for ack for: 5
17/04/04 11:11:01 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 5 offsetInBlock: 10240 lastPacketInBlock: false lastByteOffsetInBlock: 14898
17/04/04 11:11:01 DEBUG DFSClient: DFSClient seqno: 5 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 2:>                                                         (0 + 4) / 10][Stage 2:=======================>                                  (4 + 4) / 10][Stage 2:==============================================>           (8 + 2) / 10]                                                                                17/04/04 11:11:09 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=6, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=14848
17/04/04 11:11:09 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=27119 lastFlushOffset=14898 createNewBlock=false
17/04/04 11:11:09 DEBUG DFSClient: Queued packet 6
17/04/04 11:11:09 DEBUG DFSClient: Waiting for ack for: 6
17/04/04 11:11:09 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 6 offsetInBlock: 14848 lastPacketInBlock: false lastByteOffsetInBlock: 27119
17/04/04 11:11:09 DEBUG DFSClient: DFSClient seqno: 6 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:11:09 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=7, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=26624
17/04/04 11:11:09 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=27119 lastFlushOffset=27119 createNewBlock=false
17/04/04 11:11:09 DEBUG DFSClient: Waiting for ack for: 6
17/04/04 11:11:09 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=8, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=26624
17/04/04 11:11:09 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=27119 lastFlushOffset=27119 createNewBlock=false
17/04/04 11:11:09 DEBUG DFSClient: Waiting for ack for: 6
[Stage 4:>                                                         (0 + 4) / 10]17/04/04 11:11:11 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:11:11 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:11:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:11:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #8
17/04/04 11:11:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #8
17/04/04 11:11:11 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/04/04 11:11:11 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:11:11 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
[Stage 4:=======================>                                  (4 + 4) / 10][Stage 4:==================================>                       (6 + 4) / 10][Stage 4:==============================================>           (8 + 2) / 10]17/04/04 11:11:21 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:11:21 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/04/04 11:11:22 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=9, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=26624
17/04/04 11:11:22 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=38798 lastFlushOffset=27119 createNewBlock=false
17/04/04 11:11:22 DEBUG DFSClient: Queued packet 9
17/04/04 11:11:22 DEBUG DFSClient: Waiting for ack for: 9
17/04/04 11:11:22 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 9 offsetInBlock: 26624 lastPacketInBlock: false lastByteOffsetInBlock: 38798
17/04/04 11:11:22 DEBUG DFSClient: DFSClient seqno: 9 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
                                                                                17/04/04 11:11:22 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=10, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=38400
17/04/04 11:11:22 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=47829 lastFlushOffset=38798 createNewBlock=false
17/04/04 11:11:22 DEBUG DFSClient: Queued packet 10
17/04/04 11:11:22 DEBUG DFSClient: Waiting for ack for: 10
17/04/04 11:11:22 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 10 offsetInBlock: 38400 lastPacketInBlock: false lastByteOffsetInBlock: 47829
17/04/04 11:11:22 DEBUG DFSClient: DFSClient seqno: 10 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:11:22 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=11, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=47616
17/04/04 11:11:22 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=47829 lastFlushOffset=47829 createNewBlock=false
17/04/04 11:11:22 DEBUG DFSClient: Waiting for ack for: 10
17/04/04 11:11:22 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=12, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=47616
17/04/04 11:11:22 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=47829 lastFlushOffset=47829 createNewBlock=false
17/04/04 11:11:22 DEBUG DFSClient: Waiting for ack for: 10
[Stage 7:>                                                         (0 + 4) / 10][Stage 7:=====>                                                    (1 + 4) / 10][Stage 7:===========>                                              (2 + 4) / 10][Stage 7:=================>                                        (3 + 4) / 10][Stage 7:=======================>                                  (4 + 4) / 10][Stage 7:=============================>                            (5 + 4) / 10][Stage 7:==================================>                       (6 + 4) / 10][Stage 7:==============================================>           (8 + 2) / 10][Stage 7:====================================================>     (9 + 1) / 10]                                                                                17/04/04 11:11:39 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=13, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=47616
17/04/04 11:11:39 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=58353 lastFlushOffset=47829 createNewBlock=false
17/04/04 11:11:39 DEBUG DFSClient: Queued packet 13
17/04/04 11:11:39 DEBUG DFSClient: Waiting for ack for: 13
17/04/04 11:11:39 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 13 offsetInBlock: 47616 lastPacketInBlock: false lastByteOffsetInBlock: 58353
17/04/04 11:11:39 DEBUG DFSClient: DFSClient seqno: 13 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:11:39 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=14, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=57856
17/04/04 11:11:39 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=58353 lastFlushOffset=58353 createNewBlock=false
17/04/04 11:11:39 DEBUG DFSClient: Waiting for ack for: 13
17/04/04 11:11:39 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=15, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=57856
17/04/04 11:11:39 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=58353 lastFlushOffset=58353 createNewBlock=false
17/04/04 11:11:39 DEBUG DFSClient: Waiting for ack for: 13
17/04/04 11:11:39 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=16, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=57856
17/04/04 11:11:39 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=58353 lastFlushOffset=58353 createNewBlock=false
17/04/04 11:11:39 DEBUG DFSClient: Waiting for ack for: 13
17/04/04 11:11:39 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=17, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=57856
17/04/04 11:11:39 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=66546 lastFlushOffset=58353 createNewBlock=false
17/04/04 11:11:39 DEBUG DFSClient: Queued packet 17
17/04/04 11:11:39 DEBUG DFSClient: Waiting for ack for: 17
17/04/04 11:11:39 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 17 offsetInBlock: 57856 lastPacketInBlock: false lastByteOffsetInBlock: 66546
17/04/04 11:11:39 DEBUG DFSClient: DFSClient seqno: 17 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 11:>                                                        (0 + 4) / 10][Stage 11:======================>                                  (4 + 4) / 10]17/04/04 11:11:41 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:11:41 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:11:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:11:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #9
17/04/04 11:11:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #9
17/04/04 11:11:41 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/04/04 11:11:41 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:11:41 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
[Stage 11:==================================>                      (6 + 4) / 10][Stage 11:=======================================>                 (7 + 3) / 10]17/04/04 11:11:42 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=18, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=66048
17/04/04 11:11:42 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=76563 lastFlushOffset=66546 createNewBlock=false
17/04/04 11:11:42 DEBUG DFSClient: Queued packet 18
17/04/04 11:11:42 DEBUG DFSClient: Waiting for ack for: 18
17/04/04 11:11:42 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 18 offsetInBlock: 66048 lastPacketInBlock: false lastByteOffsetInBlock: 76563
17/04/04 11:11:42 DEBUG DFSClient: DFSClient seqno: 18 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
                                                                                17/04/04 11:11:42 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=19, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=76288
17/04/04 11:11:42 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=81246 lastFlushOffset=76563 createNewBlock=false
17/04/04 11:11:42 DEBUG DFSClient: Queued packet 19
17/04/04 11:11:42 DEBUG DFSClient: Waiting for ack for: 19
17/04/04 11:11:42 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 19 offsetInBlock: 76288 lastPacketInBlock: false lastByteOffsetInBlock: 81246
17/04/04 11:11:42 DEBUG DFSClient: DFSClient seqno: 19 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:11:42 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=20, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=80896
17/04/04 11:11:42 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=81246 lastFlushOffset=81246 createNewBlock=false
17/04/04 11:11:42 DEBUG DFSClient: Waiting for ack for: 19
17/04/04 11:11:42 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=21, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=80896
17/04/04 11:11:42 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=85488 lastFlushOffset=81246 createNewBlock=false
17/04/04 11:11:42 DEBUG DFSClient: Queued packet 21
17/04/04 11:11:42 DEBUG DFSClient: Waiting for ack for: 21
17/04/04 11:11:42 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 21 offsetInBlock: 80896 lastPacketInBlock: false lastByteOffsetInBlock: 85488
17/04/04 11:11:42 DEBUG DFSClient: DFSClient seqno: 21 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:11:43 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=22, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=84992
17/04/04 11:11:43 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=89936 lastFlushOffset=85488 createNewBlock=false
17/04/04 11:11:43 DEBUG DFSClient: Queued packet 22
17/04/04 11:11:43 DEBUG DFSClient: Waiting for ack for: 22
17/04/04 11:11:43 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 22 offsetInBlock: 84992 lastPacketInBlock: false lastByteOffsetInBlock: 89936
17/04/04 11:11:43 DEBUG DFSClient: DFSClient seqno: 22 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:11:43 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=23, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=89600
17/04/04 11:11:43 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=103108 lastFlushOffset=89936 createNewBlock=false
17/04/04 11:11:43 DEBUG DFSClient: Queued packet 23
17/04/04 11:11:43 DEBUG DFSClient: Waiting for ack for: 23
17/04/04 11:11:43 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 23 offsetInBlock: 89600 lastPacketInBlock: false lastByteOffsetInBlock: 103108
17/04/04 11:11:43 DEBUG DFSClient: DFSClient seqno: 23 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:11:43 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=24, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=102912
17/04/04 11:11:43 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=103108 lastFlushOffset=103108 createNewBlock=false
17/04/04 11:11:43 DEBUG DFSClient: Waiting for ack for: 23
17/04/04 11:11:43 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=25, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=102912
17/04/04 11:11:43 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=103108 lastFlushOffset=103108 createNewBlock=false
17/04/04 11:11:43 DEBUG DFSClient: Waiting for ack for: 23
17/04/04 11:11:43 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=26, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=102912
17/04/04 11:11:43 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=103108 lastFlushOffset=103108 createNewBlock=false
17/04/04 11:11:43 DEBUG DFSClient: Waiting for ack for: 23
17/04/04 11:11:43 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=27, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=102912
17/04/04 11:11:43 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=107605 lastFlushOffset=103108 createNewBlock=false
17/04/04 11:11:43 DEBUG DFSClient: Queued packet 27
17/04/04 11:11:43 DEBUG DFSClient: Waiting for ack for: 27
17/04/04 11:11:43 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 27 offsetInBlock: 102912 lastPacketInBlock: false lastByteOffsetInBlock: 107605
17/04/04 11:11:43 DEBUG DFSClient: DFSClient seqno: 27 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 24:>                                                        (0 + 4) / 10][Stage 24:======================>                                  (4 + 4) / 10][Stage 24:=======================================>                 (7 + 3) / 10][Stage 24:=============================================>           (8 + 2) / 10]17/04/04 11:11:46 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=28, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=107520
17/04/04 11:11:46 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=113212 lastFlushOffset=107605 createNewBlock=false
17/04/04 11:11:46 DEBUG DFSClient: Queued packet 28
17/04/04 11:11:46 DEBUG DFSClient: Waiting for ack for: 28
17/04/04 11:11:46 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 28 offsetInBlock: 107520 lastPacketInBlock: false lastByteOffsetInBlock: 113212
17/04/04 11:11:46 DEBUG DFSClient: DFSClient seqno: 28 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 25:=============================================>           (8 + 2) / 10]                                                                                17/04/04 11:11:46 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=29, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=113152
17/04/04 11:11:46 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=122473 lastFlushOffset=113212 createNewBlock=false
17/04/04 11:11:46 DEBUG DFSClient: Queued packet 29
17/04/04 11:11:46 DEBUG DFSClient: Waiting for ack for: 29
17/04/04 11:11:46 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 29 offsetInBlock: 113152 lastPacketInBlock: false lastByteOffsetInBlock: 122473
17/04/04 11:11:46 DEBUG DFSClient: DFSClient seqno: 29 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:11:46 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=30, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=122368
17/04/04 11:11:46 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=122473 lastFlushOffset=122473 createNewBlock=false
17/04/04 11:11:46 DEBUG DFSClient: Waiting for ack for: 29
17/04/04 11:11:46 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=31, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=122368
17/04/04 11:11:46 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=126392 lastFlushOffset=122473 createNewBlock=false
17/04/04 11:11:46 DEBUG DFSClient: Queued packet 31
17/04/04 11:11:46 DEBUG DFSClient: Waiting for ack for: 31
17/04/04 11:11:46 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 31 offsetInBlock: 122368 lastPacketInBlock: false lastByteOffsetInBlock: 126392
17/04/04 11:11:46 DEBUG DFSClient: DFSClient seqno: 31 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 32:========================================================(10 + 0) / 10]17/04/04 11:11:47 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=32, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=125952
17/04/04 11:11:47 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=131712 lastFlushOffset=126392 createNewBlock=false
17/04/04 11:11:47 DEBUG DFSClient: Queued packet 32
17/04/04 11:11:47 DEBUG DFSClient: Waiting for ack for: 32
17/04/04 11:11:47 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 32 offsetInBlock: 125952 lastPacketInBlock: false lastByteOffsetInBlock: 131712
17/04/04 11:11:47 DEBUG DFSClient: DFSClient seqno: 32 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 33:======================>                                  (4 + 4) / 10]17/04/04 11:11:51 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:11:51 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 33:=============================================>           (8 + 2) / 10]                                                                                17/04/04 11:11:53 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=33, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=131584
17/04/04 11:11:53 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=149745 lastFlushOffset=131712 createNewBlock=false
17/04/04 11:11:53 DEBUG DFSClient: Queued packet 33
17/04/04 11:11:53 DEBUG DFSClient: Waiting for ack for: 33
17/04/04 11:11:53 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 33 offsetInBlock: 131584 lastPacketInBlock: false lastByteOffsetInBlock: 149745
17/04/04 11:11:53 DEBUG DFSClient: DFSClient seqno: 33 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:11:53 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=34, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=149504
17/04/04 11:11:53 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=149745 lastFlushOffset=149745 createNewBlock=false
17/04/04 11:11:53 DEBUG DFSClient: Waiting for ack for: 33
17/04/04 11:11:53 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=35, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=149504
17/04/04 11:11:53 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=149745 lastFlushOffset=149745 createNewBlock=false
17/04/04 11:11:53 DEBUG DFSClient: Waiting for ack for: 33
17/04/04 11:11:53 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=36, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=149504
17/04/04 11:11:53 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=149745 lastFlushOffset=149745 createNewBlock=false
17/04/04 11:11:53 DEBUG DFSClient: Waiting for ack for: 33
17/04/04 11:11:53 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=37, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=149504
17/04/04 11:11:53 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=153997 lastFlushOffset=149745 createNewBlock=false
17/04/04 11:11:53 DEBUG DFSClient: Queued packet 37
17/04/04 11:11:53 DEBUG DFSClient: Waiting for ack for: 37
17/04/04 11:11:53 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 37 offsetInBlock: 149504 lastPacketInBlock: false lastByteOffsetInBlock: 153997
17/04/04 11:11:53 DEBUG DFSClient: DFSClient seqno: 37 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 41:>                                                        (0 + 4) / 10]17/04/04 11:12:11 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:12:11 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:12:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:12:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #10
17/04/04 11:12:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #10
17/04/04 11:12:11 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/04/04 11:12:11 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:12:11 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
17/04/04 11:12:21 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:12:21 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/04/04 11:12:41 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:12:41 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:12:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:12:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #11
17/04/04 11:12:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #11
17/04/04 11:12:41 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/04/04 11:12:41 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:12:41 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
[Stage 41:=====>                                                   (1 + 4) / 10][Stage 41:======================>                                  (4 + 4) / 10]17/04/04 11:12:51 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:12:51 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 41:============================>                            (5 + 4) / 10][Stage 41:=======================================>                 (7 + 3) / 10][Stage 41:=============================================>           (8 + 2) / 10]17/04/04 11:13:11 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:13:11 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:13:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:13:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #12
17/04/04 11:13:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #12
17/04/04 11:13:11 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/04/04 11:13:11 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:13:11 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
[Stage 41:===================================================>     (9 + 1) / 10]17/04/04 11:13:12 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=38, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=153600
17/04/04 11:13:12 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=162684 lastFlushOffset=153997 createNewBlock=false
17/04/04 11:13:12 DEBUG DFSClient: Queued packet 38
17/04/04 11:13:12 DEBUG DFSClient: Waiting for ack for: 38
17/04/04 11:13:12 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 38 offsetInBlock: 153600 lastPacketInBlock: false lastByteOffsetInBlock: 162684
17/04/04 11:13:12 WARN DFSClient: Slow ReadProcessor read fields took 79554ms (threshold=30000ms); ack: seqno: 38 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
[Stage 42:>                                                        (0 + 4) / 10][Stage 42:===========>                                             (2 + 4) / 10][Stage 42:=================>                                       (3 + 4) / 10][Stage 42:======================>                                  (4 + 4) / 10][Stage 42:==================================>                      (6 + 4) / 10][Stage 42:=======================================>                 (7 + 3) / 10][Stage 42:=============================================>           (8 + 2) / 10][Stage 42:===================================================>     (9 + 1) / 10]                                                                                17/04/04 11:13:17 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=39, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=162304
17/04/04 11:13:17 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=175862 lastFlushOffset=162684 createNewBlock=false
17/04/04 11:13:17 DEBUG DFSClient: Queued packet 39
17/04/04 11:13:17 DEBUG DFSClient: Waiting for ack for: 39
17/04/04 11:13:17 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 39 offsetInBlock: 162304 lastPacketInBlock: false lastByteOffsetInBlock: 175862
17/04/04 11:13:17 DEBUG DFSClient: DFSClient seqno: 39 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:13:17 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=40, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=175616
17/04/04 11:13:17 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=175862 lastFlushOffset=175862 createNewBlock=false
17/04/04 11:13:17 DEBUG DFSClient: Waiting for ack for: 39
17/04/04 11:13:17 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=41, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=175616
17/04/04 11:13:17 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=179207 lastFlushOffset=175862 createNewBlock=false
17/04/04 11:13:17 DEBUG DFSClient: Queued packet 41
17/04/04 11:13:17 DEBUG DFSClient: Waiting for ack for: 41
17/04/04 11:13:17 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 41 offsetInBlock: 175616 lastPacketInBlock: false lastByteOffsetInBlock: 179207
17/04/04 11:13:17 DEBUG DFSClient: DFSClient seqno: 41 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 51:=================>                                       (3 + 4) / 10][Stage 51:======================>                                  (4 + 4) / 10][Stage 51:============================>                            (5 + 4) / 10][Stage 51:==================================>                      (6 + 4) / 10][Stage 51:=======================================>                 (7 + 3) / 10]17/04/04 11:13:21 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:13:21 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 51:===================================================>     (9 + 1) / 10]17/04/04 11:13:22 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=42, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=179200
17/04/04 11:13:22 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=184167 lastFlushOffset=179207 createNewBlock=false
17/04/04 11:13:22 DEBUG DFSClient: Queued packet 42
17/04/04 11:13:22 DEBUG DFSClient: Waiting for ack for: 42
17/04/04 11:13:22 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 42 offsetInBlock: 179200 lastPacketInBlock: false lastByteOffsetInBlock: 184167
17/04/04 11:13:22 DEBUG DFSClient: DFSClient seqno: 42 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 52:>                                                        (0 + 4) / 10][Stage 52:=====>                                                   (1 + 4) / 10][Stage 52:===========>                                             (2 + 4) / 10][Stage 52:======================>                                  (4 + 4) / 10]17/04/04 11:13:41 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:13:41 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:13:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:13:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #13
17/04/04 11:13:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #13
17/04/04 11:13:41 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/04/04 11:13:41 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:13:41 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
[Stage 52:============================>                            (5 + 4) / 10][Stage 52:==================================>                      (6 + 4) / 10][Stage 52:=======================================>                 (7 + 3) / 10]17/04/04 11:13:46 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=43, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=183808
[Stage 52:=============================================>           (8 + 2) / 10]                                                                                17/04/04 11:13:47 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=205456 lastFlushOffset=184167 createNewBlock=false
17/04/04 11:13:47 DEBUG DFSClient: Queued packet 43
17/04/04 11:13:47 DEBUG DFSClient: Waiting for ack for: 43
17/04/04 11:13:47 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 43 offsetInBlock: 183808 lastPacketInBlock: false lastByteOffsetInBlock: 205456
17/04/04 11:13:47 DEBUG DFSClient: DFSClient seqno: 43 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:13:47 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=44, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=205312
17/04/04 11:13:47 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=205456 lastFlushOffset=205456 createNewBlock=false
17/04/04 11:13:47 DEBUG DFSClient: Waiting for ack for: 43
17/04/04 11:13:47 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=45, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=205312
17/04/04 11:13:47 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=205456 lastFlushOffset=205456 createNewBlock=false
17/04/04 11:13:48 DEBUG DFSClient: Waiting for ack for: 43
17/04/04 11:13:48 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=46, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=205312
17/04/04 11:13:48 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=205456 lastFlushOffset=205456 createNewBlock=false
17/04/04 11:13:48 DEBUG DFSClient: Waiting for ack for: 43
17/04/04 11:13:48 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=47, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=205312
17/04/04 11:13:48 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=213082 lastFlushOffset=205456 createNewBlock=false
17/04/04 11:13:48 DEBUG DFSClient: Queued packet 47
17/04/04 11:13:48 DEBUG DFSClient: Waiting for ack for: 47
17/04/04 11:13:48 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 47 offsetInBlock: 205312 lastPacketInBlock: false lastByteOffsetInBlock: 213082
17/04/04 11:13:48 DEBUG DFSClient: DFSClient seqno: 47 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 62:>                                                        (0 + 4) / 10]17/04/04 11:13:51 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:13:51 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 62:=====>                                                   (1 + 4) / 10][Stage 62:===========>                                             (2 + 4) / 10][Stage 62:=================>                                       (3 + 4) / 10][Stage 62:============================>                            (5 + 4) / 10][Stage 62:==================================>                      (6 + 4) / 10][Stage 62:=======================================>                 (7 + 3) / 10][Stage 62:=============================================>           (8 + 2) / 10][Stage 62:===================================================>     (9 + 1) / 10]17/04/04 11:14:02 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=48, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=212992
17/04/04 11:14:02 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=222096 lastFlushOffset=213082 createNewBlock=false
17/04/04 11:14:02 DEBUG DFSClient: Queued packet 48
17/04/04 11:14:02 DEBUG DFSClient: Waiting for ack for: 48
17/04/04 11:14:02 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 48 offsetInBlock: 212992 lastPacketInBlock: false lastByteOffsetInBlock: 222096
17/04/04 11:14:02 DEBUG DFSClient: DFSClient seqno: 48 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 63:>                                                        (0 + 4) / 10][Stage 63:=====>                                                   (1 + 4) / 10][Stage 63:======================>                                  (4 + 4) / 10][Stage 63:==================================>                      (6 + 4) / 10][Stage 63:=======================================>                 (7 + 3) / 10][Stage 63:=============================================>           (8 + 2) / 10]                                                                                17/04/04 11:14:09 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=49, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=221696
17/04/04 11:14:09 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=234551 lastFlushOffset=222096 createNewBlock=false
17/04/04 11:14:09 DEBUG DFSClient: Queued packet 49
17/04/04 11:14:09 DEBUG DFSClient: Waiting for ack for: 49
17/04/04 11:14:09 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 49 offsetInBlock: 221696 lastPacketInBlock: false lastByteOffsetInBlock: 234551
17/04/04 11:14:09 DEBUG DFSClient: DFSClient seqno: 49 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:14:09 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=50, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=234496
17/04/04 11:14:09 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=234551 lastFlushOffset=234551 createNewBlock=false
17/04/04 11:14:09 DEBUG DFSClient: Waiting for ack for: 49
17/04/04 11:14:09 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=51, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=234496
17/04/04 11:14:09 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=241644 lastFlushOffset=234551 createNewBlock=false
17/04/04 11:14:09 DEBUG DFSClient: Queued packet 51
17/04/04 11:14:09 DEBUG DFSClient: Waiting for ack for: 51
17/04/04 11:14:09 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 51 offsetInBlock: 234496 lastPacketInBlock: false lastByteOffsetInBlock: 241644
17/04/04 11:14:09 DEBUG DFSClient: DFSClient seqno: 51 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 74:======================>                                  (4 + 4) / 10][Stage 74:=============================================>           (8 + 2) / 10][Stage 74:===================================================>     (9 + 1) / 10]17/04/04 11:14:11 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=52, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=241152
17/04/04 11:14:11 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=246935 lastFlushOffset=241644 createNewBlock=false
17/04/04 11:14:11 DEBUG DFSClient: Queued packet 52
17/04/04 11:14:11 DEBUG DFSClient: Waiting for ack for: 52
17/04/04 11:14:11 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 52 offsetInBlock: 241152 lastPacketInBlock: false lastByteOffsetInBlock: 246935
17/04/04 11:14:11 DEBUG DFSClient: DFSClient seqno: 52 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:14:11 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:14:11 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:14:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:14:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #14
17/04/04 11:14:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #14
17/04/04 11:14:11 DEBUG ProtobufRpcEngine: Call: renewLease took 8ms
17/04/04 11:14:11 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:14:11 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
[Stage 75:>                                                        (0 + 4) / 10][Stage 75:=====>                                                   (1 + 4) / 10][Stage 75:===========>                                             (2 + 4) / 10][Stage 75:======================>                                  (4 + 4) / 10]17/04/04 11:14:21 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:14:21 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 75:============================>                            (5 + 4) / 10][Stage 75:==================================>                      (6 + 4) / 10][Stage 75:=======================================>                 (7 + 3) / 10]17/04/04 11:14:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=53, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=246784
[Stage 75:=============================================>           (8 + 2) / 10][Stage 75:===================================================>     (9 + 1) / 10]                                                                                17/04/04 11:14:33 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=268032 lastFlushOffset=246935 createNewBlock=false
17/04/04 11:14:33 DEBUG DFSClient: Queued packet 53
17/04/04 11:14:33 DEBUG DFSClient: Waiting for ack for: 53
17/04/04 11:14:33 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 53 offsetInBlock: 246784 lastPacketInBlock: false lastByteOffsetInBlock: 268032
17/04/04 11:14:33 DEBUG DFSClient: DFSClient seqno: 53 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:14:34 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=54, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=267776
17/04/04 11:14:34 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=268032 lastFlushOffset=268032 createNewBlock=false
17/04/04 11:14:34 DEBUG DFSClient: Waiting for ack for: 53
17/04/04 11:14:34 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=55, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=267776
17/04/04 11:14:34 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=268032 lastFlushOffset=268032 createNewBlock=false
17/04/04 11:14:34 DEBUG DFSClient: Waiting for ack for: 53
17/04/04 11:14:34 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=56, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=267776
17/04/04 11:14:34 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=268032 lastFlushOffset=268032 createNewBlock=false
17/04/04 11:14:34 DEBUG DFSClient: Waiting for ack for: 53
17/04/04 11:14:34 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=57, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=267776
17/04/04 11:14:34 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=275650 lastFlushOffset=268032 createNewBlock=false
17/04/04 11:14:34 DEBUG DFSClient: Queued packet 57
17/04/04 11:14:34 DEBUG DFSClient: Waiting for ack for: 57
17/04/04 11:14:34 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 57 offsetInBlock: 267776 lastPacketInBlock: false lastByteOffsetInBlock: 275650
17/04/04 11:14:34 DEBUG DFSClient: DFSClient seqno: 57 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 87:>                                                        (0 + 4) / 10]17/04/04 11:14:41 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:14:41 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:14:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:14:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #15
17/04/04 11:14:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #15
17/04/04 11:14:41 DEBUG ProtobufRpcEngine: Call: renewLease took 8ms
17/04/04 11:14:41 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:14:41 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
17/04/04 11:14:51 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:14:51 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 87:===========>                                             (2 + 4) / 10]17/04/04 11:15:00 WARN TaskSetManager: Lost task 2.0 in stage 87.0 (TID 232, 172.21.15.173, executor 0): java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at org.apache.spark.graphx.impl.EdgePartitionBuilder.toEdgePartition(EdgePartitionBuilder.scala:43)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:110)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:105)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:845)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:845)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/04/04 11:15:02 WARN TaskSetManager: Lost task 5.0 in stage 87.0 (TID 235, 172.21.15.173, executor 0): java.io.FileNotFoundException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-cac9138d-a598-48b5-9bd5-dddbc20bac5b/0b/temp_shuffle_40d558b7-7cc8-48f6-bcfa-e9cb3df7d2fa (没有那个文件或目录)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:229)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:152)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/04/04 11:15:05 WARN TaskSetManager: Lost task 4.0 in stage 87.0 (TID 234, 172.21.15.173, executor 0): FetchFailed(BlockManagerId(0, 172.21.15.173, 56478, None), shuffleId=2, mapId=1, reduceId=4, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-cac9138d-a598-48b5-9bd5-dddbc20bac5b/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-cac9138d-a598-48b5-9bd5-dddbc20bac5b/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:199)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:302)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:269)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:303)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:132)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	... 23 more

)
17/04/04 11:15:05 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=58, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=275456
17/04/04 11:15:05 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=285530 lastFlushOffset=275650 createNewBlock=false
17/04/04 11:15:05 DEBUG DFSClient: Queued packet 58
17/04/04 11:15:05 DEBUG DFSClient: Waiting for ack for: 58
17/04/04 11:15:05 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 58 offsetInBlock: 275456 lastPacketInBlock: false lastByteOffsetInBlock: 285530
17/04/04 11:15:05 WARN DFSClient: Slow ReadProcessor read fields took 31049ms (threshold=30000ms); ack: seqno: 58 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
17/04/04 11:15:05 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=59, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=285184
17/04/04 11:15:05 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=285530 lastFlushOffset=285530 createNewBlock=false
17/04/04 11:15:05 DEBUG DFSClient: Waiting for ack for: 58
[Stage 76:>                                                        (0 + 0) / 10]17/04/04 11:15:06 ERROR TaskSchedulerImpl: Lost executor 0 on 172.21.15.173: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/04/04 11:15:06 WARN TaskSetManager: Lost task 6.0 in stage 87.0 (TID 236, 172.21.15.173, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/04/04 11:15:06 WARN TaskSetManager: Lost task 5.1 in stage 87.0 (TID 238, 172.21.15.173, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/04/04 11:15:06 WARN TaskSetManager: Lost task 2.1 in stage 87.0 (TID 237, 172.21.15.173, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/04/04 11:15:06 WARN TaskSetManager: Lost task 1.0 in stage 87.0 (TID 231, 172.21.15.173, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/04/04 11:15:06 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=60, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=285184
17/04/04 11:15:06 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=285530 lastFlushOffset=285530 createNewBlock=false
17/04/04 11:15:06 DEBUG DFSClient: Waiting for ack for: 58
17/04/04 11:15:11 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:15:11 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:15:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:15:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #16
17/04/04 11:15:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #16
17/04/04 11:15:11 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/04/04 11:15:11 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:15:11 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
17/04/04 11:15:12 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=61, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=285184
17/04/04 11:15:12 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=285530 lastFlushOffset=285530 createNewBlock=false
17/04/04 11:15:12 DEBUG DFSClient: Waiting for ack for: 58
[Stage 76:>                                                        (0 + 4) / 10]17/04/04 11:15:12 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=62, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=285184
17/04/04 11:15:12 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=285530 lastFlushOffset=285530 createNewBlock=false
17/04/04 11:15:12 DEBUG DFSClient: Waiting for ack for: 58
[Stage 76:=================>                                       (3 + 4) / 10]17/04/04 11:15:21 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:15:21 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 76:======================>                                  (4 + 4) / 10][Stage 76:============================>                            (5 + 4) / 10][Stage 76:=======================================>                 (7 + 3) / 10][Stage 76:=============================================>           (8 + 2) / 10][Stage 76:===================================================>     (9 + 1) / 10]17/04/04 11:15:25 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=63, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=285184
17/04/04 11:15:25 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=296433 lastFlushOffset=285530 createNewBlock=false
17/04/04 11:15:25 DEBUG DFSClient: Queued packet 63
17/04/04 11:15:25 DEBUG DFSClient: Waiting for ack for: 63
17/04/04 11:15:25 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 63 offsetInBlock: 285184 lastPacketInBlock: false lastByteOffsetInBlock: 296433
17/04/04 11:15:25 DEBUG DFSClient: DFSClient seqno: 63 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 77:>                                                        (0 + 4) / 10][Stage 77:======================>                                  (4 + 4) / 10][Stage 77:=============================================>           (8 + 2) / 10]17/04/04 11:15:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=64, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=295936
17/04/04 11:15:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=305023 lastFlushOffset=296433 createNewBlock=false
17/04/04 11:15:35 DEBUG DFSClient: Queued packet 64
17/04/04 11:15:35 DEBUG DFSClient: Waiting for ack for: 64
17/04/04 11:15:35 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 64 offsetInBlock: 295936 lastPacketInBlock: false lastByteOffsetInBlock: 305023
17/04/04 11:15:35 DEBUG DFSClient: DFSClient seqno: 64 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:15:36 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=65, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=304640
17/04/04 11:15:36 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=314431 lastFlushOffset=305023 createNewBlock=false
17/04/04 11:15:36 DEBUG DFSClient: Queued packet 65
17/04/04 11:15:36 DEBUG DFSClient: Waiting for ack for: 65
17/04/04 11:15:36 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 65 offsetInBlock: 304640 lastPacketInBlock: false lastByteOffsetInBlock: 314431
17/04/04 11:15:36 DEBUG DFSClient: DFSClient seqno: 65 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 79:>                                                        (0 + 4) / 10][Stage 79:======================>                                  (4 + 4) / 10][Stage 79:=======================================>                 (7 + 3) / 10][Stage 79:===================================================>     (9 + 1) / 10]17/04/04 11:15:37 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=66, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=314368
17/04/04 11:15:37 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=319862 lastFlushOffset=314431 createNewBlock=false
17/04/04 11:15:37 DEBUG DFSClient: Queued packet 66
17/04/04 11:15:37 DEBUG DFSClient: Waiting for ack for: 66
17/04/04 11:15:37 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 66 offsetInBlock: 314368 lastPacketInBlock: false lastByteOffsetInBlock: 319862
17/04/04 11:15:37 DEBUG DFSClient: DFSClient seqno: 66 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 80:======================>                                  (4 + 4) / 10]17/04/04 11:15:38 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=67, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=319488
17/04/04 11:15:38 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=325423 lastFlushOffset=319862 createNewBlock=false
17/04/04 11:15:38 DEBUG DFSClient: Queued packet 67
17/04/04 11:15:38 DEBUG DFSClient: Waiting for ack for: 67
17/04/04 11:15:38 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 67 offsetInBlock: 319488 lastPacketInBlock: false lastByteOffsetInBlock: 325423
17/04/04 11:15:38 DEBUG DFSClient: DFSClient seqno: 67 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 81:>                                                        (0 + 4) / 10][Stage 81:======================>                                  (4 + 4) / 10][Stage 81:=============================================>           (8 + 2) / 10]17/04/04 11:15:40 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=68, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=325120
17/04/04 11:15:40 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=336353 lastFlushOffset=325423 createNewBlock=false
17/04/04 11:15:40 DEBUG DFSClient: Queued packet 68
17/04/04 11:15:40 DEBUG DFSClient: Waiting for ack for: 68
17/04/04 11:15:40 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 68 offsetInBlock: 325120 lastPacketInBlock: false lastByteOffsetInBlock: 336353
17/04/04 11:15:40 DEBUG DFSClient: DFSClient seqno: 68 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 82:>                                                        (0 + 4) / 10][Stage 82:======================>                                  (4 + 4) / 10]17/04/04 11:15:41 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:15:41 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:15:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:15:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #17
17/04/04 11:15:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #17
17/04/04 11:15:41 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/04/04 11:15:41 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:15:41 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
[Stage 82:=============================================>           (8 + 2) / 10][Stage 82:===================================================>     (9 + 1) / 10]17/04/04 11:15:42 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=69, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=335872
17/04/04 11:15:42 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=342346 lastFlushOffset=336353 createNewBlock=false
17/04/04 11:15:42 DEBUG DFSClient: Queued packet 69
17/04/04 11:15:42 DEBUG DFSClient: Waiting for ack for: 69
17/04/04 11:15:42 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 69 offsetInBlock: 335872 lastPacketInBlock: false lastByteOffsetInBlock: 342346
17/04/04 11:15:42 DEBUG DFSClient: DFSClient seqno: 69 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 83:>                                                        (0 + 4) / 10]17/04/04 11:15:51 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:15:51 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 83:===========>                                             (2 + 4) / 10][Stage 83:======================>                                  (4 + 4) / 10]17/04/04 11:16:11 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:16:11 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:16:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:16:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #18
17/04/04 11:16:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #18
17/04/04 11:16:11 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/04/04 11:16:11 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:16:11 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
17/04/04 11:16:21 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:16:21 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 83:============================>                            (5 + 4) / 10][Stage 83:=============================================>           (8 + 2) / 10]17/04/04 11:16:41 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:16:41 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:16:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:16:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #19
17/04/04 11:16:41 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #19
17/04/04 11:16:41 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/04/04 11:16:41 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:16:41 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
17/04/04 11:16:42 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=70, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=342016
17/04/04 11:16:42 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=353330 lastFlushOffset=342346 createNewBlock=false
17/04/04 11:16:42 DEBUG DFSClient: Queued packet 70
17/04/04 11:16:42 DEBUG DFSClient: Waiting for ack for: 70
17/04/04 11:16:42 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 70 offsetInBlock: 342016 lastPacketInBlock: false lastByteOffsetInBlock: 353330
17/04/04 11:16:42 WARN DFSClient: Slow ReadProcessor read fields took 60003ms (threshold=30000ms); ack: seqno: 70 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
[Stage 84:>                                                        (0 + 4) / 10][Stage 84:=====>                                                   (1 + 4) / 10][Stage 84:===========>                                             (2 + 4) / 10][Stage 84:======================>                                  (4 + 4) / 10][Stage 84:============================>                            (5 + 4) / 10][Stage 84:==================================>                      (6 + 4) / 10]17/04/04 11:16:51 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:16:51 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 84:=============================================>           (8 + 2) / 10][Stage 84:===================================================>     (9 + 1) / 10]17/04/04 11:16:55 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=71, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=353280
17/04/04 11:16:55 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=364759 lastFlushOffset=353330 createNewBlock=false
17/04/04 11:16:55 DEBUG DFSClient: Queued packet 71
17/04/04 11:16:55 DEBUG DFSClient: Waiting for ack for: 71
17/04/04 11:16:55 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 71 offsetInBlock: 353280 lastPacketInBlock: false lastByteOffsetInBlock: 364759
17/04/04 11:16:55 DEBUG DFSClient: DFSClient seqno: 71 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 85:>                                                        (0 + 4) / 10][Stage 85:=====>                                                   (1 + 4) / 10][Stage 85:======================>                                  (4 + 4) / 10][Stage 85:============================>                            (5 + 4) / 10][Stage 85:==================================>                      (6 + 4) / 10][Stage 85:=======================================>                 (7 + 3) / 10][Stage 85:=============================================>           (8 + 2) / 10]17/04/04 11:17:08 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=72, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=364544
17/04/04 11:17:08 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=370385 lastFlushOffset=364759 createNewBlock=false
17/04/04 11:17:08 DEBUG DFSClient: Queued packet 72
17/04/04 11:17:08 DEBUG DFSClient: Waiting for ack for: 72
17/04/04 11:17:08 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 72 offsetInBlock: 364544 lastPacketInBlock: false lastByteOffsetInBlock: 370385
17/04/04 11:17:08 DEBUG DFSClient: DFSClient seqno: 72 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 86:>                                                        (0 + 0) / 10][Stage 86:>                                                        (0 + 4) / 10]17/04/04 11:17:11 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:17:11 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:17:12 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:17:12 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #20
17/04/04 11:17:12 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #20
17/04/04 11:17:12 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/04/04 11:17:12 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:17:12 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
[Stage 86:===========>                                             (2 + 4) / 10][Stage 86:=================>                                       (3 + 4) / 10][Stage 86:======================>                                  (4 + 4) / 10][Stage 86:============================>                            (5 + 4) / 10][Stage 86:==================================>                      (6 + 4) / 10][Stage 86:=============================================>           (8 + 2) / 10]17/04/04 11:17:22 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:17:22 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 86:===================================================>     (9 + 1) / 10]17/04/04 11:17:23 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=73, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=370176
17/04/04 11:17:23 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=380349 lastFlushOffset=370385 createNewBlock=false
17/04/04 11:17:23 DEBUG DFSClient: Queued packet 73
17/04/04 11:17:23 DEBUG DFSClient: Waiting for ack for: 73
17/04/04 11:17:23 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 73 offsetInBlock: 370176 lastPacketInBlock: false lastByteOffsetInBlock: 380349
17/04/04 11:17:23 DEBUG DFSClient: DFSClient seqno: 73 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 87:>                                                        (0 + 4) / 10]17/04/04 11:17:40 WARN TaskSetManager: Lost task 2.0 in stage 87.1 (TID 351, 172.21.15.173, executor 1): java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:331)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:364)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:364)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:356)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:322)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:322)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1303)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:362)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:369)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)

17/04/04 11:17:41 WARN TaskSetManager: Lost task 3.0 in stage 87.1 (TID 352, 172.21.15.173, executor 1): java.io.FileNotFoundException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-670ed35e-002c-4f86-a722-1daac6f8853a/35/temp_shuffle_a5f88fe3-11ab-47db-ab35-92c618891cba (没有那个文件或目录)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:229)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:152)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/04/04 11:17:41 WARN TaskSetManager: Lost task 2.1 in stage 87.1 (TID 354, 172.21.15.173, executor 1): FetchFailed(BlockManagerId(1, 172.21.15.173, 45923, None), shuffleId=7, mapId=8, reduceId=2, message=
org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-670ed35e-002c-4f86-a722-1daac6f8853a/1d/shuffle_7_8_0.data, offset=14961288, length=7480825}
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:356)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-670ed35e-002c-4f86-a722-1daac6f8853a/1d/shuffle_7_8_0.data, offset=14961288, length=7480825}
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:113)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:349)
	... 36 more
Caused by: java.io.FileNotFoundException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-670ed35e-002c-4f86-a722-1daac6f8853a/1d/shuffle_7_8_0.data (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:98)
	... 37 more

)
17/04/04 11:17:41 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=74, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=379904
17/04/04 11:17:41 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=394023 lastFlushOffset=380349 createNewBlock=false
17/04/04 11:17:41 DEBUG DFSClient: Queued packet 74
17/04/04 11:17:41 DEBUG DFSClient: Waiting for ack for: 74
17/04/04 11:17:41 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 74 offsetInBlock: 379904 lastPacketInBlock: false lastByteOffsetInBlock: 394023
17/04/04 11:17:41 DEBUG DFSClient: DFSClient seqno: 74 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:17:41 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=75, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=393728
17/04/04 11:17:41 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=394023 lastFlushOffset=394023 createNewBlock=false
17/04/04 11:17:41 DEBUG DFSClient: Waiting for ack for: 74
17/04/04 11:17:41 WARN TaskSetManager: Lost task 1.1 in stage 87.1 (TID 355, 172.21.15.173, executor 1): FetchFailed(BlockManagerId(1, 172.21.15.173, 45923, None), shuffleId=7, mapId=8, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-670ed35e-002c-4f86-a722-1daac6f8853a/1d/shuffle_7_8_0.data, offset=7480738, length=7480550}
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:356)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-670ed35e-002c-4f86-a722-1daac6f8853a/1d/shuffle_7_8_0.data, offset=7480738, length=7480550}
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:113)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:349)
	... 36 more
Caused by: java.io.FileNotFoundException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-670ed35e-002c-4f86-a722-1daac6f8853a/1d/shuffle_7_8_0.data (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:98)
	... 37 more

)
17/04/04 11:17:41 WARN TaskSetManager: Lost task 0.1 in stage 87.1 (TID 356, 172.21.15.173, executor 1): FetchFailed(BlockManagerId(1, 172.21.15.173, 45923, None), shuffleId=2, mapId=1, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-670ed35e-002c-4f86-a722-1daac6f8853a/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-670ed35e-002c-4f86-a722-1daac6f8853a/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:199)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:302)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:269)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:303)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:132)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	... 23 more

)
17/04/04 11:17:41 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=76, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=393728
17/04/04 11:17:41 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=394023 lastFlushOffset=394023 createNewBlock=false
17/04/04 11:17:41 DEBUG DFSClient: Waiting for ack for: 74
17/04/04 11:17:41 WARN TaskSetManager: Lost task 4.0 in stage 87.1 (TID 353, 172.21.15.173, executor 1): FetchFailed(BlockManagerId(1, 172.21.15.173, 45923, None), shuffleId=5, mapId=4, reduceId=4, message=
org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-670ed35e-002c-4f86-a722-1daac6f8853a/1d/shuffle_5_4_0.data, offset=15009157, length=3752308}
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:356)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-670ed35e-002c-4f86-a722-1daac6f8853a/1d/shuffle_5_4_0.data, offset=15009157, length=3752308}
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:113)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:349)
	... 37 more
Caused by: java.io.FileNotFoundException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-670ed35e-002c-4f86-a722-1daac6f8853a/1d/shuffle_5_4_0.data (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:98)
	... 38 more

)
17/04/04 11:17:41 WARN TaskSetManager: Lost task 1.0 in stage 76.1 (TID 359, 172.21.15.173, executor 1): org.apache.spark.SparkException: Block rdd_3_1 was not found even though it's read-locked
	at org.apache.spark.storage.BlockManager.handleLocalReadFailure(BlockManager.scala:436)
	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:478)
	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:630)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:688)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/04/04 11:17:41 WARN TaskSetManager: Lost task 2.0 in stage 76.1 (TID 360, 172.21.15.173, executor 1): org.apache.spark.SparkException: Block rdd_3_2 was not found even though it's read-locked
	at org.apache.spark.storage.BlockManager.handleLocalReadFailure(BlockManager.scala:436)
	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:478)
	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:630)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:688)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/04/04 11:17:41 WARN TaskSetManager: Lost task 3.0 in stage 76.1 (TID 361, 172.21.15.173, executor 1): org.apache.spark.SparkException: Block rdd_3_3 was not found even though it's read-locked
	at org.apache.spark.storage.BlockManager.handleLocalReadFailure(BlockManager.scala:436)
	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:478)
	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:630)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:688)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/04/04 11:17:41 WARN TaskSetManager: Lost task 0.0 in stage 76.1 (TID 358, 172.21.15.173, executor 1): java.io.FileNotFoundException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-670ed35e-002c-4f86-a722-1daac6f8853a/21/temp_shuffle_6a583a79-a5d5-424d-8b52-a4a633fd9ded (没有那个文件或目录)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:229)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:152)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/04/04 11:17:42 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:17:42 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:17:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:17:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #21
17/04/04 11:17:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #21
17/04/04 11:17:42 DEBUG ProtobufRpcEngine: Call: renewLease took 8ms
17/04/04 11:17:42 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:17:42 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
[Stage 76:>                                                        (0 + 3) / 10]17/04/04 11:17:42 ERROR TaskSchedulerImpl: Lost executor 1 on 172.21.15.173: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/04/04 11:17:42 WARN TaskSetManager: Lost task 3.1 in stage 87.1 (TID 357, 172.21.15.173, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/04/04 11:17:42 WARN TaskSetManager: Lost task 3.1 in stage 76.1 (TID 364, 172.21.15.173, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/04/04 11:17:42 WARN TaskSetManager: Lost task 2.1 in stage 76.1 (TID 363, 172.21.15.173, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/04/04 11:17:42 WARN TaskSetManager: Lost task 1.1 in stage 76.1 (TID 362, 172.21.15.173, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/04/04 11:17:42 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=77, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=393728
17/04/04 11:17:42 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=403108 lastFlushOffset=394023 createNewBlock=false
17/04/04 11:17:42 DEBUG DFSClient: Queued packet 77
17/04/04 11:17:42 DEBUG DFSClient: Waiting for ack for: 77
17/04/04 11:17:42 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 77 offsetInBlock: 393728 lastPacketInBlock: false lastByteOffsetInBlock: 403108
17/04/04 11:17:42 DEBUG DFSClient: DFSClient seqno: 77 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:17:42 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=78, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=402944
17/04/04 11:17:42 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=403108 lastFlushOffset=403108 createNewBlock=false
17/04/04 11:17:42 DEBUG DFSClient: Waiting for ack for: 77
[Stage 76:>                                                        (0 + 0) / 10]17/04/04 11:17:48 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=79, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=402944
17/04/04 11:17:48 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=403108 lastFlushOffset=403108 createNewBlock=false
17/04/04 11:17:48 DEBUG DFSClient: Waiting for ack for: 77
17/04/04 11:17:48 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=80, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=402944
17/04/04 11:17:48 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=403108 lastFlushOffset=403108 createNewBlock=false
17/04/04 11:17:48 DEBUG DFSClient: Waiting for ack for: 77
[Stage 76:>                                                        (0 + 4) / 10]17/04/04 11:17:52 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:17:52 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 76:=================>                                       (3 + 4) / 10][Stage 76:============================>                            (5 + 4) / 10][Stage 76:==================================>                      (6 + 4) / 10][Stage 76:=======================================>                 (7 + 3) / 10][Stage 76:=============================================>           (8 + 2) / 10][Stage 76:===================================================>     (9 + 1) / 10]17/04/04 11:18:00 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=81, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=402944
17/04/04 11:18:00 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=411500 lastFlushOffset=403108 createNewBlock=false
17/04/04 11:18:00 DEBUG DFSClient: Queued packet 81
17/04/04 11:18:00 DEBUG DFSClient: Waiting for ack for: 81
17/04/04 11:18:00 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 81 offsetInBlock: 402944 lastPacketInBlock: false lastByteOffsetInBlock: 411500
17/04/04 11:18:00 DEBUG DFSClient: DFSClient seqno: 81 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 77:>                                                        (0 + 4) / 10][Stage 77:======================>                                  (4 + 4) / 10][Stage 77:=============================================>           (8 + 2) / 10]17/04/04 11:18:10 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=82, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=411136
17/04/04 11:18:10 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=419489 lastFlushOffset=411500 createNewBlock=false
17/04/04 11:18:10 DEBUG DFSClient: Queued packet 82
17/04/04 11:18:10 DEBUG DFSClient: Waiting for ack for: 82
17/04/04 11:18:10 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 82 offsetInBlock: 411136 lastPacketInBlock: false lastByteOffsetInBlock: 419489
17/04/04 11:18:10 DEBUG DFSClient: DFSClient seqno: 82 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:18:10 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=83, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=419328
17/04/04 11:18:10 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=424950 lastFlushOffset=419489 createNewBlock=false
17/04/04 11:18:10 DEBUG DFSClient: Queued packet 83
17/04/04 11:18:10 DEBUG DFSClient: Waiting for ack for: 83
17/04/04 11:18:10 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 83 offsetInBlock: 419328 lastPacketInBlock: false lastByteOffsetInBlock: 424950
17/04/04 11:18:10 DEBUG DFSClient: DFSClient seqno: 83 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 79:>                                                        (0 + 4) / 10][Stage 79:======================>                                  (4 + 4) / 10][Stage 79:=======================================>                 (7 + 3) / 10][Stage 79:===================================================>     (9 + 1) / 10]17/04/04 11:18:11 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=84, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=424448
17/04/04 11:18:11 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=435440 lastFlushOffset=424950 createNewBlock=false
17/04/04 11:18:11 DEBUG DFSClient: Queued packet 84
17/04/04 11:18:11 DEBUG DFSClient: Waiting for ack for: 84
17/04/04 11:18:11 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 84 offsetInBlock: 424448 lastPacketInBlock: false lastByteOffsetInBlock: 435440
17/04/04 11:18:11 DEBUG DFSClient: DFSClient seqno: 84 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:18:12 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:18:12 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:18:12 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:18:12 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #22
17/04/04 11:18:12 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #22
17/04/04 11:18:12 DEBUG ProtobufRpcEngine: Call: renewLease took 5ms
17/04/04 11:18:12 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:18:12 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
[Stage 80:======================>                                  (4 + 4) / 10]17/04/04 11:18:12 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=85, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=435200
17/04/04 11:18:12 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=440662 lastFlushOffset=435440 createNewBlock=false
17/04/04 11:18:12 DEBUG DFSClient: Queued packet 85
17/04/04 11:18:12 DEBUG DFSClient: Waiting for ack for: 85
17/04/04 11:18:12 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 85 offsetInBlock: 435200 lastPacketInBlock: false lastByteOffsetInBlock: 440662
17/04/04 11:18:12 DEBUG DFSClient: DFSClient seqno: 85 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 81:>                                                        (0 + 4) / 10][Stage 81:======================>                                  (4 + 4) / 10][Stage 81:=============================================>           (8 + 2) / 10]17/04/04 11:18:15 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=86, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=440320
17/04/04 11:18:15 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=452065 lastFlushOffset=440662 createNewBlock=false
17/04/04 11:18:15 DEBUG DFSClient: Queued packet 86
17/04/04 11:18:15 DEBUG DFSClient: Waiting for ack for: 86
17/04/04 11:18:15 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 86 offsetInBlock: 440320 lastPacketInBlock: false lastByteOffsetInBlock: 452065
17/04/04 11:18:15 DEBUG DFSClient: DFSClient seqno: 86 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 82:>                                                        (0 + 4) / 10][Stage 82:======================>                                  (4 + 4) / 10][Stage 82:==================================>                      (6 + 4) / 10][Stage 82:=============================================>           (8 + 2) / 10]17/04/04 11:18:17 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=87, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=451584
17/04/04 11:18:17 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=457475 lastFlushOffset=452065 createNewBlock=false
17/04/04 11:18:17 DEBUG DFSClient: Queued packet 87
17/04/04 11:18:17 DEBUG DFSClient: Waiting for ack for: 87
17/04/04 11:18:17 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 87 offsetInBlock: 451584 lastPacketInBlock: false lastByteOffsetInBlock: 457475
17/04/04 11:18:17 DEBUG DFSClient: DFSClient seqno: 87 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 83:>                                                        (0 + 4) / 10]17/04/04 11:18:22 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:18:22 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/04/04 11:18:42 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:18:42 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:18:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:18:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #23
17/04/04 11:18:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #23
17/04/04 11:18:42 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/04/04 11:18:42 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:18:42 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
[Stage 83:======================>                                  (4 + 4) / 10]17/04/04 11:18:52 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:18:52 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/04/04 11:19:12 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:19:12 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:19:12 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:19:12 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #24
17/04/04 11:19:12 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #24
17/04/04 11:19:12 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/04/04 11:19:12 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:19:12 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
[Stage 83:=======================================>                 (7 + 3) / 10][Stage 83:=============================================>           (8 + 2) / 10]17/04/04 11:19:16 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=88, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=457216
17/04/04 11:19:16 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=469068 lastFlushOffset=457475 createNewBlock=false
17/04/04 11:19:16 DEBUG DFSClient: Queued packet 88
17/04/04 11:19:16 DEBUG DFSClient: Waiting for ack for: 88
17/04/04 11:19:16 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 88 offsetInBlock: 457216 lastPacketInBlock: false lastByteOffsetInBlock: 469068
17/04/04 11:19:16 WARN DFSClient: Slow ReadProcessor read fields took 59296ms (threshold=30000ms); ack: seqno: 88 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
[Stage 84:>                                                        (0 + 4) / 10][Stage 84:=====>                                                   (1 + 4) / 10][Stage 84:===========>                                             (2 + 4) / 10][Stage 84:======================>                                  (4 + 4) / 10]17/04/04 11:19:22 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:19:22 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 84:============================>                            (5 + 4) / 10][Stage 84:==================================>                      (6 + 4) / 10][Stage 84:=======================================>                 (7 + 3) / 10][Stage 84:=============================================>           (8 + 2) / 10][Stage 84:===================================================>     (9 + 1) / 10]17/04/04 11:19:31 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=89, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=468992
17/04/04 11:19:31 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=474707 lastFlushOffset=469068 createNewBlock=false
17/04/04 11:19:31 DEBUG DFSClient: Queued packet 89
17/04/04 11:19:31 DEBUG DFSClient: Waiting for ack for: 89
17/04/04 11:19:31 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 89 offsetInBlock: 468992 lastPacketInBlock: false lastByteOffsetInBlock: 474707
17/04/04 11:19:31 DEBUG DFSClient: DFSClient seqno: 89 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 85:>                                                        (0 + 4) / 10][Stage 85:=====>                                                   (1 + 4) / 10][Stage 85:============================>                            (5 + 4) / 10]17/04/04 11:19:42 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:19:42 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:19:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:19:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #25
17/04/04 11:19:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #25
17/04/04 11:19:42 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/04/04 11:19:42 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:19:42 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
[Stage 85:==================================>                      (6 + 4) / 10][Stage 85:=======================================>                 (7 + 3) / 10][Stage 85:===================================================>     (9 + 1) / 10]17/04/04 11:19:44 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=90, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=474624
17/04/04 11:19:44 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=486052 lastFlushOffset=474707 createNewBlock=false
17/04/04 11:19:44 DEBUG DFSClient: Queued packet 90
17/04/04 11:19:44 DEBUG DFSClient: Waiting for ack for: 90
17/04/04 11:19:44 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 90 offsetInBlock: 474624 lastPacketInBlock: false lastByteOffsetInBlock: 486052
17/04/04 11:19:44 DEBUG DFSClient: DFSClient seqno: 90 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 86:>                                                        (0 + 4) / 10][Stage 86:===========>                                             (2 + 4) / 10][Stage 86:======================>                                  (4 + 4) / 10]17/04/04 11:19:52 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:19:52 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 86:============================>                            (5 + 4) / 10][Stage 86:==================================>                      (6 + 4) / 10][Stage 86:=======================================>                 (7 + 3) / 10][Stage 86:=============================================>           (8 + 2) / 10][Stage 86:===================================================>     (9 + 1) / 10]17/04/04 11:20:03 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=91, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=485888
17/04/04 11:20:03 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=495720 lastFlushOffset=486052 createNewBlock=false
17/04/04 11:20:03 DEBUG DFSClient: Queued packet 91
17/04/04 11:20:03 DEBUG DFSClient: Waiting for ack for: 91
17/04/04 11:20:03 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 91 offsetInBlock: 485888 lastPacketInBlock: false lastByteOffsetInBlock: 495720
17/04/04 11:20:03 DEBUG DFSClient: DFSClient seqno: 91 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 87:>                                                        (0 + 4) / 10]17/04/04 11:20:12 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:20:12 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:20:12 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:20:12 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #26
17/04/04 11:20:12 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #26
17/04/04 11:20:12 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/04/04 11:20:12 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:20:12 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
[Stage 87:======================>                                  (4 + 4) / 10]17/04/04 11:20:22 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:20:22 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/04/04 11:20:36 WARN TaskSetManager: Lost task 4.0 in stage 87.2 (TID 479, 172.21.15.173, executor 2): java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:331)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:364)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:364)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:356)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:322)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:322)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1303)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:362)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:369)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)

17/04/04 11:20:36 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=92, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=495616
17/04/04 11:20:41 ERROR TaskSchedulerImpl: Lost executor 2 on 172.21.15.173: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/04/04 11:20:41 WARN TaskSetManager: Lost task 6.0 in stage 87.2 (TID 481, 172.21.15.173, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/04/04 11:20:41 WARN TaskSetManager: Lost task 5.0 in stage 87.2 (TID 480, 172.21.15.173, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/04/04 11:20:41 WARN TaskSetManager: Lost task 8.0 in stage 87.2 (TID 483, 172.21.15.173, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/04/04 11:20:41 WARN TaskSetManager: Lost task 9.0 in stage 87.2 (TID 484, 172.21.15.173, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/04/04 11:20:41 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=515151 lastFlushOffset=495720 createNewBlock=false
17/04/04 11:20:41 DEBUG DFSClient: Queued packet 92
17/04/04 11:20:41 DEBUG DFSClient: Waiting for ack for: 92
17/04/04 11:20:41 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 92 offsetInBlock: 495616 lastPacketInBlock: false lastByteOffsetInBlock: 515151
17/04/04 11:20:41 WARN DFSClient: Slow ReadProcessor read fields took 38273ms (threshold=30000ms); ack: seqno: 92 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
17/04/04 11:20:41 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=93, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=515072
17/04/04 11:20:41 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=515151 lastFlushOffset=515151 createNewBlock=false
17/04/04 11:20:41 DEBUG DFSClient: Waiting for ack for: 92
[Stage 87:======================>                                 (4 + -4) / 10]17/04/04 11:20:42 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:20:42 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:20:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:20:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #27
17/04/04 11:20:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #27
17/04/04 11:20:42 DEBUG ProtobufRpcEngine: Call: renewLease took 10ms
17/04/04 11:20:42 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:20:42 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
17/04/04 11:20:47 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=94, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=515072
17/04/04 11:20:47 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=515151 lastFlushOffset=515151 createNewBlock=false
17/04/04 11:20:47 DEBUG DFSClient: Waiting for ack for: 92
17/04/04 11:20:47 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=95, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=515072
17/04/04 11:20:47 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=515151 lastFlushOffset=515151 createNewBlock=false
17/04/04 11:20:47 DEBUG DFSClient: Waiting for ack for: 92
[Stage 87:======================>                                  (4 + 0) / 10]17/04/04 11:20:48 WARN TaskSetManager: Lost task 9.1 in stage 87.2 (TID 485, 172.21.15.173, executor 3): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=9, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/04/04 11:20:48 WARN TaskSetManager: Lost task 8.1 in stage 87.2 (TID 486, 172.21.15.173, executor 3): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=8, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/04/04 11:20:48 WARN TaskSetManager: Lost task 5.1 in stage 87.2 (TID 487, 172.21.15.173, executor 3): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=5, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/04/04 11:20:48 WARN TaskSetManager: Lost task 6.1 in stage 87.2 (TID 488, 172.21.15.173, executor 3): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=6, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/04/04 11:20:48 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=96, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=515072
17/04/04 11:20:48 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=520535 lastFlushOffset=515151 createNewBlock=false
17/04/04 11:20:48 DEBUG DFSClient: Queued packet 96
17/04/04 11:20:48 DEBUG DFSClient: Waiting for ack for: 96
17/04/04 11:20:48 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 96 offsetInBlock: 515072 lastPacketInBlock: false lastByteOffsetInBlock: 520535
17/04/04 11:20:48 WARN TaskSetManager: Lost task 0.1 in stage 87.2 (TID 489, 172.21.15.173, executor 3): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=0, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/04/04 11:20:48 DEBUG DFSClient: DFSClient seqno: 96 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 76:>                                                        (0 + 4) / 10]17/04/04 11:20:52 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:20:52 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 76:=================>                                       (3 + 4) / 10][Stage 76:======================>                                  (4 + 4) / 10][Stage 76:============================>                            (5 + 4) / 10][Stage 76:=======================================>                 (7 + 3) / 10][Stage 76:=============================================>           (8 + 2) / 10][Stage 76:===================================================>     (9 + 1) / 10]17/04/04 11:21:00 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=97, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=520192
17/04/04 11:21:00 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=532887 lastFlushOffset=520535 createNewBlock=false
17/04/04 11:21:00 DEBUG DFSClient: Queued packet 97
17/04/04 11:21:00 DEBUG DFSClient: Waiting for ack for: 97
17/04/04 11:21:00 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 97 offsetInBlock: 520192 lastPacketInBlock: false lastByteOffsetInBlock: 532887
17/04/04 11:21:00 DEBUG DFSClient: DFSClient seqno: 97 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 77:>                                                        (0 + 4) / 10][Stage 77:======================>                                  (4 + 4) / 10][Stage 77:=============================================>           (8 + 2) / 10]17/04/04 11:21:10 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=98, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=532480
17/04/04 11:21:10 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=541302 lastFlushOffset=532887 createNewBlock=false
17/04/04 11:21:10 DEBUG DFSClient: Queued packet 98
17/04/04 11:21:10 DEBUG DFSClient: Waiting for ack for: 98
17/04/04 11:21:10 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 98 offsetInBlock: 532480 lastPacketInBlock: false lastByteOffsetInBlock: 541302
17/04/04 11:21:10 DEBUG DFSClient: DFSClient seqno: 98 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:21:10 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=99, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=541184
17/04/04 11:21:10 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=546425 lastFlushOffset=541302 createNewBlock=false
17/04/04 11:21:10 DEBUG DFSClient: Queued packet 99
17/04/04 11:21:10 DEBUG DFSClient: Waiting for ack for: 99
17/04/04 11:21:10 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 99 offsetInBlock: 541184 lastPacketInBlock: false lastByteOffsetInBlock: 546425
17/04/04 11:21:10 DEBUG DFSClient: DFSClient seqno: 99 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 79:======================>                                  (4 + 4) / 10]17/04/04 11:21:12 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:21:12 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:21:12 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:21:12 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #28
17/04/04 11:21:12 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #28
17/04/04 11:21:12 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/04/04 11:21:12 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:21:12 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
[Stage 79:=============================================>           (8 + 2) / 10]17/04/04 11:21:12 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=100, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=546304
17/04/04 11:21:12 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=556386 lastFlushOffset=546425 createNewBlock=false
17/04/04 11:21:12 DEBUG DFSClient: Queued packet 100
17/04/04 11:21:12 DEBUG DFSClient: Waiting for ack for: 100
17/04/04 11:21:12 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 100 offsetInBlock: 546304 lastPacketInBlock: false lastByteOffsetInBlock: 556386
17/04/04 11:21:12 DEBUG DFSClient: DFSClient seqno: 100 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 80:=============================================>           (8 + 2) / 10]17/04/04 11:21:13 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=101, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=556032
17/04/04 11:21:13 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=561946 lastFlushOffset=556386 createNewBlock=false
17/04/04 11:21:13 DEBUG DFSClient: Queued packet 101
17/04/04 11:21:13 DEBUG DFSClient: Waiting for ack for: 101
17/04/04 11:21:13 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 101 offsetInBlock: 556032 lastPacketInBlock: false lastByteOffsetInBlock: 561946
17/04/04 11:21:13 DEBUG DFSClient: DFSClient seqno: 101 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 81:>                                                        (0 + 4) / 10][Stage 81:======================>                                  (4 + 4) / 10][Stage 81:=============================================>           (8 + 2) / 10]17/04/04 11:21:15 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=102, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=561664
17/04/04 11:21:15 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=573078 lastFlushOffset=561946 createNewBlock=false
17/04/04 11:21:15 DEBUG DFSClient: Queued packet 102
17/04/04 11:21:15 DEBUG DFSClient: Waiting for ack for: 102
17/04/04 11:21:15 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 102 offsetInBlock: 561664 lastPacketInBlock: false lastByteOffsetInBlock: 573078
17/04/04 11:21:15 DEBUG DFSClient: DFSClient seqno: 102 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 82:>                                                        (0 + 4) / 10][Stage 82:======================>                                  (4 + 4) / 10][Stage 82:=======================================>                 (7 + 3) / 10][Stage 82:=============================================>           (8 + 2) / 10]17/04/04 11:21:17 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=103, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=572928
17/04/04 11:21:17 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=578539 lastFlushOffset=573078 createNewBlock=false
17/04/04 11:21:17 DEBUG DFSClient: Queued packet 103
17/04/04 11:21:17 DEBUG DFSClient: Waiting for ack for: 103
17/04/04 11:21:17 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 103 offsetInBlock: 572928 lastPacketInBlock: false lastByteOffsetInBlock: 578539
17/04/04 11:21:17 DEBUG DFSClient: DFSClient seqno: 103 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 83:>                                                        (0 + 4) / 10]17/04/04 11:21:22 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:21:22 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/04/04 11:21:42 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:21:42 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:21:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:21:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #29
17/04/04 11:21:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #29
17/04/04 11:21:42 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/04/04 11:21:42 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:21:42 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
[Stage 83:======================>                                  (4 + 4) / 10]17/04/04 11:21:52 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:21:52 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 83:=======================================>                 (7 + 3) / 10][Stage 83:=============================================>           (8 + 2) / 10]17/04/04 11:22:12 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:22:12 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:22:12 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:22:12 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #30
17/04/04 11:22:12 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #30
17/04/04 11:22:12 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/04/04 11:22:12 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:22:12 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
17/04/04 11:22:15 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=104, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=578048
17/04/04 11:22:15 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=589557 lastFlushOffset=578539 createNewBlock=false
17/04/04 11:22:15 DEBUG DFSClient: Queued packet 104
17/04/04 11:22:15 DEBUG DFSClient: Waiting for ack for: 104
17/04/04 11:22:15 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 104 offsetInBlock: 578048 lastPacketInBlock: false lastByteOffsetInBlock: 589557
17/04/04 11:22:15 WARN DFSClient: Slow ReadProcessor read fields took 58433ms (threshold=30000ms); ack: seqno: 104 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
[Stage 84:>                                                        (0 + 4) / 10][Stage 84:=====>                                                   (1 + 4) / 10][Stage 84:===========>                                             (2 + 4) / 10][Stage 84:======================>                                  (4 + 4) / 10]17/04/04 11:22:22 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:22:22 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 84:============================>                            (5 + 4) / 10][Stage 84:==================================>                      (6 + 4) / 10][Stage 84:=======================================>                 (7 + 3) / 10][Stage 84:=============================================>           (8 + 2) / 10][Stage 84:===================================================>     (9 + 1) / 10]17/04/04 11:22:27 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=105, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=589312
17/04/04 11:22:27 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=595284 lastFlushOffset=589557 createNewBlock=false
17/04/04 11:22:27 DEBUG DFSClient: Queued packet 105
17/04/04 11:22:27 DEBUG DFSClient: Waiting for ack for: 105
17/04/04 11:22:27 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 105 offsetInBlock: 589312 lastPacketInBlock: false lastByteOffsetInBlock: 595284
17/04/04 11:22:27 DEBUG DFSClient: DFSClient seqno: 105 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 85:>                                                        (0 + 4) / 10][Stage 85:===========>                                             (2 + 4) / 10][Stage 85:=================>                                       (3 + 4) / 10][Stage 85:======================>                                  (4 + 4) / 10][Stage 85:============================>                            (5 + 4) / 10][Stage 85:==================================>                      (6 + 4) / 10][Stage 85:=======================================>                 (7 + 3) / 10][Stage 85:=============================================>           (8 + 2) / 10]17/04/04 11:22:39 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=106, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=594944
17/04/04 11:22:39 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=605788 lastFlushOffset=595284 createNewBlock=false
17/04/04 11:22:39 DEBUG DFSClient: Queued packet 106
17/04/04 11:22:39 DEBUG DFSClient: Waiting for ack for: 106
17/04/04 11:22:39 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 106 offsetInBlock: 594944 lastPacketInBlock: false lastByteOffsetInBlock: 605788
17/04/04 11:22:39 DEBUG DFSClient: DFSClient seqno: 106 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 86:>                                                        (0 + 4) / 10]17/04/04 11:22:42 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:22:42 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:22:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:22:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #31
17/04/04 11:22:42 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #31
17/04/04 11:22:42 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/04/04 11:22:42 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1859705979_1
17/04/04 11:22:42 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1859705979_1] with renew id 1 executed
[Stage 86:=====>                                                   (1 + 4) / 10][Stage 86:======================>                                  (4 + 4) / 10][Stage 86:============================>                            (5 + 4) / 10][Stage 86:=======================================>                 (7 + 3) / 10][Stage 86:=============================================>           (8 + 2) / 10][Stage 86:===================================================>     (9 + 1) / 10]17/04/04 11:22:52 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:22:52 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/04/04 11:22:52 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=107, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=605696
17/04/04 11:22:52 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=615631 lastFlushOffset=605788 createNewBlock=false
17/04/04 11:22:52 DEBUG DFSClient: Queued packet 107
17/04/04 11:22:52 DEBUG DFSClient: Waiting for ack for: 107
17/04/04 11:22:52 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 107 offsetInBlock: 605696 lastPacketInBlock: false lastByteOffsetInBlock: 615631
17/04/04 11:22:52 DEBUG DFSClient: DFSClient seqno: 107 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 87:>                                                        (0 + 4) / 10]17/04/04 11:23:10 WARN TaskSetManager: Lost task 2.0 in stage 87.3 (TID 602, 172.21.15.173, executor 3): java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:331)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:364)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:364)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:356)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:322)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:322)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1303)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:362)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:369)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)

17/04/04 11:23:10 WARN TaskSetManager: Lost task 1.0 in stage 87.3 (TID 601, 172.21.15.173, executor 3): java.io.FileNotFoundException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/39/rdd_14_6 (没有那个文件或目录)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:171)
	at org.apache.spark.storage.DiskStore.put(DiskStore.scala:54)
	at org.apache.spark.storage.DiskStore.putBytes(DiskStore.scala:76)
	at org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:1268)
	at org.apache.spark.storage.memory.MemoryStore.org$apache$spark$storage$memory$MemoryStore$$dropBlock$1(MemoryStore.scala:526)
	at org.apache.spark.storage.memory.MemoryStore$$anonfun$evictBlocksToFreeSpace$2.apply(MemoryStore.scala:547)
	at org.apache.spark.storage.memory.MemoryStore$$anonfun$evictBlocksToFreeSpace$2.apply(MemoryStore.scala:541)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.storage.memory.MemoryStore.evictBlocksToFreeSpace(MemoryStore.scala:541)
	at org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:92)
	at org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:73)
	at org.apache.spark.memory.UnifiedMemoryManager.acquireStorageMemory(UnifiedMemoryManager.scala:178)
	at org.apache.spark.memory.UnifiedMemoryManager.acquireUnrollMemory(UnifiedMemoryManager.scala:185)
	at org.apache.spark.storage.memory.MemoryStore.reserveUnrollMemoryForThisTask(MemoryStore.scala:584)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:223)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/04/04 11:23:10 WARN TaskSetManager: Lost task 3.0 in stage 87.3 (TID 603, 172.21.15.173, executor 3): java.io.FileNotFoundException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/3f/broadcast_59_piece0 (没有那个文件或目录)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:171)
	at org.apache.spark.storage.DiskStore.put(DiskStore.scala:54)
	at org.apache.spark.storage.DiskStore.putBytes(DiskStore.scala:76)
	at org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:1268)
	at org.apache.spark.storage.memory.MemoryStore.org$apache$spark$storage$memory$MemoryStore$$dropBlock$1(MemoryStore.scala:526)
	at org.apache.spark.storage.memory.MemoryStore$$anonfun$evictBlocksToFreeSpace$2.apply(MemoryStore.scala:547)
	at org.apache.spark.storage.memory.MemoryStore$$anonfun$evictBlocksToFreeSpace$2.apply(MemoryStore.scala:541)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.storage.memory.MemoryStore.evictBlocksToFreeSpace(MemoryStore.scala:541)
	at org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:92)
	at org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:73)
	at org.apache.spark.memory.UnifiedMemoryManager.acquireStorageMemory(UnifiedMemoryManager.scala:178)
	at org.apache.spark.memory.UnifiedMemoryManager.acquireUnrollMemory(UnifiedMemoryManager.scala:185)
	at org.apache.spark.storage.memory.MemoryStore.reserveUnrollMemoryForThisTask(MemoryStore.scala:584)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:223)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/04/04 11:23:11 WARN TaskSetManager: Lost task 4.0 in stage 87.3 (TID 604, 172.21.15.173, executor 3): FetchFailed(BlockManagerId(3, 172.21.15.173, 59248, None), shuffleId=3, mapId=1, reduceId=4, message=
org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/08/shuffle_3_1_0.data, offset=321364, length=80336}
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:356)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/08/shuffle_3_1_0.data, offset=321364, length=80336}
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:113)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:349)
	... 38 more
Caused by: java.io.FileNotFoundException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/08/shuffle_3_1_0.data (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:98)
	... 39 more

)
[Stage 87:>                                                        (0 + 5) / 10]17/04/04 11:23:11 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=108, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=615424
17/04/04 11:23:11 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=625745 lastFlushOffset=615631 createNewBlock=false
17/04/04 11:23:11 DEBUG DFSClient: Queued packet 108
17/04/04 11:23:11 DEBUG DFSClient: Waiting for ack for: 108
17/04/04 11:23:11 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 108 offsetInBlock: 615424 lastPacketInBlock: false lastByteOffsetInBlock: 625745
17/04/04 11:23:11 DEBUG DFSClient: DFSClient seqno: 108 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 87 (mapPartitions at VertexRDD.scala:356) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/08/shuffle_3_1_0.data, offset=321364, length=80336} 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:356) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58) 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32) 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39) 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439) 	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89) 	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115) 	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113) 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285) 	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54) 	at org.apache.spark.scheduler.Task.run(Task.scala:114) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) 	at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/08/shuffle_3_1_0.data, offset=321364, length=80336} 	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:113) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:349) 	... 38 more Caused by: java.io.FileNotFoundException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/08/shuffle_3_1_0.data (没有那个文件或目录) 	at java.io.FileInputStream.open(Native Method) 	at java.io.FileInputStream.<init>(FileInputStream.java:146) 	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:98) 	... 39 more 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1456)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1444)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1443)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1443)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1273)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1668)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1626)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1615)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2016)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2037)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2056)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2081)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1159)
	at src.main.scala.SVDPlusPlusApp$.main(SVDPlusPlusApp.scala:105)
	at src.main.scala.SVDPlusPlusApp.main(SVDPlusPlusApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:728)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
17/04/04 11:23:11 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=109, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=625664
17/04/04 11:23:11 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=631149 lastFlushOffset=625745 createNewBlock=false
17/04/04 11:23:11 DEBUG DFSClient: Queued packet 109
17/04/04 11:23:11 DEBUG DFSClient: Waiting for ack for: 109
17/04/04 11:23:11 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 109 offsetInBlock: 625664 lastPacketInBlock: false lastByteOffsetInBlock: 631149
17/04/04 11:23:11 DEBUG DFSClient: DFSClient seqno: 109 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:23:11 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=110, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=630784
17/04/04 11:23:11 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=631149 lastFlushOffset=631149 createNewBlock=false
17/04/04 11:23:11 DEBUG DFSClient: Waiting for ack for: 109
17/04/04 11:23:11 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=111, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=630784
17/04/04 11:23:11 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=631149 lastFlushOffset=631149 createNewBlock=false
17/04/04 11:23:11 DEBUG DFSClient: Waiting for ack for: 109
17/04/04 11:23:11 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=112, src=/eventLogs/app-20170404111041-0045.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=630784
17/04/04 11:23:11 DEBUG DFSClient: Queued packet 112
17/04/04 11:23:11 DEBUG DFSClient: Queued packet 113
17/04/04 11:23:11 DEBUG DFSClient: Waiting for ack for: 113
17/04/04 11:23:11 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 112 offsetInBlock: 630784 lastPacketInBlock: false lastByteOffsetInBlock: 632751
17/04/04 11:23:11 DEBUG DFSClient: DFSClient seqno: 112 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:23:11 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928 sending packet packet seqno: 113 offsetInBlock: 632751 lastPacketInBlock: true lastByteOffsetInBlock: 632751
17/04/04 11:23:11 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 59248, None),rdd_90_2,StorageLevel(1 replicas),0,0))
17/04/04 11:23:11 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 59248, None),rdd_90_3,StorageLevel(1 replicas),0,0))
17/04/04 11:23:11 WARN TaskSetManager: Lost task 2.1 in stage 87.3 (TID 605, 172.21.15.173, executor 3): FetchFailed(BlockManagerId(3, 172.21.15.173, 59248, None), shuffleId=2, mapId=1, reduceId=2, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:199)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:302)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:269)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:303)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:132)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	... 23 more

)
17/04/04 11:23:11 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(87,3,ShuffleMapTask,FetchFailed(BlockManagerId(3, 172.21.15.173, 59248, None),2,1,2,org.apache.spark.shuffle.FetchFailedException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:199)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:302)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:269)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:303)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:132)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	... 23 more
),org.apache.spark.scheduler.TaskInfo@513f39c,null)
17/04/04 11:23:11 WARN TaskSetManager: Lost task 3.1 in stage 87.3 (TID 607, 172.21.15.173, executor 3): FetchFailed(BlockManagerId(3, 172.21.15.173, 59248, None), shuffleId=2, mapId=1, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:199)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:302)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:269)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:303)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:132)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	... 23 more

)
17/04/04 11:23:11 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(87,3,ShuffleMapTask,FetchFailed(BlockManagerId(3, 172.21.15.173, 59248, None),2,1,3,org.apache.spark.shuffle.FetchFailedException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:199)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:302)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:269)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:303)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:132)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	... 23 more
),org.apache.spark.scheduler.TaskInfo@5df08f52,null)
17/04/04 11:23:11 DEBUG DFSClient: DFSClient seqno: 113 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/04/04 11:23:11 DEBUG DFSClient: Closing old block BP-519507147-172.21.15.90-1479901973323:blk_1073812750_71928
17/04/04 11:23:11 DEBUG Client: The ping interval is 60000 ms.
17/04/04 11:23:11 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/04/04 11:23:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/04/04 11:23:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #32
17/04/04 11:23:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #32
17/04/04 11:23:11 DEBUG ProtobufRpcEngine: Call: complete took 2ms
17/04/04 11:23:11 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockManagerAdded(1491276191522,BlockManagerId(3, 172.21.15.173, 59248, None),589824000)
17/04/04 11:23:11 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 59248, None),rdd_90_1,StorageLevel(1 replicas),0,0))
17/04/04 11:23:11 WARN TaskSetManager: Lost task 1.1 in stage 87.3 (TID 606, 172.21.15.173, executor 3): FetchFailed(BlockManagerId(3, 172.21.15.173, 59248, None), shuffleId=2, mapId=1, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:199)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:302)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:269)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:303)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:132)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	... 23 more

)
17/04/04 11:23:11 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(87,3,ShuffleMapTask,FetchFailed(BlockManagerId(3, 172.21.15.173, 59248, None),2,1,1,org.apache.spark.shuffle.FetchFailedException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:199)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:302)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:269)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:303)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:132)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	... 23 more
),org.apache.spark.scheduler.TaskInfo@950d712,null)
17/04/04 11:23:11 WARN TaskSetManager: Lost task 0.0 in stage 87.3 (TID 600, 172.21.15.173, executor 3): java.io.FileNotFoundException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/1f/temp_shuffle_128672a5-33e0-4df6-b8bc-c80ffc0fb85c (没有那个文件或目录)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:229)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:152)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/04/04 11:23:11 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(87,3,ShuffleMapTask,ExceptionFailure(java.io.FileNotFoundException,/tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/1f/temp_shuffle_128672a5-33e0-4df6-b8bc-c80ffc0fb85c (没有那个文件或目录),[Ljava.lang.StackTraceElement;@534ba4d2,java.io.FileNotFoundException: /tmp/spark-dbb2962c-edc8-46a3-8dd6-28d2019c6bbc/executor-40bad4af-9c97-4d71-806c-412a0e06f78b/blockmgr-7e133278-56f0-42d0-b78f-c705a1b48d84/1f/temp_shuffle_128672a5-33e0-4df6-b8bc-c80ffc0fb85c (没有那个文件或目录)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:229)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:152)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
,Some(org.apache.spark.ThrowableSerializationWrapper@b3201b7),Vector(AccumulableInfo(15818,Some(internal.metrics.executorRunTime),Some(19318),None,true,true,None), AccumulableInfo(15820,Some(internal.metrics.resultSize),Some(0),None,true,true,None), AccumulableInfo(15821,Some(internal.metrics.jvmGCTime),Some(12751),None,true,true,None), AccumulableInfo(15826,Some(internal.metrics.updatedBlockStatuses),Some([(broadcast_60_piece0,BlockStatus(StorageLevel(memory, 1 replicas),3388,0)), (broadcast_60,BlockStatus(StorageLevel(memory, deserialized, 1 replicas),6512,0)), (broadcast_49_piece0,BlockStatus(StorageLevel(disk, 1 replicas),0,3290)), (broadcast_0_piece0,BlockStatus(StorageLevel(disk, 1 replicas),0,22634)), (rdd_3_2,BlockStatus(StorageLevel(disk, 1 replicas),0,8686024)), (rdd_3_3,BlockStatus(StorageLevel(disk, 1 replicas),0,7810189)), (rdd_3_1,BlockStatus(StorageLevel(disk, 1 replicas),0,8618863)), (rdd_3_0,BlockStatus(StorageLevel(disk, 1 replicas),0,17955083)), (rdd_3_5,BlockStatus(StorageLevel(disk, 1 replicas),0,6377594)), (rdd_3_4,BlockStatus(StorageLevel(disk, 1 replicas),0,8604332)), (rdd_3_6,BlockStatus(StorageLevel(disk, 1 replicas),0,9312671)), (rdd_3_8,BlockStatus(StorageLevel(disk, 1 replicas),0,2400882)), (broadcast_49,BlockStatus(StorageLevel(disk, 1 replicas),0,3290)), (broadcast_0,BlockStatus(StorageLevel(disk, 1 replicas),0,22649)), (rdd_3_7,BlockStatus(StorageLevel(disk, 1 replicas),0,8603940)), (rdd_90_0,BlockStatus(StorageLevel(memory, deserialized, 1 replicas),105599608,0))]),None,true,true,None), AccumulableInfo(15827,Some(internal.metrics.shuffle.read.remoteBlocksFetched),Some(0),None,true,true,None), AccumulableInfo(15828,Some(internal.metrics.shuffle.read.localBlocksFetched),Some(50),None,true,true,None), AccumulableInfo(15829,Some(internal.metrics.shuffle.read.remoteBytesRead),Some(0),None,true,true,None), AccumulableInfo(15830,Some(internal.metrics.shuffle.read.localBytesRead),Some(187842396),None,true,true,None), AccumulableInfo(15831,Some(internal.metrics.shuffle.read.fetchWaitTime),Some(0),None,true,true,None), AccumulableInfo(15832,Some(internal.metrics.shuffle.read.recordsRead),Some(50),None,true,true,None), AccumulableInfo(15835,Some(internal.metrics.shuffle.write.writeTime),Some(239423),None,true,true,None), AccumulableInfo(15836,Some(internal.metrics.input.bytesRead),Some(23519608),None,true,true,None), AccumulableInfo(15837,Some(internal.metrics.input.recordsRead),Some(1),None,true,true,None)),Vector(LongAccumulator(id: 15818, name: Some(internal.metrics.executorRunTime), value: 19318), LongAccumulator(id: 15820, name: Some(internal.metrics.resultSize), value: 0), LongAccumulator(id: 15821, name: Some(internal.metrics.jvmGCTime), value: 12751), CollectionAccumulator(id: 15826, name: Some(internal.metrics.updatedBlockStatuses), value: [(broadcast_60_piece0,BlockStatus(StorageLevel(memory, 1 replicas),3388,0)), (broadcast_60,BlockStatus(StorageLevel(memory, deserialized, 1 replicas),6512,0)), (broadcast_49_piece0,BlockStatus(StorageLevel(disk, 1 replicas),0,3290)), (broadcast_0_piece0,BlockStatus(StorageLevel(disk, 1 replicas),0,22634)), (rdd_3_2,BlockStatus(StorageLevel(disk, 1 replicas),0,8686024)), (rdd_3_3,BlockStatus(StorageLevel(disk, 1 replicas),0,7810189)), (rdd_3_1,BlockStatus(StorageLevel(disk, 1 replicas),0,8618863)), (rdd_3_0,BlockStatus(StorageLevel(disk, 1 replicas),0,17955083)), (rdd_3_5,BlockStatus(StorageLevel(disk, 1 replicas),0,6377594)), (rdd_3_4,BlockStatus(StorageLevel(disk, 1 replicas),0,8604332)), (rdd_3_6,BlockStatus(StorageLevel(disk, 1 replicas),0,9312671)), (rdd_3_8,BlockStatus(StorageLevel(disk, 1 replicas),0,2400882)), (broadcast_49,BlockStatus(StorageLevel(disk, 1 replicas),0,3290)), (broadcast_0,BlockStatus(StorageLevel(disk, 1 replicas),0,22649)), (rdd_3_7,BlockStatus(StorageLevel(disk, 1 replicas),0,8603940)), (rdd_90_0,BlockStatus(StorageLevel(memory, deserialized, 1 replicas),105599608,0))]), LongAccumulator(id: 15827, name: Some(internal.metrics.shuffle.read.remoteBlocksFetched), value: 0), LongAccumulator(id: 15828, name: Some(internal.metrics.shuffle.read.localBlocksFetched), value: 50), LongAccumulator(id: 15829, name: Some(internal.metrics.shuffle.read.remoteBytesRead), value: 0), LongAccumulator(id: 15830, name: Some(internal.metrics.shuffle.read.localBytesRead), value: 187842396), LongAccumulator(id: 15831, name: Some(internal.metrics.shuffle.read.fetchWaitTime), value: 0), LongAccumulator(id: 15832, name: Some(internal.metrics.shuffle.read.recordsRead), value: 50), LongAccumulator(id: 15835, name: Some(internal.metrics.shuffle.write.writeTime), value: 239423), LongAccumulator(id: 15836, name: Some(internal.metrics.input.bytesRead), value: 23519608), LongAccumulator(id: 15837, name: Some(internal.metrics.input.recordsRead), value: 1))),org.apache.spark.scheduler.TaskInfo@e9b1f,org.apache.spark.executor.TaskMetrics@3eea1040)
17/04/04 11:23:11 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 59248, None),rdd_3_1,StorageLevel(1 replicas),0,0))
17/04/04 11:23:11 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 59248, None),broadcast_52_piece0,StorageLevel(memory, 1 replicas),3106,0))
17/04/04 11:23:11 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 59248, None),rdd_23_0,StorageLevel(memory, deserialized, 1 replicas),23519608,0))
17/04/04 11:23:11 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 59248, None),rdd_84_8,StorageLevel(memory, deserialized, 1 replicas),7053104,0))
17/04/04 11:23:11 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 59248, None),broadcast_0_piece0,StorageLevel(1 replicas),0,0))
17/04/04 11:23:11 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 59248, None),broadcast_60_piece0,StorageLevel(memory, 1 replicas),3388,0))
17/04/04 11:23:11 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 59248, None),broadcast_56_piece0,StorageLevel(memory, 1 replicas),3752,0))
17/04/04 11:23:11 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 59248, None),rdd_23_3,StorageLevel(memory, deserialized, 1 replicas),23536472,0))
17/04/04 11:23:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #33
17/04/04 11:23:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #33
17/04/04 11:23:11 DEBUG ProtobufRpcEngine: Call: complete took 23ms
17/04/04 11:23:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #34
17/04/04 11:23:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #34
17/04/04 11:23:11 DEBUG ProtobufRpcEngine: Call: getFileInfo took 2ms
17/04/04 11:23:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #35
17/04/04 11:23:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #35
17/04/04 11:23:11 DEBUG ProtobufRpcEngine: Call: rename took 9ms
17/04/04 11:23:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop sending #36
17/04/04 11:23:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop got value #36
17/04/04 11:23:11 DEBUG ProtobufRpcEngine: Call: setTimes took 8ms
17/04/04 11:23:11 DEBUG PoolThreadCache: Freed 16 thread-local buffer(s) from thread: shuffle-server-6-4
17/04/04 11:23:11 DEBUG PoolThreadCache: Freed 16 thread-local buffer(s) from thread: shuffle-server-6-3
17/04/04 11:23:11 DEBUG PoolThreadCache: Freed 33 thread-local buffer(s) from thread: shuffle-server-6-2
17/04/04 11:23:11 DEBUG PoolThreadCache: Freed 33 thread-local buffer(s) from thread: rpc-server-3-2
17/04/04 11:23:11 DEBUG PoolThreadCache: Freed 27 thread-local buffer(s) from thread: rpc-server-3-3
17/04/04 11:23:11 DEBUG Client: stopping client from cache: org.apache.hadoop.ipc.Client@2209ad9a
17/04/04 11:23:11 DEBUG Client: removing client from cache: org.apache.hadoop.ipc.Client@2209ad9a
17/04/04 11:23:11 DEBUG Client: stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@2209ad9a
17/04/04 11:23:11 DEBUG Client: Stopping client
17/04/04 11:23:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/04/04 11:23:11 DEBUG Client: IPC Client (1268233048) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
