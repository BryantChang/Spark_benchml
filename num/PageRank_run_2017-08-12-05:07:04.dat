17/08/12 05:07:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/08/12 05:07:07 WARN SparkConf: Detected deprecated memory fraction settings: [spark.storage.memoryFraction]. As of Spark 1.6, execution and storage memory management are unified. All memory fractions used in the old model are now deprecated and no longer read. If you wish to use the old memory management, you may explicitly enable `spark.memory.useLegacyMode` (not recommended).
17/08/12 05:07:08 INFO log: Logging initialized @2966ms
17/08/12 05:07:08 INFO Server: jetty-9.3.z-SNAPSHOT
17/08/12 05:07:08 INFO Server: Started @3063ms
17/08/12 05:07:08 INFO AbstractConnector: Started ServerConnector@26b894bd{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@62da83ed{/jobs,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@499b2a5c{/jobs/json,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@c1fca1e{/jobs/job,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2db2cd5{/jobs/job/json,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@615f972{/stages,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@73393584{/stages/json,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1827a871{/stages/stage,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@66238be2{/stages/stage/json,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@200606de{/stages/pool,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@f8908f6{/stages/pool/json,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2ef8a8c3{/storage,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@63fd4873{/storage/json,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7544a1e4{/storage/rdd,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7957dc72{/storage/rdd/json,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3aacf32a{/environment,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@82c57b3{/environment/json,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@600b0b7{/executors,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5ea502e0{/executors/json,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@473b3b7a{/executors/threadDump,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@77b7ffa4{/executors/threadDump/json,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@402f80f5{/static,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3113a37{/,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4e9658b5{/api,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@54ec8cc9{/jobs/job/kill,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5528a42c{/stages/stage/kill,null,AVAILABLE,@Spark}
17/08/12 05:07:08 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2a037324{/metrics/json,null,AVAILABLE,@Spark}
17/08/12 05:07:10 INFO FileInputFormat: Total input paths to process : 10
17/08/12 05:07:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[Stage 0:>                                                        (0 + 10) / 10][Stage 0:=====>                                                    (1 + 9) / 10][Stage 0:===========>                                              (2 + 8) / 10][Stage 0:=================>                                        (3 + 7) / 10][Stage 0:=======================>                                  (4 + 6) / 10]17/08/12 05:08:12 WARN DFSClient: Slow ReadProcessor read fields took 60042ms (threshold=30000ms); ack: seqno: -2 reply: SUCCESS reply: SUCCESS reply: ERROR downstreamAckTimeNanos: 0 flag: 0 flag: 0 flag: 1, targets: [DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK], DatanodeInfoWithStorage[192.168.4.45:50010,DS-3c29c839-4349-4136-8c4b-70878f4cafc5,DISK]]
17/08/12 05:08:12 WARN DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-1696099266-192.168.4.14-1468972266503:blk_1073745952_5142
java.io.IOException: Bad response ERROR for block BP-1696099266-192.168.4.14-1468972266503:blk_1073745952_5142 from datanode DatanodeInfoWithStorage[192.168.4.45:50010,DS-3c29c839-4349-4136-8c4b-70878f4cafc5,DISK]
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:765)
17/08/12 05:08:12 WARN DFSClient: Error Recovery for block BP-1696099266-192.168.4.14-1468972266503:blk_1073745952_5142 in pipeline DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK], DatanodeInfoWithStorage[192.168.4.45:50010,DS-3c29c839-4349-4136-8c4b-70878f4cafc5,DISK]: bad datanode DatanodeInfoWithStorage[192.168.4.45:50010,DS-3c29c839-4349-4136-8c4b-70878f4cafc5,DISK]
17/08/12 05:08:12 WARN DFSClient: DataStreamer Exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]], original=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[Stage 0:=============================>                            (5 + 5) / 10][Stage 0:==================================>                       (6 + 4) / 10][Stage 0:========================================>                 (7 + 3) / 10][Stage 0:==============================================>           (8 + 2) / 10][Stage 0:====================================================>     (9 + 1) / 10]17/08/12 05:08:24 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]], original=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[Stage 1:>                                                        (0 + 10) / 10][Stage 1:=====>                                                    (1 + 9) / 10][Stage 1:===========>                                              (2 + 8) / 10][Stage 1:=================>                                        (3 + 7) / 10][Stage 1:=============================>                            (5 + 5) / 10][Stage 1:==================================>                       (6 + 4) / 10][Stage 1:========================================>                 (7 + 3) / 10][Stage 1:==============================================>           (8 + 2) / 10][Stage 1:====================================================>     (9 + 1) / 10]17/08/12 05:08:50 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]], original=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[Stage 2:>                                                        (0 + 12) / 15][Stage 2:>                                                        (0 + 15) / 15][Stage 2:===>                                                     (1 + 14) / 15][Stage 2:=======>                                                 (2 + 13) / 15][Stage 2:===========>                                             (3 + 12) / 15][Stage 2:===============>                                         (4 + 11) / 15][Stage 2:===========================>                              (7 + 8) / 15][Stage 2:==================================>                       (9 + 6) / 15][Stage 2:=============================================>           (12 + 3) / 15][Stage 2:=================================================>       (13 + 2) / 15]17/08/12 05:09:10 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]], original=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[Stage 3:>                                                        (0 + 15) / 15][Stage 3:===>                                                     (1 + 14) / 15][Stage 3:===========>                                             (3 + 12) / 15][Stage 3:===============>                                         (4 + 11) / 15][Stage 3:=======================>                                  (6 + 9) / 15][Stage 3:===========================>                              (7 + 8) / 15][Stage 3:==============================>                           (8 + 7) / 15][Stage 3:==================================>                       (9 + 6) / 15][Stage 3:======================================>                  (10 + 5) / 15][Stage 3:=============================================>           (12 + 3) / 15][Stage 3:=================================================>       (13 + 2) / 15][Stage 3:=====================================================>   (14 + 1) / 15]17/08/12 05:09:20 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]], original=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[Stage 4:>                                                        (0 + 15) / 15][Stage 4:===>                                                     (1 + 14) / 15][Stage 4:=======>                                                 (2 + 13) / 15][Stage 4:===========>                                             (3 + 12) / 15][Stage 4:===============>                                         (4 + 11) / 15][Stage 4:===================>                                     (5 + 10) / 15][Stage 4:=======================>                                  (6 + 9) / 15][Stage 4:===========================>                              (7 + 8) / 15][Stage 4:==============================>                           (8 + 7) / 15][Stage 4:==================================>                       (9 + 6) / 15][Stage 4:======================================>                  (10 + 5) / 15][Stage 4:=============================================>           (12 + 3) / 15]17/08/12 05:09:28 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]], original=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[Stage 5:>                                                        (0 + 15) / 15][Stage 5:===>                                                     (1 + 14) / 15][Stage 5:=======>                                                 (2 + 13) / 15][Stage 5:===========>                                             (3 + 12) / 15][Stage 5:===================>                                     (5 + 10) / 15][Stage 5:===========================>                              (7 + 8) / 15][Stage 5:==================================>                       (9 + 6) / 15][Stage 5:======================================>                  (10 + 5) / 15][Stage 5:=============================================>           (12 + 3) / 15][Stage 5:=================================================>       (13 + 2) / 15][Stage 5:=====================================================>   (14 + 1) / 15]17/08/12 05:09:37 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]], original=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[Stage 6:>                                                        (0 + 15) / 15][Stage 6:===>                                                     (1 + 14) / 15][Stage 6:=======>                                                 (2 + 13) / 15][Stage 6:===========>                                             (3 + 12) / 15][Stage 6:===========================>                              (7 + 8) / 15][Stage 6:==============================>                           (8 + 7) / 15][Stage 6:======================================>                  (10 + 5) / 15][Stage 6:=========================================>               (11 + 4) / 15][Stage 6:=====================================================>   (14 + 1) / 15]17/08/12 05:09:46 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]], original=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[Stage 7:>                                                        (0 + 15) / 15][Stage 7:===>                                                     (1 + 14) / 15][Stage 7:=======>                                                 (2 + 13) / 15][Stage 7:===========>                                             (3 + 12) / 15][Stage 7:===================>                                     (5 + 10) / 15][Stage 7:===========================>                              (7 + 8) / 15][Stage 7:==================================>                       (9 + 6) / 15][Stage 7:=============================================>           (12 + 3) / 15][Stage 7:=====================================================>   (14 + 1) / 15]17/08/12 05:09:53 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]], original=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[Stage 8:>                                                        (0 + 15) / 15][Stage 8:=======>                                                 (2 + 13) / 15][Stage 8:===========>                                             (3 + 12) / 15][Stage 8:===================>                                     (5 + 10) / 15][Stage 8:===========================>                              (7 + 8) / 15][Stage 8:==============================>                           (8 + 7) / 15][Stage 8:==================================>                       (9 + 6) / 15][Stage 8:=========================================>               (11 + 4) / 15][Stage 8:=================================================>       (13 + 2) / 15][Stage 8:=====================================================>   (14 + 1) / 15]17/08/12 05:10:01 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]], original=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[Stage 9:>                                                        (0 + 15) / 15][Stage 9:===>                                                     (1 + 14) / 15][Stage 9:===========>                                             (3 + 12) / 15][Stage 9:=======================>                                  (6 + 9) / 15][Stage 9:==============================>                           (8 + 7) / 15][Stage 9:======================================>                  (10 + 5) / 15][Stage 9:=========================================>               (11 + 4) / 15][Stage 9:=====================================================>   (14 + 1) / 15]17/08/12 05:10:08 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]], original=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[Stage 10:>                                                       (0 + 15) / 15][Stage 10:===>                                                    (1 + 14) / 15][Stage 10:=======>                                                (2 + 13) / 15][Stage 10:===========>                                            (3 + 12) / 15][Stage 10:==============>                                         (4 + 11) / 15][Stage 10:======================>                                  (6 + 9) / 15][Stage 10:==================================>                      (9 + 6) / 15][Stage 10:=====================================>                  (10 + 5) / 15][Stage 10:=========================================>              (11 + 4) / 15]17/08/12 05:10:17 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]], original=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[Stage 11:>                                                        (0 + 0) / 15][Stage 11:>                                                       (0 + 15) / 15][Stage 11:===>                                                    (1 + 14) / 15][Stage 11:=======>                                                (2 + 13) / 15][Stage 11:===========>                                            (3 + 12) / 15][Stage 11:==============>                                         (4 + 11) / 15][Stage 11:==================>                                     (5 + 10) / 15][Stage 11:======================>                                  (6 + 9) / 15][Stage 11:==========================>                              (7 + 8) / 15][Stage 11:==============================>                          (8 + 7) / 15][Stage 11:==================================>                      (9 + 6) / 15][Stage 11:================================================>       (13 + 2) / 15]17/08/12 05:10:24 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]], original=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[Stage 12:>                                                       (0 + 15) / 15][Stage 12:===========>                                            (3 + 12) / 15][Stage 12:==============>                                         (4 + 11) / 15][Stage 12:==========================>                              (7 + 8) / 15][Stage 12:==================================>                      (9 + 6) / 15][Stage 12:================================================>       (13 + 2) / 15][Stage 12:====================================================>   (14 + 1) / 15]17/08/12 05:10:32 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]], original=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[Stage 13:>                                                       (0 + 15) / 15][Stage 13:=======>                                                (2 + 13) / 15][Stage 13:===========>                                            (3 + 12) / 15][Stage 13:==============>                                         (4 + 11) / 15][Stage 13:==================>                                     (5 + 10) / 15][Stage 13:==========================>                              (7 + 8) / 15][Stage 13:==================================>                      (9 + 6) / 15][Stage 13:=========================================>              (11 + 4) / 15][Stage 13:============================================>           (12 + 3) / 15][Stage 13:================================================>       (13 + 2) / 15][Stage 13:====================================================>   (14 + 1) / 15]17/08/12 05:10:39 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]], original=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
[Stage 14:>                                                        (0 + 0) / 15][Stage 14:>                                                       (0 + 15) / 15][Stage 14:==================>                                     (5 + 10) / 15][Stage 14:=====================================>                  (10 + 5) / 15]17/08/12 05:10:42 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]], original=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
                                                                                17/08/12 05:10:42 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]], original=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/08/12 05:10:42 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]], original=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
17/08/12 05:10:42 INFO AbstractConnector: Stopped Spark@26b894bd{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17/08/12 05:10:42 ERROR Utils: Uncaught exception in thread main
java.lang.IllegalArgumentException: Self-suppression not permitted
	at java.lang.Throwable.addSuppressed(Throwable.java:1043)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:159)
	at sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:320)
	at sun.nio.cs.StreamEncoder.close(StreamEncoder.java:149)
	at java.io.OutputStreamWriter.close(OutputStreamWriter.java:233)
	at java.io.BufferedWriter.close(BufferedWriter.java:266)
	at java.io.PrintWriter.close(PrintWriter.java:339)
	at org.apache.spark.scheduler.EventLoggingListener$$anonfun$stop$1.apply(EventLoggingListener.scala:230)
	at org.apache.spark.scheduler.EventLoggingListener$$anonfun$stop$1.apply(EventLoggingListener.scala:230)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.EventLoggingListener.stop(EventLoggingListener.scala:230)
	at org.apache.spark.SparkContext$$anonfun$stop$7$$anonfun$apply$mcV$sp$5.apply(SparkContext.scala:1917)
	at org.apache.spark.SparkContext$$anonfun$stop$7$$anonfun$apply$mcV$sp$5.apply(SparkContext.scala:1917)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext$$anonfun$stop$7.apply$mcV$sp(SparkContext.scala:1917)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1916)
	at src.main.scala.SimplePageRank$.main(SimplePageRank.scala:66)
	at src.main.scala.SimplePageRank.main(SimplePageRank.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]], original=[DatanodeInfoWithStorage[192.168.4.16:50010,DS-bc96f566-3132-482f-a45a-68ec8803d03f,DISK], DatanodeInfoWithStorage[192.168.4.17:50010,DS-d5cf341a-3862-4780-86ee-f81b00985f0b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
