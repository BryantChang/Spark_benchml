Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/12/02 15:33:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/12/02 15:33:14 WARN SparkConf: Detected deprecated memory fraction settings: [spark.storage.memoryFraction]. As of Spark 1.6, execution and storage memory management are unified. All memory fractions used in the old model are now deprecated and no longer read. If you wish to use the old memory management, you may explicitly enable `spark.memory.useLegacyMode` (not recommended).
16/12/02 15:33:15 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
16/12/02 15:33:15 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
16/12/02 15:33:15 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
16/12/02 15:33:15 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
16/12/02 15:33:15 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
16/12/02 15:33:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[Stage 0:>                                                         (0 + 0) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10]16/12/02 15:36:14 WARN HeartbeatReceiver: Removing executor 0 with no recent heartbeats: 178506 ms exceeds timeout 120000 ms
16/12/02 15:36:14 ERROR TaskSchedulerImpl: Lost executor 0 on 172.21.15.173: Executor heartbeat timed out after 178506 ms
16/12/02 15:36:14 WARN TaskSetManager: Lost task 2.0 in stage 0.0 (TID 2, 172.21.15.173): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 178506 ms
16/12/02 15:36:14 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, 172.21.15.173): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 178506 ms
16/12/02 15:36:14 WARN TaskSetManager: Lost task 3.0 in stage 0.0 (TID 3, 172.21.15.173): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 178506 ms
16/12/02 15:36:14 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, 172.21.15.173): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 178506 ms
[Stage 0:>                                                         (0 + 3) / 10][Stage 0:>                                                         (0 + 0) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10]16/12/02 15:37:28 WARN TaskSetManager: Lost task 0.1 in stage 0.0 (TID 4, 172.21.15.173): java.lang.OutOfMemoryError: Java heap space
	at java.lang.reflect.Array.newInstance(Array.java:70)
	at scala.reflect.ClassTag$class.newArray(ClassTag.scala:61)
	at scala.reflect.ClassTag$$anon$1.newArray(ClassTag.scala:152)
	at org.apache.spark.util.collection.PrimitiveVector.copyArrayWithLength(PrimitiveVector.scala:87)
	at org.apache.spark.util.collection.PrimitiveVector.resize(PrimitiveVector.scala:74)
	at org.apache.spark.util.collection.PrimitiveVector.$plus$eq(PrimitiveVector.scala:41)
	at org.apache.spark.graphx.impl.EdgePartitionBuilder.add(EdgePartitionBuilder.scala:34)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1$$anonfun$apply$1.apply(EdgeRDD.scala:108)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1$$anonfun$apply$1.apply(EdgeRDD.scala:107)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:107)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:105)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)

16/12/02 15:37:30 WARN TaskSetManager: Lost task 1.1 in stage 0.0 (TID 6, 172.21.15.173): java.lang.OutOfMemoryError: GC overhead limit exceeded
	at org.apache.spark.graphx.util.GraphGenerators$$anonfun$generateRandomEdges$1.apply(GraphGenerators.scala:85)
	at org.apache.spark.graphx.util.GraphGenerators$$anonfun$generateRandomEdges$1.apply(GraphGenerators.scala:85)
	at scala.Array$.fill(Array.scala:267)
	at org.apache.spark.graphx.util.GraphGenerators$.generateRandomEdges(GraphGenerators.scala:85)
	at org.apache.spark.graphx.util.GraphGenerators$$anonfun$3.apply(GraphGenerators.scala:72)
	at org.apache.spark.graphx.util.GraphGenerators$$anonfun$3.apply(GraphGenerators.scala:71)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:107)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:105)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)

