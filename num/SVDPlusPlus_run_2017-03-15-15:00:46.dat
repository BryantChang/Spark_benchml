17/03/15 15:00:46 WARN SparkContext: Support for Java 7 is deprecated as of Spark 2.0.0
17/03/15 15:00:46 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)], about=, always=false, type=DEFAULT, sampleName=Ops)
17/03/15 15:00:46 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)], about=, always=false, type=DEFAULT, sampleName=Ops)
17/03/15 15:00:46 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, value=[GetGroups], about=, always=false, type=DEFAULT, sampleName=Ops)
17/03/15 15:00:46 DEBUG MetricsSystemImpl: UgiMetrics, User and group related metrics
17/03/15 15:00:47 DEBUG Shell: setsid exited with exit code 0
17/03/15 15:00:47 DEBUG Groups:  Creating new Groups object
17/03/15 15:00:47 DEBUG NativeCodeLoader: Trying to load the custom-built native-hadoop library...
17/03/15 15:00:47 DEBUG NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
17/03/15 15:00:47 DEBUG NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
17/03/15 15:00:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/03/15 15:00:47 DEBUG PerformanceAdvisory: Falling back to shell based
17/03/15 15:00:47 DEBUG JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
17/03/15 15:00:47 DEBUG Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
17/03/15 15:00:47 DEBUG UserGroupInformation: hadoop login
17/03/15 15:00:47 DEBUG UserGroupInformation: hadoop login commit
17/03/15 15:00:47 DEBUG UserGroupInformation: using local user:UnixPrincipal: hadoop
17/03/15 15:00:47 DEBUG UserGroupInformation: Using user: "UnixPrincipal: hadoop" with name hadoop
17/03/15 15:00:47 DEBUG UserGroupInformation: User entry: "hadoop"
17/03/15 15:00:47 DEBUG UserGroupInformation: UGI loginUser:hadoop (auth:SIMPLE)
17/03/15 15:00:47 WARN SparkConf: Detected deprecated memory fraction settings: [spark.storage.memoryFraction]. As of Spark 1.6, execution and storage memory management are unified. All memory fractions used in the old model are now deprecated and no longer read. If you wish to use the old memory management, you may explicitly enable `spark.memory.useLegacyMode` (not recommended).
17/03/15 15:00:47 WARN SparkConf: 
SPARK_CLASSPATH was detected (set to '/home/hadoop/bryantchang/platforms/spark2.0/lib/mysql-connector-java-5.1.40-bin.jar:').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with --driver-class-path to augment the driver classpath
 - spark.executor.extraClassPath to augment the executor classpath
        
17/03/15 15:00:47 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/home/hadoop/bryantchang/platforms/spark2.0/lib/mysql-connector-java-5.1.40-bin.jar:' as a work-around.
17/03/15 15:00:47 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/home/hadoop/bryantchang/platforms/spark2.0/lib/mysql-connector-java-5.1.40-bin.jar:' as a work-around.
17/03/15 15:00:47 DEBUG InternalLoggerFactory: Using SLF4J as the default logging framework
17/03/15 15:00:47 DEBUG PlatformDependent0: java.nio.Buffer.address: available
17/03/15 15:00:47 DEBUG PlatformDependent0: sun.misc.Unsafe.theUnsafe: available
17/03/15 15:00:47 DEBUG PlatformDependent0: sun.misc.Unsafe.copyMemory: available
17/03/15 15:00:47 DEBUG PlatformDependent0: direct buffer constructor: available
17/03/15 15:00:47 DEBUG PlatformDependent0: java.nio.Bits.unaligned: available, true
17/03/15 15:00:47 DEBUG PlatformDependent0: java.nio.DirectByteBuffer.<init>(long, int): available
17/03/15 15:00:47 DEBUG Cleaner0: java.nio.ByteBuffer.cleaner(): available
17/03/15 15:00:47 DEBUG PlatformDependent: Java version: 7
17/03/15 15:00:47 DEBUG PlatformDependent: -Dio.netty.noUnsafe: false
17/03/15 15:00:47 DEBUG PlatformDependent: sun.misc.Unsafe: available
17/03/15 15:00:47 DEBUG PlatformDependent: -Dio.netty.noJavassist: false
17/03/15 15:00:47 DEBUG PlatformDependent: Javassist: available
17/03/15 15:00:47 DEBUG PlatformDependent: -Dio.netty.tmpdir: /tmp (java.io.tmpdir)
17/03/15 15:00:47 DEBUG PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)
17/03/15 15:00:47 DEBUG PlatformDependent: -Dio.netty.noPreferDirect: false
17/03/15 15:00:47 DEBUG PlatformDependent: io.netty.maxDirectMemory: 954728448 bytes
17/03/15 15:00:47 DEBUG JavassistTypeParameterMatcherGenerator: Generated: io.netty.util.internal.__matchers__.org.apache.spark.network.protocol.MessageMatcher
17/03/15 15:00:47 DEBUG JavassistTypeParameterMatcherGenerator: Generated: io.netty.util.internal.__matchers__.io.netty.buffer.ByteBufMatcher
17/03/15 15:00:47 DEBUG MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 8
17/03/15 15:00:47 DEBUG NioEventLoop: -Dio.netty.noKeySetOptimization: false
17/03/15 15:00:47 DEBUG NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512
17/03/15 15:00:47 DEBUG PlatformDependent: org.jctools-core.MpscChunkedArrayQueue: available
17/03/15 15:00:47 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 8
17/03/15 15:00:47 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 8
17/03/15 15:00:47 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192
17/03/15 15:00:47 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 11
17/03/15 15:00:47 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 16777216
17/03/15 15:00:47 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.tinyCacheSize: 512
17/03/15 15:00:47 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256
17/03/15 15:00:47 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64
17/03/15 15:00:47 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768
17/03/15 15:00:47 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192
17/03/15 15:00:47 DEBUG ThreadLocalRandom: -Dio.netty.initialSeedUniquifier: 0x701bb655209d3bd5 (took 0 ms)
17/03/15 15:00:47 DEBUG ByteBufUtil: -Dio.netty.allocator.type: unpooled
17/03/15 15:00:47 DEBUG ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 65536
17/03/15 15:00:47 DEBUG ByteBufUtil: -Dio.netty.maxThreadLocalCharBufferSize: 16384
17/03/15 15:00:47 DEBUG NetUtil: Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%1)
17/03/15 15:00:47 DEBUG NetUtil: /proc/sys/net/core/somaxconn: 128
17/03/15 15:00:47 DEBUG AbstractByteBuf: -Dio.netty.buffer.bytebuf.checkAccessible: true
17/03/15 15:00:47 DEBUG ResourceLeakDetector: -Dio.netty.leakDetection.level: simple
17/03/15 15:00:47 DEBUG ResourceLeakDetector: -Dio.netty.leakDetection.maxRecords: 4
17/03/15 15:00:47 DEBUG ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: io.netty.util.ResourceLeakDetector@3c116de6
17/03/15 15:00:47 DEBUG Recycler: -Dio.netty.recycler.maxCapacity.default: 32768
17/03/15 15:00:47 DEBUG Recycler: -Dio.netty.recycler.maxSharedCapacityFactor: 2
17/03/15 15:00:47 DEBUG Recycler: -Dio.netty.recycler.linkCapacity: 16
17/03/15 15:00:47 DEBUG Recycler: -Dio.netty.recycler.ratio: 8
17/03/15 15:00:47 DEBUG BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false
17/03/15 15:00:47 DEBUG BlockReaderLocal: dfs.client.read.shortcircuit = false
17/03/15 15:00:47 DEBUG BlockReaderLocal: dfs.client.domain.socket.data.traffic = false
17/03/15 15:00:47 DEBUG BlockReaderLocal: dfs.domain.socket.path = 
17/03/15 15:00:47 DEBUG RetryUtils: multipleLinearRandomRetry = null
17/03/15 15:00:47 DEBUG Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcRequestWrapper, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@2f943dc1
17/03/15 15:00:47 DEBUG Client: getting client out of cache: org.apache.hadoop.ipc.Client@66893099
17/03/15 15:00:48 DEBUG PerformanceAdvisory: Both short-circuit local reads and UNIX domain socket are disabled.
17/03/15 15:00:48 DEBUG DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
17/03/15 15:00:48 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:00:48 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:00:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:00:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #0
17/03/15 15:00:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #0
17/03/15 15:00:48 DEBUG ProtobufRpcEngine: Call: getFileInfo took 25ms
17/03/15 15:00:48 DEBUG DFSClient: /eventLogs/app-20170315150047-0022.lz4.inprogress: masked=rw-r--r--
17/03/15 15:00:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #1
17/03/15 15:00:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #1
17/03/15 15:00:48 DEBUG ProtobufRpcEngine: Call: create took 12ms
17/03/15 15:00:48 DEBUG DFSClient: computePacketChunkSize: src=/eventLogs/app-20170315150047-0022.lz4.inprogress, chunkSize=516, chunksPerPacket=126, packetSize=65016
17/03/15 15:00:48 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 started
17/03/15 15:00:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #2
17/03/15 15:00:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #2
17/03/15 15:00:48 DEBUG ProtobufRpcEngine: Call: setPermission took 5ms
17/03/15 15:00:48 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=0 lastFlushOffset=0 createNewBlock=false
17/03/15 15:00:48 DEBUG DFSClient: Waiting for ack for: -1
17/03/15 15:00:48 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=0 lastFlushOffset=0 createNewBlock=false
17/03/15 15:00:48 DEBUG DFSClient: Waiting for ack for: -1
17/03/15 15:00:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #3
17/03/15 15:00:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #3
17/03/15 15:00:48 DEBUG ProtobufRpcEngine: Call: getFileInfo took 1ms
17/03/15 15:00:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #4
17/03/15 15:00:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #4
17/03/15 15:00:48 DEBUG ProtobufRpcEngine: Call: getListing took 2ms
17/03/15 15:00:48 DEBUG FileInputFormat: Time taken to get FileStatuses: 24
17/03/15 15:00:48 INFO FileInputFormat: Total input paths to process : 10
17/03/15 15:00:48 DEBUG FileInputFormat: Total # of splits generated by getSplits: 16, TimeTaken: 27
17/03/15 15:00:48 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=0 lastFlushOffset=0 createNewBlock=false
17/03/15 15:00:48 DEBUG DFSClient: Waiting for ack for: -1
[Stage 0:>                                                         (0 + 0) / 10]17/03/15 15:00:52 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=0, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0
17/03/15 15:00:52 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=6294 lastFlushOffset=0 createNewBlock=false
17/03/15 15:00:52 DEBUG DFSClient: Queued packet 0
17/03/15 15:00:52 DEBUG DFSClient: Waiting for ack for: 0
17/03/15 15:00:52 DEBUG DFSClient: Allocating new block
17/03/15 15:00:52 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #5
17/03/15 15:00:52 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #5
17/03/15 15:00:52 DEBUG ProtobufRpcEngine: Call: addBlock took 4ms
17/03/15 15:00:52 DEBUG DFSClient: pipeline = DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]
17/03/15 15:00:52 DEBUG DFSClient: Connecting to datanode 172.21.15.173:50010
17/03/15 15:00:52 DEBUG DFSClient: Send buf size 124928
17/03/15 15:00:52 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #6
17/03/15 15:00:52 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #6
17/03/15 15:00:52 DEBUG ProtobufRpcEngine: Call: getServerDefaults took 1ms
17/03/15 15:00:52 DEBUG SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = /172.21.15.173, datanodeId = DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]
17/03/15 15:00:52 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 6294
17/03/15 15:00:52 DEBUG DFSClient: DFSClient seqno: 0 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 15:00:52 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #7
17/03/15 15:00:52 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #7
17/03/15 15:00:52 DEBUG ProtobufRpcEngine: Call: fsync took 6ms
17/03/15 15:00:52 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=1, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=6144
17/03/15 15:00:52 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=6294 lastFlushOffset=6294 createNewBlock=false
17/03/15 15:00:52 DEBUG DFSClient: Waiting for ack for: 0
[Stage 0:>                                                         (0 + 4) / 10][Stage 0:=====>                                                    (1 + 4) / 10][Stage 0:=======================>                                  (4 + 4) / 10][Stage 0:=============================>                            (5 + 4) / 10]17/03/15 15:01:02 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:01:02 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 0:==================================>                       (6 + 4) / 10][Stage 0:========================================>                 (7 + 3) / 10][Stage 0:====================================================>     (9 + 1) / 10]                                                                                17/03/15 15:01:05 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=2, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=6144
17/03/15 15:01:05 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=10447 lastFlushOffset=6294 createNewBlock=false
17/03/15 15:01:05 DEBUG DFSClient: Queued packet 2
17/03/15 15:01:05 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 2 offsetInBlock: 6144 lastPacketInBlock: false lastByteOffsetInBlock: 10447
17/03/15 15:01:05 DEBUG DFSClient: Waiting for ack for: 2
17/03/15 15:01:05 DEBUG DFSClient: DFSClient seqno: 2 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 15:01:05 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=3, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=10240
17/03/15 15:01:05 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=10447 lastFlushOffset=10447 createNewBlock=false
17/03/15 15:01:05 DEBUG DFSClient: Waiting for ack for: 2
17/03/15 15:01:05 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=4, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=10240
17/03/15 15:01:05 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=10447 lastFlushOffset=10447 createNewBlock=false
17/03/15 15:01:05 DEBUG DFSClient: Waiting for ack for: 2
[Stage 1:>                                                         (0 + 4) / 10][Stage 1:=====>                                                    (1 + 4) / 10][Stage 1:=======================>                                  (4 + 4) / 10][Stage 1:=============================>                            (5 + 4) / 10][Stage 1:==================================>                       (6 + 4) / 10][Stage 1:========================================>                 (7 + 3) / 10][Stage 1:====================================================>     (9 + 1) / 10]17/03/15 15:01:07 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=5, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=10240
17/03/15 15:01:07 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=14811 lastFlushOffset=10447 createNewBlock=false
17/03/15 15:01:07 DEBUG DFSClient: Queued packet 5
17/03/15 15:01:07 DEBUG DFSClient: Waiting for ack for: 5
17/03/15 15:01:07 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 5 offsetInBlock: 10240 lastPacketInBlock: false lastByteOffsetInBlock: 14811
17/03/15 15:01:07 DEBUG DFSClient: DFSClient seqno: 5 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 2:>                                                         (0 + 4) / 10][Stage 2:=======================>                                  (4 + 4) / 10][Stage 2:==============================================>           (8 + 2) / 10]                                                                                17/03/15 15:01:16 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=6, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=14336
17/03/15 15:01:16 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=28037 lastFlushOffset=14811 createNewBlock=false
17/03/15 15:01:16 DEBUG DFSClient: Queued packet 6
17/03/15 15:01:16 DEBUG DFSClient: Waiting for ack for: 6
17/03/15 15:01:16 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 6 offsetInBlock: 14336 lastPacketInBlock: false lastByteOffsetInBlock: 28037
17/03/15 15:01:16 DEBUG DFSClient: DFSClient seqno: 6 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 15:01:16 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=7, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=27648
17/03/15 15:01:16 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=28037 lastFlushOffset=28037 createNewBlock=false
17/03/15 15:01:16 DEBUG DFSClient: Waiting for ack for: 6
17/03/15 15:01:17 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=8, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=27648
17/03/15 15:01:17 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=28037 lastFlushOffset=28037 createNewBlock=false
17/03/15 15:01:17 DEBUG DFSClient: Waiting for ack for: 6
[Stage 4:>                                                         (0 + 4) / 10]17/03/15 15:01:18 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:01:18 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:01:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:01:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #8
17/03/15 15:01:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #8
17/03/15 15:01:18 DEBUG ProtobufRpcEngine: Call: renewLease took 8ms
17/03/15 15:01:18 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:01:18 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 4:=======================>                                  (4 + 4) / 10]17/03/15 15:01:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:01:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 4:==============================================>           (8 + 2) / 10]17/03/15 15:01:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=9, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=27648
17/03/15 15:01:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=39005 lastFlushOffset=28037 createNewBlock=false
17/03/15 15:01:35 DEBUG DFSClient: Queued packet 9
17/03/15 15:01:35 DEBUG DFSClient: Waiting for ack for: 9
17/03/15 15:01:35 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 9 offsetInBlock: 27648 lastPacketInBlock: false lastByteOffsetInBlock: 39005
17/03/15 15:01:35 DEBUG DFSClient: DFSClient seqno: 9 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
                                                                                17/03/15 15:01:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=10, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=38912
17/03/15 15:01:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=48041 lastFlushOffset=39005 createNewBlock=false
17/03/15 15:01:35 DEBUG DFSClient: Queued packet 10
17/03/15 15:01:35 DEBUG DFSClient: Waiting for ack for: 10
17/03/15 15:01:35 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 10 offsetInBlock: 38912 lastPacketInBlock: false lastByteOffsetInBlock: 48041
17/03/15 15:01:35 DEBUG DFSClient: DFSClient seqno: 10 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 15:01:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=11, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=47616
17/03/15 15:01:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=48041 lastFlushOffset=48041 createNewBlock=false
17/03/15 15:01:35 DEBUG DFSClient: Waiting for ack for: 10
17/03/15 15:01:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=12, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=47616
17/03/15 15:01:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=48041 lastFlushOffset=48041 createNewBlock=false
17/03/15 15:01:35 DEBUG DFSClient: Waiting for ack for: 10
17/03/15 15:01:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=13, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=47616
17/03/15 15:01:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=52578 lastFlushOffset=48041 createNewBlock=false
17/03/15 15:01:35 DEBUG DFSClient: Queued packet 13
17/03/15 15:01:35 DEBUG DFSClient: Waiting for ack for: 13
17/03/15 15:01:35 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 13 offsetInBlock: 47616 lastPacketInBlock: false lastByteOffsetInBlock: 52578
17/03/15 15:01:35 DEBUG DFSClient: DFSClient seqno: 13 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 15:01:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=14, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=52224
17/03/15 15:01:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=52578 lastFlushOffset=52578 createNewBlock=false
17/03/15 15:01:35 DEBUG DFSClient: Waiting for ack for: 13
17/03/15 15:01:36 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=15, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=52224
17/03/15 15:01:36 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=52578 lastFlushOffset=52578 createNewBlock=false
17/03/15 15:01:36 DEBUG DFSClient: Waiting for ack for: 13
17/03/15 15:01:36 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=16, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=52224
17/03/15 15:01:36 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=52578 lastFlushOffset=52578 createNewBlock=false
17/03/15 15:01:36 DEBUG DFSClient: Waiting for ack for: 13
17/03/15 15:01:36 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=17, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=52224
17/03/15 15:01:36 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=57247 lastFlushOffset=52578 createNewBlock=false
17/03/15 15:01:36 DEBUG DFSClient: Queued packet 17
17/03/15 15:01:36 DEBUG DFSClient: Waiting for ack for: 17
17/03/15 15:01:36 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 17 offsetInBlock: 52224 lastPacketInBlock: false lastByteOffsetInBlock: 57247
17/03/15 15:01:36 DEBUG DFSClient: DFSClient seqno: 17 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 11:>                                                        (0 + 4) / 10][Stage 11:======================>                                  (4 + 4) / 10][Stage 11:============================>                            (5 + 4) / 10][Stage 11:===================================================>     (9 + 1) / 10]17/03/15 15:01:38 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=18, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=56832
17/03/15 15:01:38 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=66953 lastFlushOffset=57247 createNewBlock=false
17/03/15 15:01:38 DEBUG DFSClient: Queued packet 18
17/03/15 15:01:38 DEBUG DFSClient: Waiting for ack for: 18
17/03/15 15:01:38 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 18 offsetInBlock: 56832 lastPacketInBlock: false lastByteOffsetInBlock: 66953
17/03/15 15:01:38 DEBUG DFSClient: DFSClient seqno: 18 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
                                                                                17/03/15 15:01:38 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=19, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=66560
17/03/15 15:01:38 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=76231 lastFlushOffset=66953 createNewBlock=false
17/03/15 15:01:38 DEBUG DFSClient: Queued packet 19
17/03/15 15:01:38 DEBUG DFSClient: Waiting for ack for: 19
17/03/15 15:01:38 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 19 offsetInBlock: 66560 lastPacketInBlock: false lastByteOffsetInBlock: 76231
17/03/15 15:01:38 DEBUG DFSClient: DFSClient seqno: 19 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 15:01:38 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=20, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=75776
17/03/15 15:01:38 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=76231 lastFlushOffset=76231 createNewBlock=false
17/03/15 15:01:38 DEBUG DFSClient: Waiting for ack for: 19
17/03/15 15:01:39 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=21, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=75776
17/03/15 15:01:39 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=76231 lastFlushOffset=76231 createNewBlock=false
17/03/15 15:01:39 DEBUG DFSClient: Waiting for ack for: 19
17/03/15 15:01:39 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=22, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=75776
17/03/15 15:01:39 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=80457 lastFlushOffset=76231 createNewBlock=false
17/03/15 15:01:39 DEBUG DFSClient: Queued packet 22
17/03/15 15:01:39 DEBUG DFSClient: Waiting for ack for: 22
17/03/15 15:01:39 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 22 offsetInBlock: 75776 lastPacketInBlock: false lastByteOffsetInBlock: 80457
17/03/15 15:01:39 DEBUG DFSClient: DFSClient seqno: 22 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 15:01:39 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=23, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=80384
17/03/15 15:01:39 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=90242 lastFlushOffset=80457 createNewBlock=false
17/03/15 15:01:39 DEBUG DFSClient: Queued packet 23
17/03/15 15:01:39 DEBUG DFSClient: Waiting for ack for: 23
17/03/15 15:01:39 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 23 offsetInBlock: 80384 lastPacketInBlock: false lastByteOffsetInBlock: 90242
17/03/15 15:01:39 DEBUG DFSClient: DFSClient seqno: 23 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 15:01:39 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=24, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=90112
17/03/15 15:01:39 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=90242 lastFlushOffset=90242 createNewBlock=false
17/03/15 15:01:39 DEBUG DFSClient: Waiting for ack for: 23
17/03/15 15:01:39 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=25, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=90112
17/03/15 15:01:39 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=90242 lastFlushOffset=90242 createNewBlock=false
17/03/15 15:01:39 DEBUG DFSClient: Waiting for ack for: 23
17/03/15 15:01:39 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=26, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=90112
17/03/15 15:01:39 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=90242 lastFlushOffset=90242 createNewBlock=false
17/03/15 15:01:39 DEBUG DFSClient: Waiting for ack for: 23
17/03/15 15:01:39 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=27, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=90112
17/03/15 15:01:39 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=94960 lastFlushOffset=90242 createNewBlock=false
17/03/15 15:01:39 DEBUG DFSClient: Queued packet 27
17/03/15 15:01:39 DEBUG DFSClient: Waiting for ack for: 27
17/03/15 15:01:39 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 27 offsetInBlock: 90112 lastPacketInBlock: false lastByteOffsetInBlock: 94960
17/03/15 15:01:39 DEBUG DFSClient: DFSClient seqno: 27 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 24:>                                                        (0 + 4) / 10][Stage 24:======================>                                  (4 + 4) / 10][Stage 24:=============================================>           (8 + 2) / 10]17/03/15 15:01:43 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=28, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=94720
17/03/15 15:01:43 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=99016 lastFlushOffset=94960 createNewBlock=false
17/03/15 15:01:43 DEBUG DFSClient: Queued packet 28
17/03/15 15:01:43 DEBUG DFSClient: Waiting for ack for: 28
17/03/15 15:01:43 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 28 offsetInBlock: 94720 lastPacketInBlock: false lastByteOffsetInBlock: 99016
17/03/15 15:01:43 DEBUG DFSClient: DFSClient seqno: 28 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 25:=============================================>           (8 + 2) / 10]                                                                                17/03/15 15:01:44 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=29, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=98816
17/03/15 15:01:44 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=109306 lastFlushOffset=99016 createNewBlock=false
17/03/15 15:01:44 DEBUG DFSClient: Queued packet 29
17/03/15 15:01:44 DEBUG DFSClient: Waiting for ack for: 29
17/03/15 15:01:44 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 29 offsetInBlock: 98816 lastPacketInBlock: false lastByteOffsetInBlock: 109306
17/03/15 15:01:44 DEBUG DFSClient: DFSClient seqno: 29 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 15:01:44 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=30, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=109056
17/03/15 15:01:44 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=109306 lastFlushOffset=109306 createNewBlock=false
17/03/15 15:01:44 DEBUG DFSClient: Waiting for ack for: 29
17/03/15 15:01:44 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=31, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=109056
17/03/15 15:01:44 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=114071 lastFlushOffset=109306 createNewBlock=false
17/03/15 15:01:44 DEBUG DFSClient: Queued packet 31
17/03/15 15:01:44 DEBUG DFSClient: Waiting for ack for: 31
17/03/15 15:01:44 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 31 offsetInBlock: 109056 lastPacketInBlock: false lastByteOffsetInBlock: 114071
17/03/15 15:01:44 DEBUG DFSClient: DFSClient seqno: 31 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 32:======================>                                  (4 + 4) / 10][Stage 32:=============================================>           (8 + 2) / 10][Stage 32:===================================================>     (9 + 1) / 10]17/03/15 15:01:45 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=32, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=113664
17/03/15 15:01:45 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=122301 lastFlushOffset=114071 createNewBlock=false
17/03/15 15:01:45 DEBUG DFSClient: Queued packet 32
17/03/15 15:01:45 DEBUG DFSClient: Waiting for ack for: 32
17/03/15 15:01:45 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 32 offsetInBlock: 113664 lastPacketInBlock: false lastByteOffsetInBlock: 122301
17/03/15 15:01:45 DEBUG DFSClient: DFSClient seqno: 32 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 33:=================>                                       (3 + 4) / 10][Stage 33:======================>                                  (4 + 4) / 10][Stage 33:=======================================>                 (7 + 3) / 10][Stage 33:=============================================>           (8 + 2) / 10]17/03/15 15:01:48 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:01:48 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:01:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:01:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #9
17/03/15 15:01:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #9
17/03/15 15:01:48 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 15:01:48 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:01:48 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
                                                                                17/03/15 15:01:49 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=33, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=121856
17/03/15 15:01:49 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=132368 lastFlushOffset=122301 createNewBlock=false
17/03/15 15:01:49 DEBUG DFSClient: Queued packet 33
17/03/15 15:01:49 DEBUG DFSClient: Waiting for ack for: 33
17/03/15 15:01:49 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 33 offsetInBlock: 121856 lastPacketInBlock: false lastByteOffsetInBlock: 132368
17/03/15 15:01:49 DEBUG DFSClient: DFSClient seqno: 33 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 15:01:49 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=34, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=132096
17/03/15 15:01:49 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=132368 lastFlushOffset=132368 createNewBlock=false
17/03/15 15:01:49 DEBUG DFSClient: Waiting for ack for: 33
17/03/15 15:01:49 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=35, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=132096
17/03/15 15:01:49 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=132368 lastFlushOffset=132368 createNewBlock=false
17/03/15 15:01:49 DEBUG DFSClient: Waiting for ack for: 33
17/03/15 15:01:49 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=36, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=132096
17/03/15 15:01:49 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=132368 lastFlushOffset=132368 createNewBlock=false
17/03/15 15:01:49 DEBUG DFSClient: Waiting for ack for: 33
17/03/15 15:01:49 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=37, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=132096
17/03/15 15:01:49 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=137123 lastFlushOffset=132368 createNewBlock=false
17/03/15 15:01:49 DEBUG DFSClient: Queued packet 37
17/03/15 15:01:49 DEBUG DFSClient: Waiting for ack for: 37
17/03/15 15:01:49 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 37 offsetInBlock: 132096 lastPacketInBlock: false lastByteOffsetInBlock: 137123
17/03/15 15:01:49 DEBUG DFSClient: DFSClient seqno: 37 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 41:>                                                        (0 + 4) / 10]17/03/15 15:01:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:01:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 41:=================>                                       (3 + 4) / 10]17/03/15 15:02:18 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:02:18 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:02:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:02:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #10
17/03/15 15:02:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #10
17/03/15 15:02:18 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 15:02:18 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:02:18 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 41:======================>                                  (4 + 4) / 10]17/03/15 15:02:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:02:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 41:==================================>                      (6 + 4) / 10]17/03/15 15:02:48 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:02:48 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:02:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:02:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #11
17/03/15 15:02:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #11
17/03/15 15:02:48 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/15 15:02:48 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:02:48 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 41:=======================================>                 (7 + 3) / 10]17/03/15 15:02:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:02:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 41:=============================================>           (8 + 2) / 10]17/03/15 15:03:01 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=38, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=136704
17/03/15 15:03:01 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=144792 lastFlushOffset=137123 createNewBlock=false
17/03/15 15:03:01 DEBUG DFSClient: Queued packet 38
17/03/15 15:03:01 DEBUG DFSClient: Waiting for ack for: 38
17/03/15 15:03:01 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 38 offsetInBlock: 136704 lastPacketInBlock: false lastByteOffsetInBlock: 144792
17/03/15 15:03:01 WARN DFSClient: Slow ReadProcessor read fields took 71775ms (threshold=30000ms); ack: seqno: 38 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
[Stage 42:>                                                        (0 + 4) / 10][Stage 42:=====>                                                   (1 + 4) / 10][Stage 42:===========>                                             (2 + 4) / 10][Stage 42:=================>                                       (3 + 4) / 10][Stage 42:======================>                                  (4 + 4) / 10][Stage 42:============================>                            (5 + 4) / 10][Stage 42:==================================>                      (6 + 4) / 10][Stage 42:=======================================>                 (7 + 3) / 10][Stage 42:=============================================>           (8 + 2) / 10]                                                                                17/03/15 15:03:11 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=39, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=144384
17/03/15 15:03:11 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=158191 lastFlushOffset=144792 createNewBlock=false
17/03/15 15:03:11 DEBUG DFSClient: Queued packet 39
17/03/15 15:03:11 DEBUG DFSClient: Waiting for ack for: 39
17/03/15 15:03:11 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 39 offsetInBlock: 144384 lastPacketInBlock: false lastByteOffsetInBlock: 158191
17/03/15 15:03:11 DEBUG DFSClient: DFSClient seqno: 39 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 15:03:11 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=40, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=157696
17/03/15 15:03:11 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=158191 lastFlushOffset=158191 createNewBlock=false
17/03/15 15:03:11 DEBUG DFSClient: Waiting for ack for: 39
17/03/15 15:03:11 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=41, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=157696
17/03/15 15:03:11 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=161771 lastFlushOffset=158191 createNewBlock=false
17/03/15 15:03:11 DEBUG DFSClient: Queued packet 41
17/03/15 15:03:11 DEBUG DFSClient: Waiting for ack for: 41
17/03/15 15:03:11 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 41 offsetInBlock: 157696 lastPacketInBlock: false lastByteOffsetInBlock: 161771
17/03/15 15:03:11 DEBUG DFSClient: DFSClient seqno: 41 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 51:>                                                        (0 + 4) / 10][Stage 51:=====>                                                   (1 + 4) / 10][Stage 51:=================>                                       (3 + 4) / 10][Stage 51:======================>                                  (4 + 4) / 10][Stage 51:============================>                            (5 + 4) / 10][Stage 51:==================================>                      (6 + 4) / 10][Stage 51:=======================================>                 (7 + 3) / 10][Stage 51:===================================================>     (9 + 1) / 10]17/03/15 15:03:17 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=42, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=161280
17/03/15 15:03:17 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=166692 lastFlushOffset=161771 createNewBlock=false
17/03/15 15:03:17 DEBUG DFSClient: Queued packet 42
17/03/15 15:03:17 DEBUG DFSClient: Waiting for ack for: 42
17/03/15 15:03:17 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 42 offsetInBlock: 161280 lastPacketInBlock: false lastByteOffsetInBlock: 166692
17/03/15 15:03:17 DEBUG DFSClient: DFSClient seqno: 42 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 52:>                                                        (0 + 4) / 10]17/03/15 15:03:18 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:03:18 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:03:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:03:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #12
17/03/15 15:03:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #12
17/03/15 15:03:18 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 15:03:18 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:03:18 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 52:=====>                                                   (1 + 4) / 10][Stage 52:===========>                                             (2 + 4) / 10]17/03/15 15:03:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:03:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 52:=================>                                       (3 + 4) / 10][Stage 52:======================>                                  (4 + 4) / 10][Stage 52:============================>                            (5 + 4) / 10][Stage 52:==================================>                      (6 + 4) / 10][Stage 52:=======================================>                 (7 + 3) / 10][Stage 52:=============================================>           (8 + 2) / 10]17/03/15 15:03:41 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=43, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=166400
[Stage 52:===================================================>     (9 + 1) / 10]                                                                                17/03/15 15:03:41 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=188209 lastFlushOffset=166692 createNewBlock=false
17/03/15 15:03:41 DEBUG DFSClient: Queued packet 43
17/03/15 15:03:41 DEBUG DFSClient: Waiting for ack for: 43
17/03/15 15:03:41 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 43 offsetInBlock: 166400 lastPacketInBlock: false lastByteOffsetInBlock: 188209
17/03/15 15:03:41 DEBUG DFSClient: DFSClient seqno: 43 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 15:03:41 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=44, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=187904
17/03/15 15:03:41 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=188209 lastFlushOffset=188209 createNewBlock=false
17/03/15 15:03:41 DEBUG DFSClient: Waiting for ack for: 43
17/03/15 15:03:41 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=45, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=187904
17/03/15 15:03:41 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=188209 lastFlushOffset=188209 createNewBlock=false
17/03/15 15:03:41 DEBUG DFSClient: Waiting for ack for: 43
17/03/15 15:03:41 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=46, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=187904
17/03/15 15:03:41 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=188209 lastFlushOffset=188209 createNewBlock=false
17/03/15 15:03:41 DEBUG DFSClient: Waiting for ack for: 43
17/03/15 15:03:41 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=47, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=187904
17/03/15 15:03:41 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=192138 lastFlushOffset=188209 createNewBlock=false
17/03/15 15:03:41 DEBUG DFSClient: Queued packet 47
17/03/15 15:03:41 DEBUG DFSClient: Waiting for ack for: 47
17/03/15 15:03:41 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 47 offsetInBlock: 187904 lastPacketInBlock: false lastByteOffsetInBlock: 192138
17/03/15 15:03:41 DEBUG DFSClient: DFSClient seqno: 47 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 62:>                                                        (0 + 4) / 10]17/03/15 15:03:48 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:03:48 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:03:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:03:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #13
17/03/15 15:03:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #13
17/03/15 15:03:48 DEBUG ProtobufRpcEngine: Call: renewLease took 8ms
17/03/15 15:03:48 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:03:48 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 62:=====>                                                   (1 + 4) / 10][Stage 62:===========>                                             (2 + 4) / 10][Stage 62:=================>                                       (3 + 4) / 10][Stage 62:======================>                                  (4 + 4) / 10][Stage 62:============================>                            (5 + 4) / 10][Stage 62:==================================>                      (6 + 4) / 10][Stage 62:=======================================>                 (7 + 3) / 10][Stage 62:=============================================>           (8 + 2) / 10][Stage 62:===================================================>     (9 + 1) / 10]17/03/15 15:03:56 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=48, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=192000
17/03/15 15:03:56 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=201021 lastFlushOffset=192138 createNewBlock=false
17/03/15 15:03:56 DEBUG DFSClient: Queued packet 48
17/03/15 15:03:56 DEBUG DFSClient: Waiting for ack for: 48
17/03/15 15:03:56 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 48 offsetInBlock: 192000 lastPacketInBlock: false lastByteOffsetInBlock: 201021
17/03/15 15:03:56 DEBUG DFSClient: DFSClient seqno: 48 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 63:>                                                        (0 + 4) / 10]17/03/15 15:03:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:03:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 63:=====>                                                   (1 + 4) / 10][Stage 63:===========>                                             (2 + 4) / 10][Stage 63:======================>                                  (4 + 4) / 10][Stage 63:============================>                            (5 + 4) / 10][Stage 63:==================================>                      (6 + 4) / 10][Stage 63:=======================================>                 (7 + 3) / 10][Stage 63:=============================================>           (8 + 2) / 10][Stage 63:===================================================>     (9 + 1) / 10]                                                                                17/03/15 15:04:08 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=49, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=200704
17/03/15 15:04:08 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=213451 lastFlushOffset=201021 createNewBlock=false
17/03/15 15:04:08 DEBUG DFSClient: Queued packet 49
17/03/15 15:04:08 DEBUG DFSClient: Waiting for ack for: 49
17/03/15 15:04:08 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 49 offsetInBlock: 200704 lastPacketInBlock: false lastByteOffsetInBlock: 213451
17/03/15 15:04:08 DEBUG DFSClient: DFSClient seqno: 49 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 15:04:08 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=50, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=212992
17/03/15 15:04:08 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=213451 lastFlushOffset=213451 createNewBlock=false
17/03/15 15:04:08 DEBUG DFSClient: Waiting for ack for: 49
17/03/15 15:04:08 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=51, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=212992
17/03/15 15:04:08 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=220605 lastFlushOffset=213451 createNewBlock=false
17/03/15 15:04:08 DEBUG DFSClient: Queued packet 51
17/03/15 15:04:08 DEBUG DFSClient: Waiting for ack for: 51
17/03/15 15:04:08 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 51 offsetInBlock: 212992 lastPacketInBlock: false lastByteOffsetInBlock: 220605
17/03/15 15:04:08 DEBUG DFSClient: DFSClient seqno: 51 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 74:>                                                        (0 + 4) / 10][Stage 74:=====>                                                   (1 + 4) / 10][Stage 74:===========>                                             (2 + 4) / 10][Stage 74:=================>                                       (3 + 4) / 10][Stage 74:======================>                                  (4 + 4) / 10][Stage 74:==================================>                      (6 + 4) / 10][Stage 74:===================================================>     (9 + 1) / 10]17/03/15 15:04:18 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:04:18 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:04:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:04:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #14
17/03/15 15:04:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #14
17/03/15 15:04:18 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 15:04:18 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:04:18 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
17/03/15 15:04:19 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=52, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=220160
17/03/15 15:04:19 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=225981 lastFlushOffset=220605 createNewBlock=false
17/03/15 15:04:19 DEBUG DFSClient: Queued packet 52
17/03/15 15:04:19 DEBUG DFSClient: Waiting for ack for: 52
17/03/15 15:04:19 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 52 offsetInBlock: 220160 lastPacketInBlock: false lastByteOffsetInBlock: 225981
17/03/15 15:04:19 DEBUG DFSClient: DFSClient seqno: 52 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 75:>                                                        (0 + 4) / 10]17/03/15 15:04:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:04:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 75:=====>                                                   (1 + 4) / 10][Stage 75:===========>                                             (2 + 4) / 10][Stage 75:=================>                                       (3 + 4) / 10][Stage 75:======================>                                  (4 + 4) / 10]17/03/15 15:04:48 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:04:48 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:04:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:04:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #15
17/03/15 15:04:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #15
17/03/15 15:04:48 DEBUG ProtobufRpcEngine: Call: renewLease took 14ms
17/03/15 15:04:48 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:04:48 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 75:============================>                            (5 + 4) / 10]17/03/15 15:04:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:04:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 75:==================================>                      (6 + 4) / 10][Stage 75:=======================================>                 (7 + 3) / 10]17/03/15 15:05:02 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=53, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=225792
[Stage 75:=============================================>           (8 + 2) / 10][Stage 75:===================================================>     (9 + 1) / 10]                                                                                17/03/15 15:05:06 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=243028 lastFlushOffset=225981 createNewBlock=false
17/03/15 15:05:06 DEBUG DFSClient: Queued packet 53
17/03/15 15:05:06 DEBUG DFSClient: Waiting for ack for: 53
17/03/15 15:05:06 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 53 offsetInBlock: 225792 lastPacketInBlock: false lastByteOffsetInBlock: 243028
17/03/15 15:05:06 WARN DFSClient: Slow ReadProcessor read fields took 46923ms (threshold=30000ms); ack: seqno: 53 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
17/03/15 15:05:06 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=54, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=242688
17/03/15 15:05:06 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=243028 lastFlushOffset=243028 createNewBlock=false
17/03/15 15:05:06 DEBUG DFSClient: Waiting for ack for: 53
17/03/15 15:05:06 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=55, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=242688
17/03/15 15:05:06 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=243028 lastFlushOffset=243028 createNewBlock=false
17/03/15 15:05:06 DEBUG DFSClient: Waiting for ack for: 53
17/03/15 15:05:06 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=56, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=242688
17/03/15 15:05:06 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=243028 lastFlushOffset=243028 createNewBlock=false
17/03/15 15:05:06 DEBUG DFSClient: Waiting for ack for: 53
17/03/15 15:05:06 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=57, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=242688
17/03/15 15:05:06 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=253717 lastFlushOffset=243028 createNewBlock=false
17/03/15 15:05:06 DEBUG DFSClient: Queued packet 57
17/03/15 15:05:06 DEBUG DFSClient: Waiting for ack for: 57
17/03/15 15:05:06 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 57 offsetInBlock: 242688 lastPacketInBlock: false lastByteOffsetInBlock: 253717
17/03/15 15:05:06 DEBUG DFSClient: DFSClient seqno: 57 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 87:>                                                        (0 + 4) / 10]17/03/15 15:05:18 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:05:18 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:05:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:05:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #16
17/03/15 15:05:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #16
17/03/15 15:05:18 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 15:05:18 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:05:18 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
17/03/15 15:05:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:05:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 87:=====>                                                   (1 + 4) / 10][Stage 87:===========>                                             (2 + 4) / 10][Stage 87:===========>                                             (2 + 6) / 10]17/03/15 15:05:34 WARN TaskSetManager: Lost task 3.0 in stage 87.0 (TID 233, 172.21.15.173, executor 0): java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:331)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:364)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:364)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:356)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:322)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:322)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1303)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:362)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:369)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)

[Stage 87:===========>                                             (2 + 4) / 10][Stage 87:=================>                                       (3 + 4) / 10]17/03/15 15:05:38 WARN TaskSetManager: Lost task 6.0 in stage 87.0 (TID 236, 172.21.15.173, executor 0): java.io.FileNotFoundException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-ed65ebe2-294e-44d7-87bb-308ba4933b7f/30/temp_shuffle_d3bdd18e-26bb-4e63-aae7-305c69fefc2d (没有那个文件或目录)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:229)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:152)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/03/15 15:05:38 WARN TaskSetManager: Lost task 0.1 in stage 87.0 (TID 238, 172.21.15.173, executor 0): FetchFailed(BlockManagerId(0, 172.21.15.173, 46776, None), shuffleId=2, mapId=0, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-ed65ebe2-294e-44d7-87bb-308ba4933b7f/0a/shuffle_2_0_0.data, offset=0, length=36259}
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:356)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-ed65ebe2-294e-44d7-87bb-308ba4933b7f/0a/shuffle_2_0_0.data, offset=0, length=36259}
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:113)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:349)
	... 39 more
Caused by: java.io.FileNotFoundException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-ed65ebe2-294e-44d7-87bb-308ba4933b7f/0a/shuffle_2_0_0.data (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:98)
	... 40 more

)
17/03/15 15:05:38 WARN TaskSetManager: Lost task 4.0 in stage 87.0 (TID 234, 172.21.15.173, executor 0): FetchFailed(BlockManagerId(0, 172.21.15.173, 46776, None), shuffleId=2, mapId=0, reduceId=4, message=
org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-ed65ebe2-294e-44d7-87bb-308ba4933b7f/0a/shuffle_2_0_0.data, offset=145036, length=36259}
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:356)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-ed65ebe2-294e-44d7-87bb-308ba4933b7f/0a/shuffle_2_0_0.data, offset=145036, length=36259}
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:113)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:349)
	... 39 more
Caused by: java.io.FileNotFoundException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-ed65ebe2-294e-44d7-87bb-308ba4933b7f/0a/shuffle_2_0_0.data (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:98)
	... 40 more

)
17/03/15 15:05:38 WARN TaskSetManager: Lost task 5.0 in stage 87.0 (TID 235, 172.21.15.173, executor 0): FetchFailed(BlockManagerId(0, 172.21.15.173, 46776, None), shuffleId=2, mapId=0, reduceId=5, message=
org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-ed65ebe2-294e-44d7-87bb-308ba4933b7f/0a/shuffle_2_0_0.data, offset=181295, length=36259}
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:356)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-ed65ebe2-294e-44d7-87bb-308ba4933b7f/0a/shuffle_2_0_0.data, offset=181295, length=36259}
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:113)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:349)
	... 39 more
Caused by: java.io.FileNotFoundException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-ed65ebe2-294e-44d7-87bb-308ba4933b7f/0a/shuffle_2_0_0.data (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:98)
	... 40 more

)
17/03/15 15:05:38 WARN TaskSetManager: Lost task 3.1 in stage 87.0 (TID 239, 172.21.15.173, executor 0): FetchFailed(BlockManagerId(0, 172.21.15.173, 46776, None), shuffleId=2, mapId=0, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-ed65ebe2-294e-44d7-87bb-308ba4933b7f/32/shuffle_2_0_0.index (没有那个文件或目录)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-ed65ebe2-294e-44d7-87bb-308ba4933b7f/32/shuffle_2_0_0.index (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:199)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:302)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:269)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:303)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:132)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	... 23 more

)
17/03/15 15:05:38 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=58, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=253440
17/03/15 15:05:38 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=263475 lastFlushOffset=253717 createNewBlock=false
17/03/15 15:05:38 DEBUG DFSClient: Queued packet 58
17/03/15 15:05:38 DEBUG DFSClient: Waiting for ack for: 58
17/03/15 15:05:38 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 58 offsetInBlock: 253440 lastPacketInBlock: false lastByteOffsetInBlock: 263475
17/03/15 15:05:38 WARN DFSClient: Slow ReadProcessor read fields took 32193ms (threshold=30000ms); ack: seqno: 58 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
17/03/15 15:05:38 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=59, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=263168
17/03/15 15:05:38 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=263475 lastFlushOffset=263475 createNewBlock=false
17/03/15 15:05:38 DEBUG DFSClient: Waiting for ack for: 58
17/03/15 15:05:38 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=60, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=263168
17/03/15 15:05:38 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=269033 lastFlushOffset=263475 createNewBlock=false
17/03/15 15:05:38 DEBUG DFSClient: Queued packet 60
17/03/15 15:05:38 DEBUG DFSClient: Waiting for ack for: 60
17/03/15 15:05:38 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 60 offsetInBlock: 263168 lastPacketInBlock: false lastByteOffsetInBlock: 269033
17/03/15 15:05:38 WARN TaskSetManager: Lost task 8.0 in stage 87.0 (TID 241, 172.21.15.173, executor 0): org.apache.spark.SparkException: Block rdd_14_8 was not found even though it's read-locked
	at org.apache.spark.storage.BlockManager.handleLocalReadFailure(BlockManager.scala:436)
	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:478)
	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:630)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:688)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/03/15 15:05:38 DEBUG DFSClient: DFSClient seqno: 60 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 76:>                                                        (0 + 0) / 10]17/03/15 15:05:39 WARN TransportChannelHandler: Exception in connection from /172.21.15.173:53680
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:221)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:899)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:275)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)
17/03/15 15:05:39 ERROR TaskSchedulerImpl: Lost executor 0 on 172.21.15.173: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 15:05:39 WARN TaskSetManager: Lost task 6.1 in stage 87.0 (TID 240, 172.21.15.173, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 15:05:39 WARN TaskSetManager: Lost task 2.0 in stage 76.0 (TID 244, 172.21.15.173, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 15:05:39 WARN TaskSetManager: Lost task 1.0 in stage 76.0 (TID 243, 172.21.15.173, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 15:05:39 WARN TaskSetManager: Lost task 0.0 in stage 76.0 (TID 242, 172.21.15.173, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 15:05:39 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=61, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=268800
17/03/15 15:05:39 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=274235 lastFlushOffset=269033 createNewBlock=false
17/03/15 15:05:39 DEBUG DFSClient: Queued packet 61
17/03/15 15:05:39 DEBUG DFSClient: Waiting for ack for: 61
17/03/15 15:05:39 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 61 offsetInBlock: 268800 lastPacketInBlock: false lastByteOffsetInBlock: 274235
17/03/15 15:05:39 DEBUG DFSClient: DFSClient seqno: 61 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 15:05:39 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=62, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=273920
17/03/15 15:05:39 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=274235 lastFlushOffset=274235 createNewBlock=false
17/03/15 15:05:39 DEBUG DFSClient: Waiting for ack for: 61
17/03/15 15:05:45 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=63, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=273920
17/03/15 15:05:45 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=274235 lastFlushOffset=274235 createNewBlock=false
17/03/15 15:05:45 DEBUG DFSClient: Waiting for ack for: 61
[Stage 76:>                                                        (0 + 4) / 10]17/03/15 15:05:46 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=64, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=273920
17/03/15 15:05:46 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=274235 lastFlushOffset=274235 createNewBlock=false
17/03/15 15:05:46 DEBUG DFSClient: Waiting for ack for: 61
17/03/15 15:05:48 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:05:48 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:05:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:05:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #17
17/03/15 15:05:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #17
17/03/15 15:05:48 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 15:05:48 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:05:48 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 76:=====>                                                   (1 + 4) / 10][Stage 76:===========>                                             (2 + 4) / 10][Stage 76:=================>                                       (3 + 4) / 10][Stage 76:======================>                                  (4 + 4) / 10]17/03/15 15:05:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:05:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 76:============================>                            (5 + 4) / 10][Stage 76:==================================>                      (6 + 4) / 10][Stage 76:=======================================>                 (7 + 3) / 10][Stage 76:===================================================>     (9 + 1) / 10]17/03/15 15:06:02 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=65, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=273920
17/03/15 15:06:02 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=283446 lastFlushOffset=274235 createNewBlock=false
17/03/15 15:06:02 DEBUG DFSClient: Queued packet 65
17/03/15 15:06:02 DEBUG DFSClient: Waiting for ack for: 65
17/03/15 15:06:02 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 65 offsetInBlock: 273920 lastPacketInBlock: false lastByteOffsetInBlock: 283446
17/03/15 15:06:02 DEBUG DFSClient: DFSClient seqno: 65 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 77:>                                                        (0 + 4) / 10][Stage 77:======================>                                  (4 + 4) / 10][Stage 77:=============================================>           (8 + 2) / 10]17/03/15 15:06:15 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=66, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=283136
17/03/15 15:06:15 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=291947 lastFlushOffset=283446 createNewBlock=false
17/03/15 15:06:15 DEBUG DFSClient: Queued packet 66
17/03/15 15:06:15 DEBUG DFSClient: Waiting for ack for: 66
17/03/15 15:06:15 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 66 offsetInBlock: 283136 lastPacketInBlock: false lastByteOffsetInBlock: 291947
17/03/15 15:06:15 DEBUG DFSClient: DFSClient seqno: 66 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 15:06:15 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=67, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=291840
17/03/15 15:06:15 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=296946 lastFlushOffset=291947 createNewBlock=false
17/03/15 15:06:15 DEBUG DFSClient: Queued packet 67
17/03/15 15:06:15 DEBUG DFSClient: Waiting for ack for: 67
17/03/15 15:06:15 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 67 offsetInBlock: 291840 lastPacketInBlock: false lastByteOffsetInBlock: 296946
17/03/15 15:06:15 DEBUG DFSClient: DFSClient seqno: 67 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 79:>                                                        (0 + 4) / 10][Stage 79:======================>                                  (4 + 4) / 10][Stage 79:=============================================>           (8 + 2) / 10]17/03/15 15:06:17 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=68, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=296448
17/03/15 15:06:17 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=306614 lastFlushOffset=296946 createNewBlock=false
17/03/15 15:06:17 DEBUG DFSClient: Queued packet 68
17/03/15 15:06:17 DEBUG DFSClient: Waiting for ack for: 68
17/03/15 15:06:17 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 68 offsetInBlock: 296448 lastPacketInBlock: false lastByteOffsetInBlock: 306614
17/03/15 15:06:17 DEBUG DFSClient: DFSClient seqno: 68 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 80:======================>                                  (4 + 4) / 10][Stage 80:=============================================>           (8 + 2) / 10]17/03/15 15:06:18 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=69, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=306176
17/03/15 15:06:18 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=312116 lastFlushOffset=306614 createNewBlock=false
17/03/15 15:06:18 DEBUG DFSClient: Queued packet 69
17/03/15 15:06:18 DEBUG DFSClient: Waiting for ack for: 69
17/03/15 15:06:18 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 69 offsetInBlock: 306176 lastPacketInBlock: false lastByteOffsetInBlock: 312116
17/03/15 15:06:18 DEBUG DFSClient: DFSClient seqno: 69 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 15:06:18 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:06:18 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:06:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:06:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #18
17/03/15 15:06:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #18
17/03/15 15:06:18 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 15:06:18 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:06:18 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 81:>                                                        (0 + 4) / 10][Stage 81:===========>                                             (2 + 4) / 10][Stage 81:======================>                                  (4 + 4) / 10][Stage 81:=============================================>           (8 + 2) / 10]17/03/15 15:06:21 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=70, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=311808
17/03/15 15:06:21 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=323082 lastFlushOffset=312116 createNewBlock=false
17/03/15 15:06:21 DEBUG DFSClient: Queued packet 70
17/03/15 15:06:21 DEBUG DFSClient: Waiting for ack for: 70
17/03/15 15:06:21 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 70 offsetInBlock: 311808 lastPacketInBlock: false lastByteOffsetInBlock: 323082
17/03/15 15:06:21 DEBUG DFSClient: DFSClient seqno: 70 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 82:>                                                        (0 + 4) / 10][Stage 82:======================>                                  (4 + 4) / 10][Stage 82:============================>                            (5 + 4) / 10][Stage 82:=============================================>           (8 + 2) / 10][Stage 82:===================================================>     (9 + 1) / 10]17/03/15 15:06:24 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=71, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=323072
17/03/15 15:06:24 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=328673 lastFlushOffset=323082 createNewBlock=false
17/03/15 15:06:24 DEBUG DFSClient: Queued packet 71
17/03/15 15:06:24 DEBUG DFSClient: Waiting for ack for: 71
17/03/15 15:06:24 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 71 offsetInBlock: 323072 lastPacketInBlock: false lastByteOffsetInBlock: 328673
17/03/15 15:06:24 DEBUG DFSClient: DFSClient seqno: 71 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 83:>                                                        (0 + 4) / 10]17/03/15 15:06:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:06:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 83:=====>                                                   (1 + 4) / 10][Stage 83:===========>                                             (2 + 4) / 10][Stage 83:=================>                                       (3 + 4) / 10][Stage 83:======================>                                  (4 + 4) / 10][Stage 83:============================>                            (5 + 4) / 10]17/03/15 15:06:48 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:06:48 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:06:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:06:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #19
17/03/15 15:06:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #19
17/03/15 15:06:48 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 15:06:48 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:06:48 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 83:==================================>                      (6 + 4) / 10][Stage 83:=======================================>                 (7 + 3) / 10][Stage 83:=============================================>           (8 + 2) / 10][Stage 83:===================================================>     (9 + 1) / 10]17/03/15 15:06:56 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=72, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=328192
17/03/15 15:06:56 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=339966 lastFlushOffset=328673 createNewBlock=false
17/03/15 15:06:56 DEBUG DFSClient: Queued packet 72
17/03/15 15:06:56 DEBUG DFSClient: Waiting for ack for: 72
17/03/15 15:06:56 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 72 offsetInBlock: 328192 lastPacketInBlock: false lastByteOffsetInBlock: 339966
17/03/15 15:06:56 WARN DFSClient: Slow ReadProcessor read fields took 32188ms (threshold=30000ms); ack: seqno: 72 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
[Stage 84:>                                                        (0 + 4) / 10]17/03/15 15:06:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:06:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 84:=====>                                                   (1 + 4) / 10][Stage 84:===========>                                             (2 + 4) / 10][Stage 84:=================>                                       (3 + 4) / 10][Stage 84:==================================>                      (6 + 4) / 10][Stage 84:=======================================>                 (7 + 3) / 10]17/03/15 15:07:16 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=73, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=339456
17/03/15 15:07:16 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=345621 lastFlushOffset=339966 createNewBlock=false
17/03/15 15:07:16 DEBUG DFSClient: Queued packet 73
17/03/15 15:07:16 DEBUG DFSClient: Waiting for ack for: 73
17/03/15 15:07:16 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 73 offsetInBlock: 339456 lastPacketInBlock: false lastByteOffsetInBlock: 345621
17/03/15 15:07:16 DEBUG DFSClient: DFSClient seqno: 73 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 85:>                                                        (0 + 4) / 10]17/03/15 15:07:18 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:07:18 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:07:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:07:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #20
17/03/15 15:07:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #20
17/03/15 15:07:18 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 15:07:18 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:07:18 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 85:===========>                                             (2 + 4) / 10][Stage 85:=================>                                       (3 + 4) / 10]17/03/15 15:07:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:07:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 85:======================>                                  (4 + 4) / 10][Stage 85:==================================>                      (6 + 4) / 10][Stage 85:=======================================>                 (7 + 3) / 10][Stage 85:=============================================>           (8 + 2) / 10][Stage 85:===================================================>     (9 + 1) / 10]17/03/15 15:07:36 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=74, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=345600
17/03/15 15:07:36 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=356715 lastFlushOffset=345621 createNewBlock=false
17/03/15 15:07:36 DEBUG DFSClient: Queued packet 74
17/03/15 15:07:36 DEBUG DFSClient: Waiting for ack for: 74
17/03/15 15:07:36 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 74 offsetInBlock: 345600 lastPacketInBlock: false lastByteOffsetInBlock: 356715
17/03/15 15:07:36 DEBUG DFSClient: DFSClient seqno: 74 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 86:>                                                        (0 + 4) / 10][Stage 86:=====>                                                   (1 + 4) / 10][Stage 86:===========>                                             (2 + 4) / 10][Stage 86:======================>                                  (4 + 4) / 10]17/03/15 15:07:48 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:07:48 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:07:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:07:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #21
17/03/15 15:07:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #21
17/03/15 15:07:48 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 15:07:48 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:07:48 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 86:============================>                            (5 + 4) / 10][Stage 86:==================================>                      (6 + 4) / 10][Stage 86:=======================================>                 (7 + 3) / 10][Stage 86:=============================================>           (8 + 2) / 10]17/03/15 15:07:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:07:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 86:===================================================>     (9 + 1) / 10]17/03/15 15:08:03 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=75, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=356352
17/03/15 15:08:03 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=366224 lastFlushOffset=356715 createNewBlock=false
17/03/15 15:08:03 DEBUG DFSClient: Queued packet 75
17/03/15 15:08:03 DEBUG DFSClient: Waiting for ack for: 75
17/03/15 15:08:03 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 75 offsetInBlock: 356352 lastPacketInBlock: false lastByteOffsetInBlock: 366224
17/03/15 15:08:03 DEBUG DFSClient: DFSClient seqno: 75 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 87:>                                                        (0 + 4) / 10]17/03/15 15:08:18 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:08:18 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:08:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:08:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #22
17/03/15 15:08:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #22
17/03/15 15:08:18 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 15:08:18 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:08:18 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 87:=====>                                                   (1 + 4) / 10]17/03/15 15:08:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:08:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 87:======================>                                  (4 + 4) / 10][Stage 87:============================>                            (5 + 4) / 10]17/03/15 15:08:48 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:08:48 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:08:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:08:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #23
17/03/15 15:08:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #23
17/03/15 15:08:48 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 15:08:48 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:08:48 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
17/03/15 15:08:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:08:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/15 15:09:04 WARN TaskSetManager: Lost task 6.0 in stage 87.1 (TID 361, 172.21.15.173, executor 1): java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:331)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:364)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:364)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:356)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:322)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:322)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1303)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:362)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:369)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)

17/03/15 15:09:04 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=76, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=366080
[Stage 87:=======================================>                 (7 + 3) / 10]17/03/15 15:09:06 WARN TaskSetManager: Lost task 6.1 in stage 87.1 (TID 365, 172.21.15.173, executor 1): FetchFailed(BlockManagerId(1, 172.21.15.173, 35108, None), shuffleId=3, mapId=6, reduceId=6, message=
org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-85335ba0-efb8-4a77-ad7e-0852831137f4/23/shuffle_3_6_0.data, offset=636592, length=106079}
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:356)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-85335ba0-efb8-4a77-ad7e-0852831137f4/23/shuffle_3_6_0.data, offset=636592, length=106079}
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:113)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:349)
	... 38 more
Caused by: java.io.FileNotFoundException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-85335ba0-efb8-4a77-ad7e-0852831137f4/23/shuffle_3_6_0.data (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:98)
	... 39 more

)
17/03/15 15:09:06 WARN TaskSetManager: Lost task 8.0 in stage 87.1 (TID 363, 172.21.15.173, executor 1): java.io.FileNotFoundException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-85335ba0-efb8-4a77-ad7e-0852831137f4/10/temp_shuffle_3142b10c-5105-4cd6-8501-a4aedf681f4c (没有那个文件或目录)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:229)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:152)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/03/15 15:09:06 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=388215 lastFlushOffset=366224 createNewBlock=false
17/03/15 15:09:06 DEBUG DFSClient: Queued packet 76
17/03/15 15:09:06 DEBUG DFSClient: Waiting for ack for: 76
17/03/15 15:09:06 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 76 offsetInBlock: 366080 lastPacketInBlock: false lastByteOffsetInBlock: 388215
17/03/15 15:09:06 WARN DFSClient: Slow ReadProcessor read fields took 62513ms (threshold=30000ms); ack: seqno: 76 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
17/03/15 15:09:06 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=77, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=388096
17/03/15 15:09:06 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=388215 lastFlushOffset=388215 createNewBlock=false
17/03/15 15:09:06 DEBUG DFSClient: Waiting for ack for: 76
17/03/15 15:09:06 WARN TransportChannelHandler: Exception in connection from /172.21.15.173:53699
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:221)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:899)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:275)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)
17/03/15 15:09:06 ERROR TaskSchedulerImpl: Lost executor 1 on 172.21.15.173: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 15:09:06 WARN TaskSetManager: Lost task 9.0 in stage 87.1 (TID 364, 172.21.15.173, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 15:09:06 WARN TaskSetManager: Lost task 1.0 in stage 76.1 (TID 367, 172.21.15.173, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 15:09:06 WARN TaskSetManager: Lost task 0.0 in stage 76.1 (TID 366, 172.21.15.173, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 15:09:06 WARN TaskSetManager: Lost task 2.0 in stage 76.1 (TID 368, 172.21.15.173, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 15:09:06 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=78, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=388096
17/03/15 15:09:06 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=402903 lastFlushOffset=388215 createNewBlock=false
17/03/15 15:09:06 DEBUG DFSClient: Queued packet 78
17/03/15 15:09:06 DEBUG DFSClient: Waiting for ack for: 78
17/03/15 15:09:06 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 78 offsetInBlock: 388096 lastPacketInBlock: false lastByteOffsetInBlock: 402903
17/03/15 15:09:06 DEBUG DFSClient: DFSClient seqno: 78 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 76:>                                                        (0 + 0) / 10]17/03/15 15:09:14 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=79, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=402432
17/03/15 15:09:14 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=402903 lastFlushOffset=402903 createNewBlock=false
17/03/15 15:09:14 DEBUG DFSClient: Waiting for ack for: 78
17/03/15 15:09:14 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=80, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=402432
17/03/15 15:09:14 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=402903 lastFlushOffset=402903 createNewBlock=false
17/03/15 15:09:14 DEBUG DFSClient: Waiting for ack for: 78
[Stage 76:>                                                        (0 + 4) / 10]17/03/15 15:09:18 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:09:18 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:09:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:09:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #24
17/03/15 15:09:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #24
17/03/15 15:09:18 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 15:09:18 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:09:18 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 76:=====>                                                   (1 + 4) / 10][Stage 76:======================>                                  (4 + 4) / 10][Stage 76:============================>                            (5 + 4) / 10][Stage 76:==================================>                      (6 + 4) / 10]17/03/15 15:09:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:09:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 76:=======================================>                 (7 + 3) / 10][Stage 76:=============================================>           (8 + 2) / 10][Stage 76:===================================================>     (9 + 1) / 10]17/03/15 15:09:30 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=81, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=402432
17/03/15 15:09:30 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=412010 lastFlushOffset=402903 createNewBlock=false
17/03/15 15:09:30 DEBUG DFSClient: Queued packet 81
17/03/15 15:09:30 DEBUG DFSClient: Waiting for ack for: 81
17/03/15 15:09:30 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 81 offsetInBlock: 402432 lastPacketInBlock: false lastByteOffsetInBlock: 412010
17/03/15 15:09:30 DEBUG DFSClient: DFSClient seqno: 81 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 77:>                                                        (0 + 4) / 10][Stage 77:======================>                                  (4 + 4) / 10][Stage 77:=============================================>           (8 + 2) / 10]17/03/15 15:09:43 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=82, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=411648
17/03/15 15:09:43 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=420420 lastFlushOffset=412010 createNewBlock=false
17/03/15 15:09:43 DEBUG DFSClient: Queued packet 82
17/03/15 15:09:43 DEBUG DFSClient: Waiting for ack for: 82
17/03/15 15:09:43 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 82 offsetInBlock: 411648 lastPacketInBlock: false lastByteOffsetInBlock: 420420
17/03/15 15:09:43 DEBUG DFSClient: DFSClient seqno: 82 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 15:09:43 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=83, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=420352
17/03/15 15:09:43 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=425410 lastFlushOffset=420420 createNewBlock=false
17/03/15 15:09:43 DEBUG DFSClient: Queued packet 83
17/03/15 15:09:43 DEBUG DFSClient: Waiting for ack for: 83
17/03/15 15:09:43 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 83 offsetInBlock: 420352 lastPacketInBlock: false lastByteOffsetInBlock: 425410
17/03/15 15:09:43 DEBUG DFSClient: DFSClient seqno: 83 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 79:>                                                        (0 + 4) / 10][Stage 79:======================>                                  (4 + 4) / 10][Stage 79:=============================================>           (8 + 2) / 10]17/03/15 15:09:45 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=84, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=424960
17/03/15 15:09:45 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=435281 lastFlushOffset=425410 createNewBlock=false
17/03/15 15:09:45 DEBUG DFSClient: Queued packet 84
17/03/15 15:09:45 DEBUG DFSClient: Waiting for ack for: 84
17/03/15 15:09:45 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 84 offsetInBlock: 424960 lastPacketInBlock: false lastByteOffsetInBlock: 435281
17/03/15 15:09:45 DEBUG DFSClient: DFSClient seqno: 84 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 80:======================>                                  (4 + 4) / 10][Stage 80:=======================================>                 (7 + 3) / 10][Stage 80:===================================================>     (9 + 1) / 10]17/03/15 15:09:46 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=85, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=435200
17/03/15 15:09:46 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=440581 lastFlushOffset=435281 createNewBlock=false
17/03/15 15:09:46 DEBUG DFSClient: Queued packet 85
17/03/15 15:09:46 DEBUG DFSClient: Waiting for ack for: 85
17/03/15 15:09:46 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 85 offsetInBlock: 435200 lastPacketInBlock: false lastByteOffsetInBlock: 440581
17/03/15 15:09:46 DEBUG DFSClient: DFSClient seqno: 85 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 81:>                                                        (0 + 4) / 10][Stage 81:======================>                                  (4 + 4) / 10]17/03/15 15:09:48 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:09:48 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:09:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:09:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #25
17/03/15 15:09:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #25
17/03/15 15:09:48 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 15:09:48 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:09:48 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 81:=======================================>                 (7 + 3) / 10][Stage 81:=============================================>           (8 + 2) / 10]17/03/15 15:09:49 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=86, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=440320
17/03/15 15:09:49 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=451855 lastFlushOffset=440581 createNewBlock=false
17/03/15 15:09:49 DEBUG DFSClient: Queued packet 86
17/03/15 15:09:49 DEBUG DFSClient: Waiting for ack for: 86
17/03/15 15:09:49 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 86 offsetInBlock: 440320 lastPacketInBlock: false lastByteOffsetInBlock: 451855
17/03/15 15:09:49 DEBUG DFSClient: DFSClient seqno: 86 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 82:>                                                        (0 + 4) / 10][Stage 82:======================>                                  (4 + 4) / 10][Stage 82:=============================================>           (8 + 2) / 10]17/03/15 15:09:52 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=87, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=451584
17/03/15 15:09:52 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=457512 lastFlushOffset=451855 createNewBlock=false
17/03/15 15:09:52 DEBUG DFSClient: Queued packet 87
17/03/15 15:09:52 DEBUG DFSClient: Waiting for ack for: 87
17/03/15 15:09:52 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 87 offsetInBlock: 451584 lastPacketInBlock: false lastByteOffsetInBlock: 457512
17/03/15 15:09:52 DEBUG DFSClient: DFSClient seqno: 87 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 83:>                                                        (0 + 4) / 10]17/03/15 15:09:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:09:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 83:=================>                                       (3 + 4) / 10][Stage 83:======================>                                  (4 + 4) / 10]17/03/15 15:10:18 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:10:18 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:10:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:10:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #26
17/03/15 15:10:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #26
17/03/15 15:10:18 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/15 15:10:18 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:10:18 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 83:============================>                            (5 + 4) / 10][Stage 83:============================>                            (5 + 5) / 10][Stage 83:==================================>                      (6 + 4) / 10][Stage 83:=======================================>                 (7 + 3) / 10][Stage 83:=============================================>           (8 + 2) / 10][Stage 83:===================================================>     (9 + 1) / 10]17/03/15 15:10:26 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=88, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=457216
17/03/15 15:10:26 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=468734 lastFlushOffset=457512 createNewBlock=false
17/03/15 15:10:26 DEBUG DFSClient: Queued packet 88
17/03/15 15:10:26 DEBUG DFSClient: Waiting for ack for: 88
17/03/15 15:10:26 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 88 offsetInBlock: 457216 lastPacketInBlock: false lastByteOffsetInBlock: 468734
17/03/15 15:10:27 WARN DFSClient: Slow ReadProcessor read fields took 34563ms (threshold=30000ms); ack: seqno: 88 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
[Stage 84:>                                                        (0 + 4) / 10]17/03/15 15:10:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:10:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 84:=====>                                                   (1 + 4) / 10][Stage 84:===========>                                             (2 + 4) / 10][Stage 84:=================>                                       (3 + 4) / 10][Stage 84:======================>                                  (4 + 4) / 10][Stage 84:==================================>                      (6 + 4) / 10][Stage 84:=======================================>                 (7 + 3) / 10][Stage 84:=============================================>           (8 + 2) / 10][Stage 84:===================================================>     (9 + 1) / 10]17/03/15 15:10:48 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=89, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=468480
17/03/15 15:10:48 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=474337 lastFlushOffset=468734 createNewBlock=false
17/03/15 15:10:48 DEBUG DFSClient: Queued packet 89
17/03/15 15:10:48 DEBUG DFSClient: Waiting for ack for: 89
17/03/15 15:10:48 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 89 offsetInBlock: 468480 lastPacketInBlock: false lastByteOffsetInBlock: 474337
17/03/15 15:10:48 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:10:48 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:10:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:10:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #27
17/03/15 15:10:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #27
17/03/15 15:10:48 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 15:10:48 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:10:48 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
17/03/15 15:10:48 DEBUG DFSClient: DFSClient seqno: 89 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 85:>                                                        (0 + 4) / 10][Stage 85:=====>                                                   (1 + 4) / 10]17/03/15 15:10:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:10:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 85:===========>                                             (2 + 4) / 10][Stage 85:=================>                                       (3 + 4) / 10][Stage 85:======================>                                  (4 + 4) / 10][Stage 85:============================>                            (5 + 4) / 10][Stage 85:==================================>                      (6 + 4) / 10][Stage 85:=======================================>                 (7 + 3) / 10][Stage 85:=============================================>           (8 + 2) / 10][Stage 85:===================================================>     (9 + 1) / 10]17/03/15 15:11:11 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=90, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=474112
17/03/15 15:11:11 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=484865 lastFlushOffset=474337 createNewBlock=false
17/03/15 15:11:11 DEBUG DFSClient: Queued packet 90
17/03/15 15:11:11 DEBUG DFSClient: Waiting for ack for: 90
17/03/15 15:11:11 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 90 offsetInBlock: 474112 lastPacketInBlock: false lastByteOffsetInBlock: 484865
17/03/15 15:11:12 DEBUG DFSClient: DFSClient seqno: 90 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 86:>                                                        (0 + 4) / 10][Stage 86:=====>                                                   (1 + 4) / 10][Stage 86:===========>                                             (2 + 4) / 10]17/03/15 15:11:18 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:11:18 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:11:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:11:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #28
17/03/15 15:11:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #28
17/03/15 15:11:18 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 15:11:18 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:11:18 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 86:=================>                                       (3 + 4) / 10][Stage 86:======================>                                  (4 + 4) / 10][Stage 86:============================>                            (5 + 4) / 10][Stage 86:==================================>                      (6 + 4) / 10]17/03/15 15:11:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:11:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 86:=======================================>                 (7 + 3) / 10][Stage 86:=============================================>           (8 + 2) / 10][Stage 86:===================================================>     (9 + 1) / 10]17/03/15 15:11:41 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=91, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=484864
17/03/15 15:11:41 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=494567 lastFlushOffset=484865 createNewBlock=false
17/03/15 15:11:41 DEBUG DFSClient: Queued packet 91
17/03/15 15:11:41 DEBUG DFSClient: Waiting for ack for: 91
17/03/15 15:11:41 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 91 offsetInBlock: 484864 lastPacketInBlock: false lastByteOffsetInBlock: 494567
17/03/15 15:11:42 WARN DFSClient: Slow ReadProcessor read fields took 30065ms (threshold=30000ms); ack: seqno: 91 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
[Stage 87:>                                                        (0 + 4) / 10]17/03/15 15:11:48 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:11:48 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:11:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:11:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #29
17/03/15 15:11:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #29
17/03/15 15:11:48 DEBUG ProtobufRpcEngine: Call: renewLease took 5ms
17/03/15 15:11:48 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:11:48 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
17/03/15 15:11:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:11:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 87:=====>                                                   (1 + 4) / 10][Stage 87:=================>                                       (3 + 4) / 10][Stage 87:======================>                                  (4 + 4) / 10]17/03/15 15:12:18 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:12:18 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:12:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:12:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #30
17/03/15 15:12:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #30
17/03/15 15:12:18 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 15:12:18 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:12:18 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
17/03/15 15:12:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:12:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/15 15:12:33 WARN TaskSetManager: Lost task 6.0 in stage 87.2 (TID 485, 172.21.15.173, executor 2): java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:331)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:364)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:364)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:356)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:322)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:322)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1303)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:362)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:369)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)

[Stage 87:============================>                            (5 + 4) / 10]17/03/15 15:12:35 WARN TaskSetManager: Lost task 6.1 in stage 87.2 (TID 488, 172.21.15.173, executor 2): FetchFailed(BlockManagerId(2, 172.21.15.173, 45958, None), shuffleId=2, mapId=5, reduceId=6, message=
org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-faaeca3f-6aad-426a-8161-e6f0659ca907/25/shuffle_2_5_0.data, offset=217566, length=36261}
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:356)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-faaeca3f-6aad-426a-8161-e6f0659ca907/25/shuffle_2_5_0.data, offset=217566, length=36261}
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:113)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:349)
	... 39 more
Caused by: java.io.FileNotFoundException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-faaeca3f-6aad-426a-8161-e6f0659ca907/25/shuffle_2_5_0.data (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:98)
	... 40 more

)
17/03/15 15:12:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=92, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=494080
17/03/15 15:12:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=512614 lastFlushOffset=494567 createNewBlock=false
17/03/15 15:12:35 DEBUG DFSClient: Queued packet 92
17/03/15 15:12:35 DEBUG DFSClient: Waiting for ack for: 92
17/03/15 15:12:35 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 92 offsetInBlock: 494080 lastPacketInBlock: false lastByteOffsetInBlock: 512614
17/03/15 15:12:35 WARN DFSClient: Slow ReadProcessor read fields took 53526ms (threshold=30000ms); ack: seqno: 92 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
17/03/15 15:12:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=93, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=512512
17/03/15 15:12:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=512614 lastFlushOffset=512614 createNewBlock=false
17/03/15 15:12:35 DEBUG DFSClient: Waiting for ack for: 92
[Stage 76:>                                                        (0 + 0) / 10]17/03/15 15:12:38 ERROR TaskSchedulerImpl: Lost executor 2 on 172.21.15.173: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 15:12:38 WARN TaskSetManager: Lost task 8.0 in stage 87.2 (TID 487, 172.21.15.173, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 15:12:38 WARN TaskSetManager: Lost task 9.0 in stage 87.2 (TID 489, 172.21.15.173, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 15:12:38 WARN TaskSetManager: Lost task 7.0 in stage 87.2 (TID 486, 172.21.15.173, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 15:12:38 WARN TaskSetManager: Lost task 5.0 in stage 87.2 (TID 484, 172.21.15.173, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 15:12:38 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=94, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=512512
17/03/15 15:12:38 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=520718 lastFlushOffset=512614 createNewBlock=false
17/03/15 15:12:38 DEBUG DFSClient: Queued packet 94
17/03/15 15:12:38 DEBUG DFSClient: Waiting for ack for: 94
17/03/15 15:12:38 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 94 offsetInBlock: 512512 lastPacketInBlock: false lastByteOffsetInBlock: 520718
17/03/15 15:12:38 DEBUG DFSClient: DFSClient seqno: 94 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 15:12:46 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=95, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=520704
17/03/15 15:12:46 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=520718 lastFlushOffset=520718 createNewBlock=false
17/03/15 15:12:46 DEBUG DFSClient: Waiting for ack for: 94
[Stage 76:>                                                        (0 + 3) / 10]17/03/15 15:12:46 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=96, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=520704
17/03/15 15:12:46 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=520718 lastFlushOffset=520718 createNewBlock=false
17/03/15 15:12:46 DEBUG DFSClient: Waiting for ack for: 94
[Stage 76:>                                                        (0 + 4) / 10]17/03/15 15:12:48 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:12:48 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:12:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:12:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #31
17/03/15 15:12:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #31
17/03/15 15:12:48 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/15 15:12:48 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:12:48 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 76:=====>                                                   (1 + 4) / 10][Stage 76:===========>                                             (2 + 4) / 10][Stage 76:======================>                                  (4 + 4) / 10][Stage 76:============================>                            (5 + 4) / 10]17/03/15 15:12:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:12:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 76:==================================>                      (6 + 4) / 10][Stage 76:=============================================>           (8 + 2) / 10][Stage 76:===================================================>     (9 + 1) / 10][Stage 76:========================================================(10 + 0) / 10]17/03/15 15:13:01 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=97, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=520704
17/03/15 15:13:01 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=530042 lastFlushOffset=520718 createNewBlock=false
17/03/15 15:13:01 DEBUG DFSClient: Queued packet 97
17/03/15 15:13:01 DEBUG DFSClient: Waiting for ack for: 97
17/03/15 15:13:01 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 97 offsetInBlock: 520704 lastPacketInBlock: false lastByteOffsetInBlock: 530042
17/03/15 15:13:02 DEBUG DFSClient: DFSClient seqno: 97 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 77:>                                                        (0 + 4) / 10][Stage 77:======================>                                  (4 + 4) / 10][Stage 77:=============================================>           (8 + 2) / 10]17/03/15 15:13:14 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=98, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=529920
17/03/15 15:13:14 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=538470 lastFlushOffset=530042 createNewBlock=false
17/03/15 15:13:14 DEBUG DFSClient: Queued packet 98
17/03/15 15:13:14 DEBUG DFSClient: Waiting for ack for: 98
17/03/15 15:13:14 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 98 offsetInBlock: 529920 lastPacketInBlock: false lastByteOffsetInBlock: 538470
17/03/15 15:13:14 DEBUG DFSClient: DFSClient seqno: 98 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 15:13:14 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=99, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=538112
17/03/15 15:13:14 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=543671 lastFlushOffset=538470 createNewBlock=false
17/03/15 15:13:14 DEBUG DFSClient: Queued packet 99
17/03/15 15:13:14 DEBUG DFSClient: Waiting for ack for: 99
17/03/15 15:13:14 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 99 offsetInBlock: 538112 lastPacketInBlock: false lastByteOffsetInBlock: 543671
17/03/15 15:13:14 DEBUG DFSClient: DFSClient seqno: 99 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 79:>                                                        (0 + 0) / 10][Stage 79:>                                                        (0 + 4) / 10][Stage 79:======================>                                  (4 + 4) / 10][Stage 79:=======================================>                 (7 + 3) / 10][Stage 79:=============================================>           (8 + 2) / 10]17/03/15 15:13:16 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=100, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=543232
17/03/15 15:13:16 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=553952 lastFlushOffset=543671 createNewBlock=false
17/03/15 15:13:16 DEBUG DFSClient: Queued packet 100
17/03/15 15:13:16 DEBUG DFSClient: Waiting for ack for: 100
17/03/15 15:13:16 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 100 offsetInBlock: 543232 lastPacketInBlock: false lastByteOffsetInBlock: 553952
17/03/15 15:13:16 DEBUG DFSClient: DFSClient seqno: 100 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 80:======================>                                  (4 + 4) / 10][Stage 80:=============================================>           (8 + 2) / 10]17/03/15 15:13:17 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=101, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=553472
17/03/15 15:13:17 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=559349 lastFlushOffset=553952 createNewBlock=false
17/03/15 15:13:17 DEBUG DFSClient: Queued packet 101
17/03/15 15:13:17 DEBUG DFSClient: Waiting for ack for: 101
17/03/15 15:13:17 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 101 offsetInBlock: 553472 lastPacketInBlock: false lastByteOffsetInBlock: 559349
17/03/15 15:13:17 DEBUG DFSClient: DFSClient seqno: 101 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 81:>                                                        (0 + 4) / 10]17/03/15 15:13:18 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:13:18 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:13:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
[Stage 81:======================>                                  (4 + 4) / 10]17/03/15 15:13:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #32
17/03/15 15:13:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #32
17/03/15 15:13:18 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 15:13:18 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:13:18 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 81:=============================================>           (8 + 2) / 10]17/03/15 15:13:20 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=102, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=559104
17/03/15 15:13:20 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=570764 lastFlushOffset=559349 createNewBlock=false
17/03/15 15:13:20 DEBUG DFSClient: Queued packet 102
17/03/15 15:13:20 DEBUG DFSClient: Waiting for ack for: 102
17/03/15 15:13:20 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 102 offsetInBlock: 559104 lastPacketInBlock: false lastByteOffsetInBlock: 570764
17/03/15 15:13:20 DEBUG DFSClient: DFSClient seqno: 102 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 82:>                                                        (0 + 4) / 10][Stage 82:======================>                                  (4 + 4) / 10][Stage 82:=======================================>                 (7 + 3) / 10][Stage 82:=============================================>           (8 + 2) / 10]17/03/15 15:13:23 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=103, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=570368
17/03/15 15:13:23 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=576277 lastFlushOffset=570764 createNewBlock=false
17/03/15 15:13:23 DEBUG DFSClient: Queued packet 103
17/03/15 15:13:23 DEBUG DFSClient: Waiting for ack for: 103
17/03/15 15:13:23 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 103 offsetInBlock: 570368 lastPacketInBlock: false lastByteOffsetInBlock: 576277
17/03/15 15:13:23 DEBUG DFSClient: DFSClient seqno: 103 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 83:>                                                        (0 + 4) / 10]17/03/15 15:13:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:13:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 83:=====>                                                   (1 + 4) / 10][Stage 83:===========>                                             (2 + 4) / 10][Stage 83:======================>                                  (4 + 4) / 10][Stage 83:============================>                            (5 + 4) / 10]17/03/15 15:13:48 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:13:48 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:13:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:13:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #33
17/03/15 15:13:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #33
17/03/15 15:13:48 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/15 15:13:48 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:13:48 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 83:==================================>                      (6 + 4) / 10][Stage 83:=======================================>                 (7 + 3) / 10][Stage 83:=============================================>           (8 + 2) / 10][Stage 83:===================================================>     (9 + 1) / 10]17/03/15 15:13:56 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=104, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=576000
17/03/15 15:13:56 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=587298 lastFlushOffset=576277 createNewBlock=false
17/03/15 15:13:56 DEBUG DFSClient: Queued packet 104
17/03/15 15:13:56 DEBUG DFSClient: Waiting for ack for: 104
17/03/15 15:13:56 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 104 offsetInBlock: 576000 lastPacketInBlock: false lastByteOffsetInBlock: 587298
17/03/15 15:13:56 WARN DFSClient: Slow ReadProcessor read fields took 33433ms (threshold=30000ms); ack: seqno: 104 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
[Stage 84:>                                                        (0 + 4) / 10]17/03/15 15:13:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:13:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 84:=====>                                                   (1 + 4) / 10][Stage 84:===========>                                             (2 + 4) / 10][Stage 84:=================>                                       (3 + 4) / 10][Stage 84:======================>                                  (4 + 4) / 10][Stage 84:============================>                            (5 + 4) / 10][Stage 84:==================================>                      (6 + 4) / 10][Stage 84:=======================================>                 (7 + 3) / 10]17/03/15 15:14:18 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:14:18 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:14:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:14:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #34
17/03/15 15:14:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #34
17/03/15 15:14:18 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/15 15:14:18 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:14:18 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 84:=============================================>           (8 + 2) / 10][Stage 84:===================================================>     (9 + 1) / 10]17/03/15 15:14:25 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=105, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=587264
17/03/15 15:14:25 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=592941 lastFlushOffset=587298 createNewBlock=false
17/03/15 15:14:25 DEBUG DFSClient: Queued packet 105
17/03/15 15:14:25 DEBUG DFSClient: Waiting for ack for: 105
17/03/15 15:14:25 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 105 offsetInBlock: 587264 lastPacketInBlock: false lastByteOffsetInBlock: 592941
17/03/15 15:14:25 DEBUG DFSClient: DFSClient seqno: 105 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 85:>                                                        (0 + 4) / 10]17/03/15 15:14:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:14:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 85:===========>                                             (2 + 4) / 10][Stage 85:======================>                                  (4 + 4) / 10][Stage 85:============================>                            (5 + 4) / 10][Stage 85:=======================================>                 (7 + 3) / 10][Stage 85:=============================================>           (8 + 2) / 10][Stage 85:===================================================>     (9 + 1) / 10]17/03/15 15:14:42 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=106, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=592896
17/03/15 15:14:42 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=603818 lastFlushOffset=592941 createNewBlock=false
17/03/15 15:14:42 DEBUG DFSClient: Queued packet 106
17/03/15 15:14:42 DEBUG DFSClient: Waiting for ack for: 106
17/03/15 15:14:42 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 106 offsetInBlock: 592896 lastPacketInBlock: false lastByteOffsetInBlock: 603818
17/03/15 15:14:42 DEBUG DFSClient: DFSClient seqno: 106 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 86:>                                                        (0 + 4) / 10]17/03/15 15:14:48 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:14:48 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:14:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:14:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #35
17/03/15 15:14:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #35
17/03/15 15:14:48 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/15 15:14:48 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:14:48 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 86:=====>                                                   (1 + 4) / 10][Stage 86:===========>                                             (2 + 4) / 10][Stage 86:=================>                                       (3 + 4) / 10][Stage 86:======================>                                  (4 + 4) / 10][Stage 86:============================>                            (5 + 4) / 10][Stage 86:==================================>                      (6 + 4) / 10]17/03/15 15:14:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:14:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 86:=======================================>                 (7 + 3) / 10][Stage 86:=============================================>           (8 + 2) / 10]17/03/15 15:15:11 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=107, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=603648
17/03/15 15:15:11 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=613300 lastFlushOffset=603818 createNewBlock=false
17/03/15 15:15:11 DEBUG DFSClient: Queued packet 107
17/03/15 15:15:11 DEBUG DFSClient: Waiting for ack for: 107
17/03/15 15:15:11 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 107 offsetInBlock: 603648 lastPacketInBlock: false lastByteOffsetInBlock: 613300
17/03/15 15:15:11 DEBUG DFSClient: DFSClient seqno: 107 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 87:>                                                        (0 + 4) / 10]17/03/15 15:15:18 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:15:18 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:15:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:15:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #36
17/03/15 15:15:18 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #36
17/03/15 15:15:18 DEBUG ProtobufRpcEngine: Call: renewLease took 8ms
17/03/15 15:15:18 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:15:18 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
[Stage 87:=====>                                                   (1 + 4) / 10][Stage 87:=================>                                       (3 + 4) / 10][Stage 87:======================>                                  (4 + 4) / 10]17/03/15 15:15:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:15:28 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 87:============================>                            (5 + 4) / 10][Stage 87:==================================>                      (6 + 4) / 10]17/03/15 15:15:48 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:15:48 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:15:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:15:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #37
17/03/15 15:15:48 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #37
17/03/15 15:15:48 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 15:15:48 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1612623195_1
17/03/15 15:15:48 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1612623195_1] with renew id 1 executed
17/03/15 15:15:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:15:58 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/15 15:16:12 WARN TaskSetManager: Lost task 8.0 in stage 87.3 (TID 608, 172.21.15.173, executor 3): java.lang.OutOfMemoryError: GC overhead limit exceeded
	at com.esotericsoftware.kryo.io.Input.readDoubles(Input.java:885)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$DoubleArraySerializer.read(DefaultArraySerializers.java:222)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$DoubleArraySerializer.read(DefaultArraySerializers.java:205)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)
	at com.twitter.chill.Tuple4Serializer.read(TupleSerializers.scala:70)
	at com.twitter.chill.Tuple4Serializer.read(TupleSerializers.scala:59)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:396)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:307)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:244)
	at org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:159)
	at org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:189)
	at org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:186)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)

17/03/15 15:16:12 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=108, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=612864
17/03/15 15:16:12 WARN TaskSetManager: Lost task 7.0 in stage 87.3 (TID 607, 172.21.15.173, executor 3): java.io.FileNotFoundException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-310b88f2-ce9f-45e2-baf2-6712324ccc67/26/temp_shuffle_0cd39b72-1d11-4360-877f-6d4057d076d8 (没有那个文件或目录)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:229)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:152)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/03/15 15:16:12 WARN TaskSetManager: Lost task 6.0 in stage 87.3 (TID 606, 172.21.15.173, executor 3): java.io.FileNotFoundException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-310b88f2-ce9f-45e2-baf2-6712324ccc67/39/temp_shuffle_feced880-7e97-43e7-9bbc-a5b0a2d520d0 (没有那个文件或目录)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:229)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:152)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/03/15 15:16:12 WARN TaskSetManager: Lost task 9.0 in stage 87.3 (TID 609, 172.21.15.173, executor 3): FetchFailed(BlockManagerId(3, 172.21.15.173, 56700, None), shuffleId=2, mapId=1, reduceId=9, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-310b88f2-ce9f-45e2-baf2-6712324ccc67/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-310b88f2-ce9f-45e2-baf2-6712324ccc67/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:199)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:302)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:269)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:303)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:132)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	... 23 more

)
17/03/15 15:16:12 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=639464 lastFlushOffset=613300 createNewBlock=false
17/03/15 15:16:12 DEBUG DFSClient: Queued packet 108
17/03/15 15:16:12 DEBUG DFSClient: Waiting for ack for: 108
17/03/15 15:16:12 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 108 offsetInBlock: 612864 lastPacketInBlock: false lastByteOffsetInBlock: 639464
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 87 (mapPartitions at VertexRDD.scala:356) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-310b88f2-ce9f-45e2-baf2-6712324ccc67/0d/shuffle_2_1_0.index (没有那个文件或目录) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58) 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32) 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39) 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439) 	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89) 	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73) 	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71) 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285) 	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54) 	at org.apache.spark.scheduler.Task.run(Task.scala:114) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) 	at java.lang.Thread.run(Thread.java:745) Caused by: java.io.FileNotFoundException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-310b88f2-ce9f-45e2-baf2-6712324ccc67/0d/shuffle_2_1_0.index (没有那个文件或目录) 	at java.io.FileInputStream.open(Native Method) 	at java.io.FileInputStream.<init>(FileInputStream.java:146) 	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:199) 	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:302) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:269) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:303) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:132) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338) 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336) 	... 23 more 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1456)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1444)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1443)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1443)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1273)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1668)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1626)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1615)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2016)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2037)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2056)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2081)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1159)
	at src.main.scala.SVDPlusPlusApp$.main(SVDPlusPlusApp.scala:105)
	at src.main.scala.SVDPlusPlusApp.main(SVDPlusPlusApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:728)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 56700, None),rdd_90_8,StorageLevel(1 replicas),0,0))
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 56700, None),rdd_90_6,StorageLevel(1 replicas),0,0))
17/03/15 15:16:13 WARN TaskSetManager: Lost task 8.1 in stage 87.3 (TID 610, 172.21.15.173, executor 3): FetchFailed(BlockManagerId(3, 172.21.15.173, 56700, None), shuffleId=7, mapId=2, reduceId=8, message=
org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-310b88f2-ce9f-45e2-baf2-6712324ccc67/1d/shuffle_7_2_0.data, offset=79798113, length=9974620}
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:356)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-310b88f2-ce9f-45e2-baf2-6712324ccc67/1d/shuffle_7_2_0.data, offset=79798113, length=9974620}
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:113)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:349)
	... 36 more
Caused by: java.io.FileNotFoundException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-310b88f2-ce9f-45e2-baf2-6712324ccc67/1d/shuffle_7_2_0.data (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:98)
	... 37 more

)
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(87,3,ShuffleMapTask,FetchFailed(BlockManagerId(3, 172.21.15.173, 56700, None),7,2,8,org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-310b88f2-ce9f-45e2-baf2-6712324ccc67/1d/shuffle_7_2_0.data, offset=79798113, length=9974620}
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:356)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-310b88f2-ce9f-45e2-baf2-6712324ccc67/1d/shuffle_7_2_0.data, offset=79798113, length=9974620}
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:113)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:349)
	... 36 more
Caused by: java.io.FileNotFoundException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-310b88f2-ce9f-45e2-baf2-6712324ccc67/1d/shuffle_7_2_0.data (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:98)
	... 37 more
),org.apache.spark.scheduler.TaskInfo@19a62d0,null)
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockManagerAdded(1489562173023,BlockManagerId(3, 172.21.15.173, 56700, None),956615884)
17/03/15 15:16:13 WARN TaskSetManager: Lost task 6.1 in stage 87.3 (TID 612, 172.21.15.173, executor 3): FetchFailed(BlockManagerId(3, 172.21.15.173, 56700, None), shuffleId=2, mapId=1, reduceId=6, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-310b88f2-ce9f-45e2-baf2-6712324ccc67/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-310b88f2-ce9f-45e2-baf2-6712324ccc67/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:199)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:302)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:269)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:303)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:132)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	... 23 more

)
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(87,3,ShuffleMapTask,FetchFailed(BlockManagerId(3, 172.21.15.173, 56700, None),2,1,6,org.apache.spark.shuffle.FetchFailedException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-310b88f2-ce9f-45e2-baf2-6712324ccc67/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-b6269300-d6a0-4ac2-93d7-0ea6f64949d2/blockmgr-310b88f2-ce9f-45e2-baf2-6712324ccc67/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:199)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:302)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:269)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:303)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:132)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	... 23 more
),org.apache.spark.scheduler.TaskInfo@6cd38dc5,null)
17/03/15 15:16:13 WARN DFSClient: Slow ReadProcessor read fields took 61598ms (threshold=30000ms); ack: seqno: 108 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
17/03/15 15:16:13 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=109, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=638976
17/03/15 15:16:13 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=639464 lastFlushOffset=639464 createNewBlock=false
17/03/15 15:16:13 DEBUG DFSClient: Waiting for ack for: 108
17/03/15 15:16:13 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=110, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=638976
17/03/15 15:16:13 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=639464 lastFlushOffset=639464 createNewBlock=false
17/03/15 15:16:13 DEBUG DFSClient: Waiting for ack for: 108
17/03/15 15:16:13 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=111, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=638976
17/03/15 15:16:13 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=639464 lastFlushOffset=639464 createNewBlock=false
17/03/15 15:16:13 DEBUG DFSClient: Waiting for ack for: 108
17/03/15 15:16:13 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=112, src=/eventLogs/app-20170315150047-0022.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=638976
17/03/15 15:16:13 DEBUG DFSClient: Queued packet 112
17/03/15 15:16:13 DEBUG DFSClient: Queued packet 113
17/03/15 15:16:13 DEBUG DFSClient: Waiting for ack for: 113
17/03/15 15:16:13 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 112 offsetInBlock: 638976 lastPacketInBlock: false lastByteOffsetInBlock: 643767
17/03/15 15:16:13 DEBUG DFSClient: DFSClient seqno: 112 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 15:16:13 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283 sending packet packet seqno: 113 offsetInBlock: 643767 lastPacketInBlock: true lastByteOffsetInBlock: 643767
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 56700, None),rdd_3_1,StorageLevel(1 replicas),0,0))
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 56700, None),rdd_23_9,StorageLevel(memory, deserialized, 1 replicas),32222640,0))
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 56700, None),broadcast_52_piece0,StorageLevel(1 replicas),0,0))
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 56700, None),rdd_90_1,StorageLevel(memory, deserialized, 1 replicas),141672216,0))
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 56700, None),broadcast_0_piece0,StorageLevel(1 replicas),0,0))
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 56700, None),broadcast_60_piece0,StorageLevel(1 replicas),0,0))
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 56700, None),broadcast_56_piece0,StorageLevel(1 replicas),0,0))
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 56700, None),rdd_14_6,StorageLevel(memory, 1 replicas),12927562,0))
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 56700, None),rdd_3_4,StorageLevel(1 replicas),0,0))
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 56700, None),rdd_14_0,StorageLevel(1 replicas),0,0))
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 56700, None),rdd_90_4,StorageLevel(memory, deserialized, 1 replicas),141677168,0))
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 56700, None),rdd_14_9,StorageLevel(memory, 1 replicas),12933983,0))
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 56700, None),broadcast_51_piece0,StorageLevel(1 replicas),0,0))
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 56700, None),rdd_90_7,StorageLevel(1 replicas),0,0))
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 56700, None),rdd_14_3,StorageLevel(1 replicas),0,0))
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 56700, None),broadcast_55_piece0,StorageLevel(1 replicas),0,0))
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 56700, None),rdd_3_6,StorageLevel(1 replicas),0,0))
17/03/15 15:16:13 DEBUG DFSClient: DFSClient seqno: 113 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 15:16:13 DEBUG DFSClient: Closing old block BP-519507147-172.21.15.90-1479901973323:blk_1073809106_68283
17/03/15 15:16:13 DEBUG Client: The ping interval is 60000 ms.
17/03/15 15:16:13 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 15:16:13 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 15:16:13 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #38
17/03/15 15:16:13 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #38
17/03/15 15:16:13 DEBUG ProtobufRpcEngine: Call: complete took 3ms
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 56700, None),rdd_23_5,StorageLevel(memory, deserialized, 1 replicas),32246832,0))
17/03/15 15:16:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(3, 172.21.15.173, 56700, None),broadcast_59_piece0,StorageLevel(1 replicas),0,0))
17/03/15 15:16:13 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #39
17/03/15 15:16:13 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #39
17/03/15 15:16:13 DEBUG ProtobufRpcEngine: Call: complete took 29ms
17/03/15 15:16:13 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #40
17/03/15 15:16:13 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #40
17/03/15 15:16:13 DEBUG ProtobufRpcEngine: Call: getFileInfo took 2ms
17/03/15 15:16:13 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #41
17/03/15 15:16:13 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #41
17/03/15 15:16:13 DEBUG ProtobufRpcEngine: Call: rename took 9ms
17/03/15 15:16:13 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop sending #42
17/03/15 15:16:13 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop got value #42
17/03/15 15:16:13 DEBUG ProtobufRpcEngine: Call: setTimes took 5ms
17/03/15 15:16:13 DEBUG PoolThreadCache: Freed 15 thread-local buffer(s) from thread: shuffle-server-6-4
17/03/15 15:16:13 DEBUG PoolThreadCache: Freed 15 thread-local buffer(s) from thread: shuffle-server-6-3
17/03/15 15:16:13 DEBUG PoolThreadCache: Freed 32 thread-local buffer(s) from thread: shuffle-server-6-2
17/03/15 15:16:13 DEBUG PoolThreadCache: Freed 40 thread-local buffer(s) from thread: rpc-server-3-3
17/03/15 15:16:13 DEBUG PoolThreadCache: Freed 39 thread-local buffer(s) from thread: rpc-server-3-2
17/03/15 15:16:13 DEBUG Client: stopping client from cache: org.apache.hadoop.ipc.Client@66893099
17/03/15 15:16:13 DEBUG Client: removing client from cache: org.apache.hadoop.ipc.Client@66893099
17/03/15 15:16:13 DEBUG Client: stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@66893099
17/03/15 15:16:13 DEBUG Client: Stopping client
17/03/15 15:16:13 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 15:16:13 DEBUG Client: IPC Client (90888093) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
