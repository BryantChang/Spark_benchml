17/03/31 23:06:44 WARN SparkContext: Support for Java 7 is deprecated as of Spark 2.0.0
17/03/31 23:06:44 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time, about=, type=DEFAULT, always=false, sampleName=Ops)
17/03/31 23:06:44 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time, about=, type=DEFAULT, always=false, sampleName=Ops)
17/03/31 23:06:44 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(value=[GetGroups], valueName=Time, about=, type=DEFAULT, always=false, sampleName=Ops)
17/03/31 23:06:44 DEBUG MetricsSystemImpl: UgiMetrics, User and group related metrics
17/03/31 23:06:44 DEBUG Shell: setsid exited with exit code 0
17/03/31 23:06:44 DEBUG Groups:  Creating new Groups object
17/03/31 23:06:44 DEBUG NativeCodeLoader: Trying to load the custom-built native-hadoop library...
17/03/31 23:06:44 DEBUG NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
17/03/31 23:06:44 DEBUG NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
17/03/31 23:06:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/03/31 23:06:44 DEBUG PerformanceAdvisory: Falling back to shell based
17/03/31 23:06:44 DEBUG JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
17/03/31 23:06:44 DEBUG Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
17/03/31 23:06:44 DEBUG UserGroupInformation: hadoop login
17/03/31 23:06:44 DEBUG UserGroupInformation: hadoop login commit
17/03/31 23:06:44 DEBUG UserGroupInformation: using local user:UnixPrincipal: hadoop
17/03/31 23:06:44 DEBUG UserGroupInformation: Using user: "UnixPrincipal: hadoop" with name hadoop
17/03/31 23:06:44 DEBUG UserGroupInformation: User entry: "hadoop"
17/03/31 23:06:44 DEBUG UserGroupInformation: UGI loginUser:hadoop (auth:SIMPLE)
17/03/31 23:06:44 WARN SparkConf: Detected deprecated memory fraction settings: [spark.shuffle.memoryFraction, spark.storage.memoryFraction, spark.storage.unrollFraction]. As of Spark 1.6, execution and storage memory management are unified. All memory fractions used in the old model are now deprecated and no longer read. If you wish to use the old memory management, you may explicitly enable `spark.memory.useLegacyMode` (not recommended).
17/03/31 23:06:44 WARN SparkConf: 
SPARK_CLASSPATH was detected (set to '/home/hadoop/bryantchang/platforms/spark2.0/lib/mysql-connector-java-5.1.40-bin.jar:').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with --driver-class-path to augment the driver classpath
 - spark.executor.extraClassPath to augment the executor classpath
        
17/03/31 23:06:44 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/home/hadoop/bryantchang/platforms/spark2.0/lib/mysql-connector-java-5.1.40-bin.jar:' as a work-around.
17/03/31 23:06:44 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/home/hadoop/bryantchang/platforms/spark2.0/lib/mysql-connector-java-5.1.40-bin.jar:' as a work-around.
17/03/31 23:06:45 DEBUG InternalLoggerFactory: Using SLF4J as the default logging framework
17/03/31 23:06:45 DEBUG PlatformDependent0: java.nio.Buffer.address: available
17/03/31 23:06:45 DEBUG PlatformDependent0: sun.misc.Unsafe.theUnsafe: available
17/03/31 23:06:45 DEBUG PlatformDependent0: sun.misc.Unsafe.copyMemory: available
17/03/31 23:06:45 DEBUG PlatformDependent0: direct buffer constructor: available
17/03/31 23:06:45 DEBUG PlatformDependent0: java.nio.Bits.unaligned: available, true
17/03/31 23:06:45 DEBUG PlatformDependent0: java.nio.DirectByteBuffer.<init>(long, int): available
17/03/31 23:06:45 DEBUG Cleaner0: java.nio.ByteBuffer.cleaner(): available
17/03/31 23:06:45 DEBUG PlatformDependent: Java version: 7
17/03/31 23:06:45 DEBUG PlatformDependent: -Dio.netty.noUnsafe: false
17/03/31 23:06:45 DEBUG PlatformDependent: sun.misc.Unsafe: available
17/03/31 23:06:45 DEBUG PlatformDependent: -Dio.netty.noJavassist: false
17/03/31 23:06:45 DEBUG PlatformDependent: Javassist: available
17/03/31 23:06:45 DEBUG PlatformDependent: -Dio.netty.tmpdir: /tmp (java.io.tmpdir)
17/03/31 23:06:45 DEBUG PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)
17/03/31 23:06:45 DEBUG PlatformDependent: -Dio.netty.noPreferDirect: false
17/03/31 23:06:45 DEBUG PlatformDependent: io.netty.maxDirectMemory: 1342177280 bytes
17/03/31 23:06:45 DEBUG JavassistTypeParameterMatcherGenerator: Generated: io.netty.util.internal.__matchers__.org.apache.spark.network.protocol.MessageMatcher
17/03/31 23:06:45 DEBUG JavassistTypeParameterMatcherGenerator: Generated: io.netty.util.internal.__matchers__.io.netty.buffer.ByteBufMatcher
17/03/31 23:06:45 DEBUG MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 8
17/03/31 23:06:45 DEBUG NioEventLoop: -Dio.netty.noKeySetOptimization: false
17/03/31 23:06:45 DEBUG NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512
17/03/31 23:06:45 DEBUG PlatformDependent: org.jctools-core.MpscChunkedArrayQueue: available
17/03/31 23:06:45 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 8
17/03/31 23:06:45 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 8
17/03/31 23:06:45 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192
17/03/31 23:06:45 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 11
17/03/31 23:06:45 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 16777216
17/03/31 23:06:45 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.tinyCacheSize: 512
17/03/31 23:06:45 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256
17/03/31 23:06:45 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64
17/03/31 23:06:45 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768
17/03/31 23:06:45 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192
17/03/31 23:06:45 DEBUG ThreadLocalRandom: -Dio.netty.initialSeedUniquifier: 0xc1887f2e7cfcba57 (took 0 ms)
17/03/31 23:06:45 DEBUG ByteBufUtil: -Dio.netty.allocator.type: unpooled
17/03/31 23:06:45 DEBUG ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 65536
17/03/31 23:06:45 DEBUG ByteBufUtil: -Dio.netty.maxThreadLocalCharBufferSize: 16384
17/03/31 23:06:45 DEBUG NetUtil: Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%1)
17/03/31 23:06:45 DEBUG NetUtil: /proc/sys/net/core/somaxconn: 128
17/03/31 23:06:45 DEBUG AbstractByteBuf: -Dio.netty.buffer.bytebuf.checkAccessible: true
17/03/31 23:06:45 DEBUG ResourceLeakDetector: -Dio.netty.leakDetection.level: simple
17/03/31 23:06:45 DEBUG ResourceLeakDetector: -Dio.netty.leakDetection.maxRecords: 4
17/03/31 23:06:45 DEBUG ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: io.netty.util.ResourceLeakDetector@7116eed7
17/03/31 23:06:45 DEBUG Recycler: -Dio.netty.recycler.maxCapacity.default: 32768
17/03/31 23:06:45 DEBUG Recycler: -Dio.netty.recycler.maxSharedCapacityFactor: 2
17/03/31 23:06:45 DEBUG Recycler: -Dio.netty.recycler.linkCapacity: 16
17/03/31 23:06:45 DEBUG Recycler: -Dio.netty.recycler.ratio: 8
17/03/31 23:06:45 DEBUG BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false
17/03/31 23:06:45 DEBUG BlockReaderLocal: dfs.client.read.shortcircuit = false
17/03/31 23:06:45 DEBUG BlockReaderLocal: dfs.client.domain.socket.data.traffic = false
17/03/31 23:06:45 DEBUG BlockReaderLocal: dfs.domain.socket.path = 
17/03/31 23:06:45 DEBUG RetryUtils: multipleLinearRandomRetry = null
17/03/31 23:06:45 DEBUG Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcRequestWrapper, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@34e19bf
17/03/31 23:06:45 DEBUG Client: getting client out of cache: org.apache.hadoop.ipc.Client@b7d5486
17/03/31 23:06:46 DEBUG PerformanceAdvisory: Both short-circuit local reads and UNIX domain socket are disabled.
17/03/31 23:06:46 DEBUG DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
17/03/31 23:06:46 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:06:46 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:06:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:06:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #0
17/03/31 23:06:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #0
17/03/31 23:06:46 DEBUG ProtobufRpcEngine: Call: getFileInfo took 26ms
17/03/31 23:06:46 DEBUG DFSClient: /eventLogs/app-20170331230645-0012.lz4.inprogress: masked=rw-r--r--
17/03/31 23:06:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #1
17/03/31 23:06:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #1
17/03/31 23:06:46 DEBUG ProtobufRpcEngine: Call: create took 7ms
17/03/31 23:06:46 DEBUG DFSClient: computePacketChunkSize: src=/eventLogs/app-20170331230645-0012.lz4.inprogress, chunkSize=516, chunksPerPacket=126, packetSize=65016
17/03/31 23:06:46 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 started
17/03/31 23:06:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #2
17/03/31 23:06:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #2
17/03/31 23:06:46 DEBUG ProtobufRpcEngine: Call: setPermission took 6ms
17/03/31 23:06:46 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=0 lastFlushOffset=0 createNewBlock=false
17/03/31 23:06:46 DEBUG DFSClient: Waiting for ack for: -1
17/03/31 23:06:46 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=0 lastFlushOffset=0 createNewBlock=false
17/03/31 23:06:46 DEBUG DFSClient: Waiting for ack for: -1
17/03/31 23:06:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #3
17/03/31 23:06:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #3
17/03/31 23:06:46 DEBUG ProtobufRpcEngine: Call: getFileInfo took 1ms
17/03/31 23:06:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #4
17/03/31 23:06:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #4
17/03/31 23:06:46 DEBUG ProtobufRpcEngine: Call: getListing took 1ms
17/03/31 23:06:46 DEBUG FileInputFormat: Time taken to get FileStatuses: 15
17/03/31 23:06:46 INFO FileInputFormat: Total input paths to process : 10
17/03/31 23:06:46 DEBUG FileInputFormat: Total # of splits generated by getSplits: 16, TimeTaken: 19
17/03/31 23:06:46 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=0 lastFlushOffset=0 createNewBlock=false
17/03/31 23:06:46 DEBUG DFSClient: Waiting for ack for: -1
[Stage 0:>                                                         (0 + 0) / 10]17/03/31 23:06:51 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=0, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0
17/03/31 23:06:51 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=6322 lastFlushOffset=0 createNewBlock=false
17/03/31 23:06:51 DEBUG DFSClient: Queued packet 0
17/03/31 23:06:51 DEBUG DFSClient: Waiting for ack for: 0
17/03/31 23:06:51 DEBUG DFSClient: Allocating new block
17/03/31 23:06:51 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #5
17/03/31 23:06:51 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #5
17/03/31 23:06:51 DEBUG ProtobufRpcEngine: Call: addBlock took 14ms
17/03/31 23:06:51 DEBUG DFSClient: pipeline = DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]
17/03/31 23:06:51 DEBUG DFSClient: Connecting to datanode 172.21.15.173:50010
17/03/31 23:06:51 DEBUG DFSClient: Send buf size 124928
17/03/31 23:06:51 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #6
17/03/31 23:06:51 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #6
17/03/31 23:06:51 DEBUG ProtobufRpcEngine: Call: getServerDefaults took 1ms
17/03/31 23:06:51 DEBUG SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = /172.21.15.173, datanodeId = DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]
17/03/31 23:06:52 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 6322
17/03/31 23:06:52 DEBUG DFSClient: DFSClient seqno: 0 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:06:52 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #7
17/03/31 23:06:52 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #7
17/03/31 23:06:52 DEBUG ProtobufRpcEngine: Call: fsync took 5ms
17/03/31 23:06:52 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=1, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=6144
17/03/31 23:06:52 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=6322 lastFlushOffset=6322 createNewBlock=false
17/03/31 23:06:52 DEBUG DFSClient: Waiting for ack for: 0
17/03/31 23:06:52 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=2, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=6144
17/03/31 23:06:52 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=6322 lastFlushOffset=6322 createNewBlock=false
17/03/31 23:06:52 DEBUG DFSClient: Waiting for ack for: 0
17/03/31 23:06:52 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=3, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=6144
17/03/31 23:06:52 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=6322 lastFlushOffset=6322 createNewBlock=false
17/03/31 23:06:52 DEBUG DFSClient: Waiting for ack for: 0
[Stage 0:>                                                         (0 + 4) / 10][Stage 0:=====>                                                    (1 + 4) / 10][Stage 0:===========>                                              (2 + 4) / 10][Stage 0:=================>                                        (3 + 4) / 10][Stage 0:=======================>                                  (4 + 4) / 10][Stage 0:=============================>                            (5 + 4) / 10][Stage 0:==================================>                       (6 + 4) / 10]17/03/31 23:07:02 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:07:02 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 0:========================================>                 (7 + 3) / 10][Stage 0:==============================================>           (8 + 2) / 10][Stage 0:====================================================>     (9 + 1) / 10]                                                                                17/03/31 23:07:04 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=4, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=6144
17/03/31 23:07:04 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=10264 lastFlushOffset=6322 createNewBlock=false
17/03/31 23:07:04 DEBUG DFSClient: Queued packet 4
17/03/31 23:07:04 DEBUG DFSClient: Waiting for ack for: 4
17/03/31 23:07:04 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 4 offsetInBlock: 6144 lastPacketInBlock: false lastByteOffsetInBlock: 10264
17/03/31 23:07:04 DEBUG DFSClient: DFSClient seqno: 4 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:07:04 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=5, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=10240
17/03/31 23:07:04 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=10264 lastFlushOffset=10264 createNewBlock=false
17/03/31 23:07:04 DEBUG DFSClient: Waiting for ack for: 4
17/03/31 23:07:04 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=6, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=10240
17/03/31 23:07:04 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=14290 lastFlushOffset=10264 createNewBlock=false
17/03/31 23:07:04 DEBUG DFSClient: Queued packet 6
17/03/31 23:07:04 DEBUG DFSClient: Waiting for ack for: 6
17/03/31 23:07:04 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 6 offsetInBlock: 10240 lastPacketInBlock: false lastByteOffsetInBlock: 14290
17/03/31 23:07:04 DEBUG DFSClient: DFSClient seqno: 6 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 1:>                                                         (0 + 4) / 10][Stage 1:=====>                                                    (1 + 4) / 10][Stage 1:=================>                                        (3 + 4) / 10][Stage 1:=============================>                            (5 + 4) / 10][Stage 1:==================================>                       (6 + 4) / 10][Stage 1:==============================================>           (8 + 2) / 10][Stage 1:====================================================>     (9 + 1) / 10]17/03/31 23:07:07 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=7, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=13824
17/03/31 23:07:07 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=19881 lastFlushOffset=14290 createNewBlock=false
17/03/31 23:07:07 DEBUG DFSClient: Queued packet 7
17/03/31 23:07:07 DEBUG DFSClient: Waiting for ack for: 7
17/03/31 23:07:07 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 7 offsetInBlock: 13824 lastPacketInBlock: false lastByteOffsetInBlock: 19881
17/03/31 23:07:07 DEBUG DFSClient: DFSClient seqno: 7 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 2:>                                                         (0 + 4) / 10][Stage 2:=======================>                                  (4 + 4) / 10][Stage 2:==================================>                       (6 + 4) / 10][Stage 2:==============================================>           (8 + 2) / 10]                                                                                17/03/31 23:07:15 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=8, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=19456
17/03/31 23:07:15 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=28321 lastFlushOffset=19881 createNewBlock=false
17/03/31 23:07:15 DEBUG DFSClient: Queued packet 8
17/03/31 23:07:15 DEBUG DFSClient: Waiting for ack for: 8
17/03/31 23:07:15 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 8 offsetInBlock: 19456 lastPacketInBlock: false lastByteOffsetInBlock: 28321
17/03/31 23:07:15 DEBUG DFSClient: DFSClient seqno: 8 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:07:15 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=9, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=28160
17/03/31 23:07:15 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=28321 lastFlushOffset=28321 createNewBlock=false
17/03/31 23:07:15 DEBUG DFSClient: Waiting for ack for: 8
17/03/31 23:07:15 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=10, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=28160
17/03/31 23:07:15 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=28321 lastFlushOffset=28321 createNewBlock=false
17/03/31 23:07:15 DEBUG DFSClient: Waiting for ack for: 8
[Stage 4:>                                                         (0 + 4) / 10]17/03/31 23:07:16 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:07:16 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:07:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:07:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #8
17/03/31 23:07:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #8
17/03/31 23:07:16 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/31 23:07:16 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:07:16 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
[Stage 4:===========>                                              (2 + 4) / 10][Stage 4:=================>                                        (3 + 3) / 10][Stage 4:=======================>                                  (4 + 2) / 10][Stage 4:==================================>                       (6 + 2) / 10][Stage 4:==================================>                       (6 + 4) / 10][Stage 4:==============================================>           (8 + 2) / 10]17/03/31 23:07:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:07:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/31 23:07:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=11, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=28160
17/03/31 23:07:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=39598 lastFlushOffset=28321 createNewBlock=false
17/03/31 23:07:32 DEBUG DFSClient: Queued packet 11
17/03/31 23:07:32 DEBUG DFSClient: Waiting for ack for: 11
17/03/31 23:07:32 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 11 offsetInBlock: 28160 lastPacketInBlock: false lastByteOffsetInBlock: 39598
17/03/31 23:07:32 DEBUG DFSClient: DFSClient seqno: 11 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
                                                                                17/03/31 23:07:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=12, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=39424
17/03/31 23:07:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=48079 lastFlushOffset=39598 createNewBlock=false
17/03/31 23:07:32 DEBUG DFSClient: Queued packet 12
17/03/31 23:07:32 DEBUG DFSClient: Waiting for ack for: 12
17/03/31 23:07:32 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 12 offsetInBlock: 39424 lastPacketInBlock: false lastByteOffsetInBlock: 48079
17/03/31 23:07:32 DEBUG DFSClient: DFSClient seqno: 12 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:07:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=13, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=47616
17/03/31 23:07:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=48079 lastFlushOffset=48079 createNewBlock=false
17/03/31 23:07:32 DEBUG DFSClient: Waiting for ack for: 12
17/03/31 23:07:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=14, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=47616
17/03/31 23:07:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=48079 lastFlushOffset=48079 createNewBlock=false
17/03/31 23:07:32 DEBUG DFSClient: Waiting for ack for: 12
17/03/31 23:07:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=15, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=47616
17/03/31 23:07:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=52733 lastFlushOffset=48079 createNewBlock=false
17/03/31 23:07:32 DEBUG DFSClient: Queued packet 15
17/03/31 23:07:32 DEBUG DFSClient: Waiting for ack for: 15
17/03/31 23:07:32 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 15 offsetInBlock: 47616 lastPacketInBlock: false lastByteOffsetInBlock: 52733
17/03/31 23:07:32 DEBUG DFSClient: DFSClient seqno: 15 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:07:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=16, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=52224
17/03/31 23:07:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=52733 lastFlushOffset=52733 createNewBlock=false
17/03/31 23:07:32 DEBUG DFSClient: Waiting for ack for: 15
17/03/31 23:07:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=17, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=52224
17/03/31 23:07:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=52733 lastFlushOffset=52733 createNewBlock=false
17/03/31 23:07:32 DEBUG DFSClient: Waiting for ack for: 15
17/03/31 23:07:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=18, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=52224
17/03/31 23:07:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=57439 lastFlushOffset=52733 createNewBlock=false
17/03/31 23:07:32 DEBUG DFSClient: Queued packet 18
17/03/31 23:07:32 DEBUG DFSClient: Waiting for ack for: 18
17/03/31 23:07:32 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 18 offsetInBlock: 52224 lastPacketInBlock: false lastByteOffsetInBlock: 57439
17/03/31 23:07:32 DEBUG DFSClient: DFSClient seqno: 18 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:07:33 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=19, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=57344
17/03/31 23:07:33 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=62414 lastFlushOffset=57439 createNewBlock=false
17/03/31 23:07:33 DEBUG DFSClient: Queued packet 19
17/03/31 23:07:33 DEBUG DFSClient: Waiting for ack for: 19
17/03/31 23:07:33 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 19 offsetInBlock: 57344 lastPacketInBlock: false lastByteOffsetInBlock: 62414
17/03/31 23:07:33 DEBUG DFSClient: DFSClient seqno: 19 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 11:>                                                        (0 + 4) / 10][Stage 11:======================>                                  (4 + 4) / 10][Stage 11:=============================================>           (8 + 2) / 10]17/03/31 23:07:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=20, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=61952
17/03/31 23:07:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=68094 lastFlushOffset=62414 createNewBlock=false
17/03/31 23:07:35 DEBUG DFSClient: Queued packet 20
17/03/31 23:07:35 DEBUG DFSClient: Waiting for ack for: 20
17/03/31 23:07:35 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 20 offsetInBlock: 61952 lastPacketInBlock: false lastByteOffsetInBlock: 68094
17/03/31 23:07:35 DEBUG DFSClient: DFSClient seqno: 20 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
                                                                                17/03/31 23:07:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=21, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=67584
17/03/31 23:07:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=77656 lastFlushOffset=68094 createNewBlock=false
17/03/31 23:07:35 DEBUG DFSClient: Queued packet 21
17/03/31 23:07:35 DEBUG DFSClient: Waiting for ack for: 21
17/03/31 23:07:35 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 21 offsetInBlock: 67584 lastPacketInBlock: false lastByteOffsetInBlock: 77656
17/03/31 23:07:35 DEBUG DFSClient: DFSClient seqno: 21 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:07:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=22, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=77312
17/03/31 23:07:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=77656 lastFlushOffset=77656 createNewBlock=false
17/03/31 23:07:35 DEBUG DFSClient: Waiting for ack for: 21
17/03/31 23:07:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=23, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=77312
17/03/31 23:07:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=82550 lastFlushOffset=77656 createNewBlock=false
17/03/31 23:07:35 DEBUG DFSClient: Queued packet 23
17/03/31 23:07:35 DEBUG DFSClient: Waiting for ack for: 23
17/03/31 23:07:35 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 23 offsetInBlock: 77312 lastPacketInBlock: false lastByteOffsetInBlock: 82550
17/03/31 23:07:35 DEBUG DFSClient: DFSClient seqno: 23 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:07:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=24, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=82432
17/03/31 23:07:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=87059 lastFlushOffset=82550 createNewBlock=false
17/03/31 23:07:35 DEBUG DFSClient: Queued packet 24
17/03/31 23:07:35 DEBUG DFSClient: Waiting for ack for: 24
17/03/31 23:07:35 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 24 offsetInBlock: 82432 lastPacketInBlock: false lastByteOffsetInBlock: 87059
17/03/31 23:07:35 DEBUG DFSClient: DFSClient seqno: 24 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:07:36 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=25, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=87040
17/03/31 23:07:36 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=96876 lastFlushOffset=87059 createNewBlock=false
17/03/31 23:07:36 DEBUG DFSClient: Queued packet 25
17/03/31 23:07:36 DEBUG DFSClient: Waiting for ack for: 25
17/03/31 23:07:36 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 25 offsetInBlock: 87040 lastPacketInBlock: false lastByteOffsetInBlock: 96876
17/03/31 23:07:36 DEBUG DFSClient: DFSClient seqno: 25 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:07:36 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=26, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=96768
17/03/31 23:07:36 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=96876 lastFlushOffset=96876 createNewBlock=false
17/03/31 23:07:36 DEBUG DFSClient: Waiting for ack for: 25
17/03/31 23:07:36 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=27, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=96768
17/03/31 23:07:36 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=96876 lastFlushOffset=96876 createNewBlock=false
17/03/31 23:07:36 DEBUG DFSClient: Waiting for ack for: 25
17/03/31 23:07:36 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=28, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=96768
17/03/31 23:07:36 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=96876 lastFlushOffset=96876 createNewBlock=false
17/03/31 23:07:36 DEBUG DFSClient: Waiting for ack for: 25
17/03/31 23:07:36 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=29, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=96768
17/03/31 23:07:36 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=101745 lastFlushOffset=96876 createNewBlock=false
17/03/31 23:07:36 DEBUG DFSClient: Queued packet 29
17/03/31 23:07:36 DEBUG DFSClient: Waiting for ack for: 29
17/03/31 23:07:36 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 29 offsetInBlock: 96768 lastPacketInBlock: false lastByteOffsetInBlock: 101745
17/03/31 23:07:36 DEBUG DFSClient: DFSClient seqno: 29 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 24:>                                                        (0 + 4) / 10][Stage 24:=====>                                                   (1 + 4) / 10][Stage 24:======================>                                  (4 + 4) / 10][Stage 24:=======================================>                 (7 + 2) / 10][Stage 24:=============================================>           (8 + 2) / 10][Stage 24:===================================================>     (9 + 1) / 10]17/03/31 23:07:40 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=30, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=101376
17/03/31 23:07:40 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=106400 lastFlushOffset=101745 createNewBlock=false
17/03/31 23:07:40 DEBUG DFSClient: Queued packet 30
17/03/31 23:07:40 DEBUG DFSClient: Waiting for ack for: 30
17/03/31 23:07:40 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 30 offsetInBlock: 101376 lastPacketInBlock: false lastByteOffsetInBlock: 106400
17/03/31 23:07:40 DEBUG DFSClient: DFSClient seqno: 30 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 25:==================================>                      (6 + 4) / 10][Stage 25:========================================================(10 + 0) / 10]                                                                                17/03/31 23:07:40 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=31, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=105984
17/03/31 23:07:40 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=116680 lastFlushOffset=106400 createNewBlock=false
17/03/31 23:07:40 DEBUG DFSClient: Queued packet 31
17/03/31 23:07:40 DEBUG DFSClient: Waiting for ack for: 31
17/03/31 23:07:40 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 31 offsetInBlock: 105984 lastPacketInBlock: false lastByteOffsetInBlock: 116680
17/03/31 23:07:40 DEBUG DFSClient: DFSClient seqno: 31 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:07:40 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=32, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=116224
17/03/31 23:07:40 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=116680 lastFlushOffset=116680 createNewBlock=false
17/03/31 23:07:40 DEBUG DFSClient: Waiting for ack for: 31
17/03/31 23:07:40 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=33, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=116224
17/03/31 23:07:40 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=121449 lastFlushOffset=116680 createNewBlock=false
17/03/31 23:07:40 DEBUG DFSClient: Queued packet 33
17/03/31 23:07:40 DEBUG DFSClient: Waiting for ack for: 33
17/03/31 23:07:40 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 33 offsetInBlock: 116224 lastPacketInBlock: false lastByteOffsetInBlock: 121449
17/03/31 23:07:40 DEBUG DFSClient: DFSClient seqno: 33 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 32:=================>                                       (3 + 4) / 10][Stage 32:======================>                                  (4 + 3) / 10][Stage 32:============================>                            (5 + 3) / 10][Stage 32:==================================>                      (6 + 3) / 10][Stage 32:=======================================>                 (7 + 3) / 10][Stage 32:=============================================>           (8 + 2) / 10]17/03/31 23:07:45 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=34, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=121344
17/03/31 23:07:45 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=130034 lastFlushOffset=121449 createNewBlock=false
17/03/31 23:07:45 DEBUG DFSClient: Queued packet 34
17/03/31 23:07:45 DEBUG DFSClient: Waiting for ack for: 34
17/03/31 23:07:45 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 34 offsetInBlock: 121344 lastPacketInBlock: false lastByteOffsetInBlock: 130034
17/03/31 23:07:46 DEBUG DFSClient: DFSClient seqno: 34 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:07:46 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:07:46 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:07:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:07:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #9
17/03/31 23:07:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #9
17/03/31 23:07:46 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/31 23:07:46 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:07:46 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
[Stage 33:>                                                        (0 + 4) / 10][Stage 33:=====>                                                   (1 + 4) / 10][Stage 33:============================>                            (5 + 3) / 10][Stage 33:==================================>                      (6 + 3) / 10]17/03/31 23:07:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:07:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 33:==================================>                      (6 + 4) / 10][Stage 33:=======================================>                 (7 + 3) / 10]17/03/31 23:07:59 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=35, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=129536
[Stage 33:===================================================>     (9 + 1) / 10]                                                                                17/03/31 23:08:01 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=147410 lastFlushOffset=130034 createNewBlock=false
17/03/31 23:08:01 DEBUG DFSClient: Queued packet 35
17/03/31 23:08:01 DEBUG DFSClient: Waiting for ack for: 35
17/03/31 23:08:01 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 35 offsetInBlock: 129536 lastPacketInBlock: false lastByteOffsetInBlock: 147410
17/03/31 23:08:01 DEBUG DFSClient: DFSClient seqno: 35 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:08:01 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=36, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=146944
17/03/31 23:08:01 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=147410 lastFlushOffset=147410 createNewBlock=false
17/03/31 23:08:01 DEBUG DFSClient: Waiting for ack for: 35
17/03/31 23:08:01 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=37, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=146944
17/03/31 23:08:01 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=147410 lastFlushOffset=147410 createNewBlock=false
17/03/31 23:08:01 DEBUG DFSClient: Waiting for ack for: 35
17/03/31 23:08:01 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=38, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=146944
17/03/31 23:08:01 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=147410 lastFlushOffset=147410 createNewBlock=false
17/03/31 23:08:01 DEBUG DFSClient: Waiting for ack for: 35
17/03/31 23:08:01 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=39, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=146944
17/03/31 23:08:01 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=151671 lastFlushOffset=147410 createNewBlock=false
17/03/31 23:08:01 DEBUG DFSClient: Queued packet 39
17/03/31 23:08:01 DEBUG DFSClient: Waiting for ack for: 39
17/03/31 23:08:01 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 39 offsetInBlock: 146944 lastPacketInBlock: false lastByteOffsetInBlock: 151671
17/03/31 23:08:01 DEBUG DFSClient: DFSClient seqno: 39 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 41:>                                                        (0 + 4) / 10][Stage 41:=====>                                                   (1 + 4) / 10][Stage 41:===========>                                             (2 + 4) / 10]17/03/31 23:08:16 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:08:16 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:08:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:08:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #10
17/03/31 23:08:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #10
17/03/31 23:08:16 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/31 23:08:16 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:08:16 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
[Stage 41:=================>                                       (3 + 4) / 10][Stage 41:======================>                                  (4 + 4) / 10][Stage 41:============================>                            (5 + 4) / 10][Stage 41:==================================>                      (6 + 4) / 10]17/03/31 23:08:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:08:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 41:=======================================>                 (7 + 3) / 10][Stage 41:=============================================>           (8 + 2) / 10][Stage 41:===================================================>     (9 + 1) / 10]17/03/31 23:08:33 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=40, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=151552
17/03/31 23:08:33 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=161805 lastFlushOffset=151671 createNewBlock=false
17/03/31 23:08:33 DEBUG DFSClient: Queued packet 40
17/03/31 23:08:33 DEBUG DFSClient: Waiting for ack for: 40
17/03/31 23:08:33 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 40 offsetInBlock: 151552 lastPacketInBlock: false lastByteOffsetInBlock: 161805
17/03/31 23:08:33 WARN DFSClient: Slow ReadProcessor read fields took 32677ms (threshold=30000ms); ack: seqno: 40 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
[Stage 42:>                                                        (0 + 4) / 10][Stage 42:=====>                                                   (1 + 4) / 10][Stage 42:===========>                                             (2 + 4) / 10][Stage 42:=================>                                       (3 + 4) / 10][Stage 42:======================>                                  (4 + 4) / 10][Stage 42:============================>                            (5 + 4) / 10][Stage 42:==================================>                      (6 + 4) / 10]17/03/31 23:08:46 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:08:46 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:08:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:08:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #11
17/03/31 23:08:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #11
17/03/31 23:08:46 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/31 23:08:46 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:08:46 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
[Stage 42:=======================================>                 (7 + 3) / 10][Stage 42:=============================================>           (8 + 2) / 10][Stage 42:===================================================>     (9 + 1) / 10]                                                                                17/03/31 23:08:48 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=41, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=161792
17/03/31 23:08:48 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=170653 lastFlushOffset=161805 createNewBlock=false
17/03/31 23:08:48 DEBUG DFSClient: Queued packet 41
17/03/31 23:08:48 DEBUG DFSClient: Waiting for ack for: 41
17/03/31 23:08:48 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 41 offsetInBlock: 161792 lastPacketInBlock: false lastByteOffsetInBlock: 170653
17/03/31 23:08:48 DEBUG DFSClient: DFSClient seqno: 41 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:08:48 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=42, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=170496
17/03/31 23:08:48 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=170653 lastFlushOffset=170653 createNewBlock=false
17/03/31 23:08:48 DEBUG DFSClient: Waiting for ack for: 41
17/03/31 23:08:48 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=43, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=170496
17/03/31 23:08:48 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=178497 lastFlushOffset=170653 createNewBlock=false
17/03/31 23:08:48 DEBUG DFSClient: Queued packet 43
17/03/31 23:08:48 DEBUG DFSClient: Waiting for ack for: 43
17/03/31 23:08:48 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 43 offsetInBlock: 170496 lastPacketInBlock: false lastByteOffsetInBlock: 178497
17/03/31 23:08:48 DEBUG DFSClient: DFSClient seqno: 43 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 51:>                                                        (0 + 4) / 10][Stage 51:===========>                                             (2 + 4) / 10]17/03/31 23:08:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:08:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 51:======================>                                  (4 + 4) / 10][Stage 51:============================>                            (5 + 4) / 10][Stage 51:==================================>                      (6 + 4) / 10][Stage 51:=======================================>                 (7 + 3) / 10][Stage 51:=============================================>           (8 + 2) / 10]17/03/31 23:09:15 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=44, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=178176
17/03/31 23:09:15 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=183732 lastFlushOffset=178497 createNewBlock=false
17/03/31 23:09:15 DEBUG DFSClient: Queued packet 44
17/03/31 23:09:15 DEBUG DFSClient: Waiting for ack for: 44
17/03/31 23:09:15 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 44 offsetInBlock: 178176 lastPacketInBlock: false lastByteOffsetInBlock: 183732
17/03/31 23:09:16 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:09:16 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:09:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:09:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #12
17/03/31 23:09:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #12
17/03/31 23:09:16 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/31 23:09:16 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:09:16 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
17/03/31 23:09:16 DEBUG DFSClient: DFSClient seqno: 44 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 52:>                                                        (0 + 4) / 10]17/03/31 23:09:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:09:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 52:=====>                                                   (1 + 4) / 10][Stage 52:===========>                                             (2 + 4) / 10][Stage 52:=================>                                       (3 + 4) / 10][Stage 52:============================>                            (5 + 4) / 10][Stage 52:==================================>                      (6 + 3) / 10][Stage 52:==================================>                      (6 + 4) / 10]17/03/31 23:09:46 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:09:46 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:09:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:09:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #13
17/03/31 23:09:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #13
17/03/31 23:09:46 DEBUG ProtobufRpcEngine: Call: renewLease took 3ms
17/03/31 23:09:46 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:09:46 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
[Stage 52:=======================================>                 (7 + 3) / 10]17/03/31 23:09:48 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=45, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=183296
[Stage 52:=============================================>           (8 + 2) / 10][Stage 52:===================================================>     (9 + 1) / 10]                                                                                17/03/31 23:09:51 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=201072 lastFlushOffset=183732 createNewBlock=false
17/03/31 23:09:51 DEBUG DFSClient: Queued packet 45
17/03/31 23:09:51 DEBUG DFSClient: Waiting for ack for: 45
17/03/31 23:09:51 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 45 offsetInBlock: 183296 lastPacketInBlock: false lastByteOffsetInBlock: 201072
17/03/31 23:09:51 WARN DFSClient: Slow ReadProcessor read fields took 34916ms (threshold=30000ms); ack: seqno: 45 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
17/03/31 23:09:51 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=46, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=200704
17/03/31 23:09:51 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=201072 lastFlushOffset=201072 createNewBlock=false
17/03/31 23:09:51 DEBUG DFSClient: Waiting for ack for: 45
17/03/31 23:09:51 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=47, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=200704
17/03/31 23:09:51 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=201072 lastFlushOffset=201072 createNewBlock=false
17/03/31 23:09:51 DEBUG DFSClient: Waiting for ack for: 45
17/03/31 23:09:51 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=48, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=200704
17/03/31 23:09:51 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=201072 lastFlushOffset=201072 createNewBlock=false
17/03/31 23:09:51 DEBUG DFSClient: Waiting for ack for: 45
17/03/31 23:09:51 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=49, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=200704
17/03/31 23:09:51 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=209032 lastFlushOffset=201072 createNewBlock=false
17/03/31 23:09:51 DEBUG DFSClient: Queued packet 49
17/03/31 23:09:51 DEBUG DFSClient: Waiting for ack for: 49
17/03/31 23:09:51 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 49 offsetInBlock: 200704 lastPacketInBlock: false lastByteOffsetInBlock: 209032
17/03/31 23:09:51 DEBUG DFSClient: DFSClient seqno: 49 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 62:>                                                        (0 + 4) / 10]17/03/31 23:09:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:09:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 62:=====>                                                   (1 + 4) / 10][Stage 62:===========>                                             (2 + 4) / 10][Stage 62:======================>                                  (4 + 4) / 10][Stage 62:============================>                            (5 + 3) / 10][Stage 62:==================================>                      (6 + 2) / 10][Stage 62:=======================================>                 (7 + 2) / 10][Stage 62:=============================================>           (8 + 2) / 10][Stage 62:===================================================>     (9 + 1) / 10]17/03/31 23:10:12 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=50, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=208896
17/03/31 23:10:12 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=219251 lastFlushOffset=209032 createNewBlock=false
17/03/31 23:10:12 DEBUG DFSClient: Queued packet 50
17/03/31 23:10:12 DEBUG DFSClient: Waiting for ack for: 50
17/03/31 23:10:12 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 50 offsetInBlock: 208896 lastPacketInBlock: false lastByteOffsetInBlock: 219251
17/03/31 23:10:12 DEBUG DFSClient: DFSClient seqno: 50 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 63:>                                                        (0 + 4) / 10]17/03/31 23:10:16 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:10:16 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:10:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:10:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #14
17/03/31 23:10:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #14
17/03/31 23:10:16 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/31 23:10:16 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:10:16 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
[Stage 63:=================>                                       (3 + 4) / 10][Stage 63:======================>                                  (4 + 4) / 10][Stage 63:============================>                            (5 + 4) / 10][Stage 63:=============================================>           (8 + 2) / 10][Stage 63:===================================================>     (9 + 1) / 10]17/03/31 23:10:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:10:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
                                                                                17/03/31 23:10:26 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=51, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=219136
17/03/31 23:10:26 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=232314 lastFlushOffset=219251 createNewBlock=false
17/03/31 23:10:26 DEBUG DFSClient: Queued packet 51
17/03/31 23:10:26 DEBUG DFSClient: Waiting for ack for: 51
17/03/31 23:10:26 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 51 offsetInBlock: 219136 lastPacketInBlock: false lastByteOffsetInBlock: 232314
17/03/31 23:10:26 DEBUG DFSClient: DFSClient seqno: 51 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:10:26 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=52, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=231936
17/03/31 23:10:26 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=232314 lastFlushOffset=232314 createNewBlock=false
17/03/31 23:10:26 DEBUG DFSClient: Waiting for ack for: 51
17/03/31 23:10:26 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=53, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=231936
17/03/31 23:10:26 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=236169 lastFlushOffset=232314 createNewBlock=false
17/03/31 23:10:26 DEBUG DFSClient: Queued packet 53
17/03/31 23:10:26 DEBUG DFSClient: Waiting for ack for: 53
17/03/31 23:10:26 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 53 offsetInBlock: 231936 lastPacketInBlock: false lastByteOffsetInBlock: 236169
17/03/31 23:10:26 DEBUG DFSClient: DFSClient seqno: 53 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 74:>                                                        (0 + 4) / 10][Stage 74:=====>                                                   (1 + 4) / 10][Stage 74:===========>                                             (2 + 4) / 10][Stage 74:=================>                                       (3 + 4) / 10][Stage 74:======================>                                  (4 + 3) / 10][Stage 74:============================>                            (5 + 3) / 10][Stage 74:==================================>                      (6 + 3) / 10][Stage 74:=======================================>                 (7 + 3) / 10][Stage 74:=============================================>           (8 + 2) / 10][Stage 74:===================================================>     (9 + 1) / 10]17/03/31 23:10:42 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=54, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=236032
17/03/31 23:10:42 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=244684 lastFlushOffset=236169 createNewBlock=false
17/03/31 23:10:42 DEBUG DFSClient: Queued packet 54
17/03/31 23:10:42 DEBUG DFSClient: Waiting for ack for: 54
17/03/31 23:10:42 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 54 offsetInBlock: 236032 lastPacketInBlock: false lastByteOffsetInBlock: 244684
17/03/31 23:10:42 DEBUG DFSClient: DFSClient seqno: 54 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 75:>                                                        (0 + 4) / 10]17/03/31 23:10:46 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:10:46 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:10:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:10:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #15
17/03/31 23:10:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #15
17/03/31 23:10:46 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/31 23:10:46 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:10:46 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
17/03/31 23:10:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:10:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 75:===========>                                             (2 + 4) / 10][Stage 75:=================>                                       (3 + 4) / 10][Stage 75:======================>                                  (4 + 4) / 10]17/03/31 23:11:16 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:11:16 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:11:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:11:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #16
17/03/31 23:11:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #16
17/03/31 23:11:16 DEBUG ProtobufRpcEngine: Call: renewLease took 5ms
17/03/31 23:11:16 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:11:16 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
[Stage 75:============================>                            (5 + 4) / 10][Stage 75:==================================>                      (6 + 3) / 10][Stage 75:=======================================>                 (7 + 3) / 10]17/03/31 23:11:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:11:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/31 23:11:26 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=55, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=244224
[Stage 75:=============================================>           (8 + 2) / 10][Stage 75:===================================================>     (9 + 1) / 10]                                                                                17/03/31 23:11:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=262204 lastFlushOffset=244684 createNewBlock=false
17/03/31 23:11:32 DEBUG DFSClient: Queued packet 55
17/03/31 23:11:32 DEBUG DFSClient: Waiting for ack for: 55
17/03/31 23:11:32 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 55 offsetInBlock: 244224 lastPacketInBlock: false lastByteOffsetInBlock: 262204
17/03/31 23:11:32 WARN DFSClient: Slow ReadProcessor read fields took 49655ms (threshold=30000ms); ack: seqno: 55 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
17/03/31 23:11:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=56, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=262144
17/03/31 23:11:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=262204 lastFlushOffset=262204 createNewBlock=false
17/03/31 23:11:32 DEBUG DFSClient: Waiting for ack for: 55
17/03/31 23:11:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=57, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=262144
17/03/31 23:11:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=262204 lastFlushOffset=262204 createNewBlock=false
17/03/31 23:11:32 DEBUG DFSClient: Waiting for ack for: 55
17/03/31 23:11:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=58, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=262144
17/03/31 23:11:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=262204 lastFlushOffset=262204 createNewBlock=false
17/03/31 23:11:32 DEBUG DFSClient: Waiting for ack for: 55
17/03/31 23:11:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=59, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=262144
17/03/31 23:11:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=273194 lastFlushOffset=262204 createNewBlock=false
17/03/31 23:11:32 DEBUG DFSClient: Queued packet 59
17/03/31 23:11:32 DEBUG DFSClient: Waiting for ack for: 59
17/03/31 23:11:32 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 59 offsetInBlock: 262144 lastPacketInBlock: false lastByteOffsetInBlock: 273194
17/03/31 23:11:32 DEBUG DFSClient: DFSClient seqno: 59 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 87:>                                                        (0 + 4) / 10]17/03/31 23:11:46 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:11:46 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:11:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:11:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #17
17/03/31 23:11:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #17
17/03/31 23:11:46 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/31 23:11:46 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:11:46 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
[Stage 87:=====>                                                   (1 + 4) / 10][Stage 87:===========>                                             (2 + 4) / 10][Stage 87:=================>                                       (3 + 4) / 10][Stage 87:======================>                                  (4 + 4) / 10]17/03/31 23:11:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:11:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 87:==================================>                      (6 + 4) / 10][Stage 87:=======================================>                 (7 + 3) / 10][Stage 87:=============================================>           (8 + 2) / 10]17/03/31 23:12:16 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:12:16 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:12:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:12:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #18
17/03/31 23:12:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #18
17/03/31 23:12:16 DEBUG ProtobufRpcEngine: Call: renewLease took 5ms
17/03/31 23:12:16 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:12:16 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
17/03/31 23:12:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:12:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 87:===================================================>     (9 + 1) / 10]17/03/31 23:12:31 WARN TaskSetManager: Lost task 9.0 in stage 87.0 (TID 239, 172.21.15.173, executor 1): java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.lang.Long.valueOf(Long.java:577)
	at scala.runtime.BoxesRunTime.boxToLong(BoxesRunTime.java:69)
	at org.apache.spark.util.collection.OpenHashSet.addWithoutResize$mcJ$sp(OpenHashSet.scala:135)
	at org.apache.spark.graphx.util.collection.GraphXPrimitiveKeyOpenHashMap$mcJI$sp.changeValue$mcJI$sp(GraphXPrimitiveKeyOpenHashMap.scala:103)
	at org.apache.spark.graphx.impl.EdgePartitionBuilder.toEdgePartition(EdgePartitionBuilder.scala:60)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:110)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:105)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:845)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:845)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/03/31 23:12:33 ERROR TaskSchedulerImpl: Lost executor 1 on 172.21.15.173: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/31 23:12:33 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=60, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=272896
17/03/31 23:12:33 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=283945 lastFlushOffset=273194 createNewBlock=false
17/03/31 23:12:33 DEBUG DFSClient: Queued packet 60
17/03/31 23:12:33 DEBUG DFSClient: Waiting for ack for: 60
17/03/31 23:12:33 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 60 offsetInBlock: 272896 lastPacketInBlock: false lastByteOffsetInBlock: 283945
[Stage 87:==================================================>     (9 + -4) / 10]17/03/31 23:12:34 WARN DFSClient: Slow ReadProcessor read fields took 61549ms (threshold=30000ms); ack: seqno: 60 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
17/03/31 23:12:34 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=61, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=283648
17/03/31 23:12:34 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=283945 lastFlushOffset=283945 createNewBlock=false
17/03/31 23:12:34 DEBUG DFSClient: Waiting for ack for: 60
[Stage 87:==================================================>     (9 + -3) / 10][Stage 87:=======================================================(11 + -5) / 10][Stage 87:=======================================================(11 + -3) / 10]17/03/31 23:12:38 WARN TaskSetManager: Lost task 5.1 in stage 87.0 (TID 243, 172.21.15.173, executor 0): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=5, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/31 23:12:38 WARN TaskSetManager: Lost task 1.1 in stage 87.0 (TID 242, 172.21.15.173, executor 0): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=1, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/31 23:12:38 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=62, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=283648
17/03/31 23:12:38 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=289916 lastFlushOffset=283945 createNewBlock=false
17/03/31 23:12:38 DEBUG DFSClient: Queued packet 62
17/03/31 23:12:38 DEBUG DFSClient: Waiting for ack for: 62
17/03/31 23:12:38 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 62 offsetInBlock: 283648 lastPacketInBlock: false lastByteOffsetInBlock: 289916
17/03/31 23:12:38 DEBUG DFSClient: DFSClient seqno: 62 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:12:38 WARN TaskSetManager: Lost task 3.1 in stage 87.0 (TID 244, 172.21.15.173, executor 0): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=3, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/31 23:12:38 WARN TaskSetManager: Lost task 4.1 in stage 87.0 (TID 245, 172.21.15.173, executor 0): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=4, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
[Stage 76:>                                                         (0 + 2) / 5]17/03/31 23:12:46 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:12:46 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:12:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:12:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #19
17/03/31 23:12:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #19
17/03/31 23:12:46 DEBUG ProtobufRpcEngine: Call: renewLease took 8ms
17/03/31 23:12:46 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:12:46 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
[Stage 76:===========>                                              (1 + 2) / 5][Stage 76:=======================>                                  (2 + 2) / 5][Stage 76:==================================>                       (3 + 2) / 5]17/03/31 23:12:55 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=63, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=289792
17/03/31 23:12:55 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=293434 lastFlushOffset=289916 createNewBlock=false
17/03/31 23:12:55 DEBUG DFSClient: Queued packet 63
17/03/31 23:12:55 DEBUG DFSClient: Waiting for ack for: 63
17/03/31 23:12:55 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 63 offsetInBlock: 289792 lastPacketInBlock: false lastByteOffsetInBlock: 293434
17/03/31 23:12:55 DEBUG DFSClient: DFSClient seqno: 63 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:12:55 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=64, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=293376
17/03/31 23:12:55 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=293434 lastFlushOffset=293434 createNewBlock=false
17/03/31 23:12:55 DEBUG DFSClient: Waiting for ack for: 63
[Stage 76:==============================================>           (4 + 1) / 5]17/03/31 23:12:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:12:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/31 23:12:56 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=65, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=293376
17/03/31 23:12:56 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=293434 lastFlushOffset=293434 createNewBlock=false
17/03/31 23:12:56 DEBUG DFSClient: Waiting for ack for: 63
[Stage 77:>                                                         (0 + 4) / 4][Stage 77:=============================>                            (2 + 2) / 4]17/03/31 23:13:06 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=66, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=293376
17/03/31 23:13:06 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=302533 lastFlushOffset=293434 createNewBlock=false
17/03/31 23:13:06 DEBUG DFSClient: Queued packet 66
17/03/31 23:13:06 DEBUG DFSClient: Waiting for ack for: 66
17/03/31 23:13:06 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 66 offsetInBlock: 293376 lastPacketInBlock: false lastByteOffsetInBlock: 302533
17/03/31 23:13:06 DEBUG DFSClient: DFSClient seqno: 66 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:13:06 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=67, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=302080
17/03/31 23:13:06 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=308200 lastFlushOffset=302533 createNewBlock=false
17/03/31 23:13:06 DEBUG DFSClient: Queued packet 67
17/03/31 23:13:06 DEBUG DFSClient: Waiting for ack for: 67
17/03/31 23:13:06 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 67 offsetInBlock: 302080 lastPacketInBlock: false lastByteOffsetInBlock: 308200
17/03/31 23:13:06 DEBUG DFSClient: DFSClient seqno: 67 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 79:=============================>                            (2 + 2) / 4]17/03/31 23:13:07 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=68, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=307712
17/03/31 23:13:07 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=313472 lastFlushOffset=308200 createNewBlock=false
17/03/31 23:13:07 DEBUG DFSClient: Queued packet 68
17/03/31 23:13:07 DEBUG DFSClient: Waiting for ack for: 68
17/03/31 23:13:07 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 68 offsetInBlock: 307712 lastPacketInBlock: false lastByteOffsetInBlock: 313472
17/03/31 23:13:07 DEBUG DFSClient: DFSClient seqno: 68 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 80:>                                                         (0 + 0) / 6][Stage 80:>                                                         (0 + 4) / 6][Stage 80:======================================>                   (4 + 2) / 6]17/03/31 23:13:08 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=69, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=313344
17/03/31 23:13:08 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=318704 lastFlushOffset=313472 createNewBlock=false
17/03/31 23:13:08 DEBUG DFSClient: Queued packet 69
17/03/31 23:13:08 DEBUG DFSClient: Waiting for ack for: 69
17/03/31 23:13:08 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 69 offsetInBlock: 313344 lastPacketInBlock: false lastByteOffsetInBlock: 318704
17/03/31 23:13:08 DEBUG DFSClient: DFSClient seqno: 69 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 81:>                                                         (0 + 4) / 4][Stage 81:==============>                                           (1 + 3) / 4]17/03/31 23:13:10 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=70, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=318464
17/03/31 23:13:10 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=318704 lastFlushOffset=318704 createNewBlock=false
17/03/31 23:13:10 DEBUG DFSClient: Waiting for ack for: 69
[Stage 82:>                                                         (0 + 4) / 6][Stage 82:=========>                                                (1 + 4) / 6][Stage 82:======================================>                   (4 + 2) / 6][Stage 82:================================================>         (5 + 1) / 6]17/03/31 23:13:12 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=71, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=318464
17/03/31 23:13:12 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=329433 lastFlushOffset=318704 createNewBlock=false
17/03/31 23:13:12 DEBUG DFSClient: Queued packet 71
17/03/31 23:13:12 DEBUG DFSClient: Waiting for ack for: 71
17/03/31 23:13:12 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 71 offsetInBlock: 318464 lastPacketInBlock: false lastByteOffsetInBlock: 329433
17/03/31 23:13:12 DEBUG DFSClient: DFSClient seqno: 71 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 83:>                                                         (0 + 4) / 5]17/03/31 23:13:16 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:13:16 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:13:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:13:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #20
17/03/31 23:13:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #20
17/03/31 23:13:16 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/31 23:13:16 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:13:16 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
[Stage 83:=======================>                                  (2 + 3) / 5]17/03/31 23:13:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:13:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 83:==================================>                       (3 + 2) / 5][Stage 83:==============================================>           (4 + 1) / 5]17/03/31 23:13:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=72, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=329216
17/03/31 23:13:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=335053 lastFlushOffset=329433 createNewBlock=false
17/03/31 23:13:32 DEBUG DFSClient: Queued packet 72
17/03/31 23:13:32 DEBUG DFSClient: Waiting for ack for: 72
17/03/31 23:13:32 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 72 offsetInBlock: 329216 lastPacketInBlock: false lastByteOffsetInBlock: 335053
17/03/31 23:13:32 DEBUG DFSClient: DFSClient seqno: 72 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 84:>                                                         (0 + 4) / 4][Stage 84:==============>                                           (1 + 3) / 4][Stage 84:=============================>                            (2 + 2) / 4][Stage 84:===========================================>              (3 + 1) / 4]17/03/31 23:13:42 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=73, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=334848
17/03/31 23:13:42 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=340532 lastFlushOffset=335053 createNewBlock=false
17/03/31 23:13:42 DEBUG DFSClient: Queued packet 73
17/03/31 23:13:42 DEBUG DFSClient: Waiting for ack for: 73
17/03/31 23:13:42 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 73 offsetInBlock: 334848 lastPacketInBlock: false lastByteOffsetInBlock: 340532
17/03/31 23:13:42 DEBUG DFSClient: DFSClient seqno: 73 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 85:>                                                         (0 + 4) / 4]17/03/31 23:13:46 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:13:46 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:13:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:13:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #21
17/03/31 23:13:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #21
17/03/31 23:13:46 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/31 23:13:46 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:13:46 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
[Stage 85:==============>                                           (1 + 3) / 4][Stage 85:=============================>                            (2 + 2) / 4][Stage 85:===========================================>              (3 + 1) / 4]17/03/31 23:13:49 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=74, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=340480
17/03/31 23:13:49 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=346019 lastFlushOffset=340532 createNewBlock=false
17/03/31 23:13:49 DEBUG DFSClient: Queued packet 74
17/03/31 23:13:49 DEBUG DFSClient: Waiting for ack for: 74
17/03/31 23:13:49 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 74 offsetInBlock: 340480 lastPacketInBlock: false lastByteOffsetInBlock: 346019
17/03/31 23:13:49 DEBUG DFSClient: DFSClient seqno: 74 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 86:>                                                         (0 + 4) / 6]17/03/31 23:13:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:13:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 86:=========>                                                (1 + 4) / 6][Stage 86:===================>                                      (2 + 4) / 6][Stage 86:=============================>                            (3 + 3) / 6][Stage 86:======================================>                   (4 + 2) / 6][Stage 86:================================================>         (5 + 1) / 6]17/03/31 23:14:16 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:14:16 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:14:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:14:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #22
17/03/31 23:14:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #22
17/03/31 23:14:16 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/31 23:14:16 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:14:16 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
17/03/31 23:14:19 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=75, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=345600
17/03/31 23:14:19 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=351487 lastFlushOffset=346019 createNewBlock=false
17/03/31 23:14:19 DEBUG DFSClient: Queued packet 75
17/03/31 23:14:19 DEBUG DFSClient: Waiting for ack for: 75
17/03/31 23:14:19 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 75 offsetInBlock: 345600 lastPacketInBlock: false lastByteOffsetInBlock: 351487
17/03/31 23:14:19 DEBUG DFSClient: DFSClient seqno: 75 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 87:>                                                         (0 + 4) / 4]17/03/31 23:14:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:14:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 87:==============>                                           (1 + 3) / 4][Stage 87:=============================>                            (2 + 2) / 4][Stage 87:===========================================>              (3 + 1) / 4]17/03/31 23:14:34 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=76, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=351232
17/03/31 23:14:34 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=356781 lastFlushOffset=351487 createNewBlock=false
17/03/31 23:14:34 DEBUG DFSClient: Queued packet 76
17/03/31 23:14:34 DEBUG DFSClient: Waiting for ack for: 76
17/03/31 23:14:34 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 76 offsetInBlock: 351232 lastPacketInBlock: false lastByteOffsetInBlock: 356781
17/03/31 23:14:34 DEBUG DFSClient: DFSClient seqno: 76 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 88:>                                                        (0 + 4) / 10][Stage 88:===========>                                             (2 + 4) / 10]17/03/31 23:14:46 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:14:46 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:14:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:14:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #23
17/03/31 23:14:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #23
17/03/31 23:14:46 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/31 23:14:46 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:14:46 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
[Stage 88:======================>                                  (4 + 3) / 10][Stage 88:======================>                                  (4 + 4) / 10]17/03/31 23:14:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:14:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 88:============================>                            (5 + 4) / 10][Stage 88:==================================>                      (6 + 4) / 10]17/03/31 23:15:16 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:15:16 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:15:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:15:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #24
17/03/31 23:15:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #24
17/03/31 23:15:16 DEBUG ProtobufRpcEngine: Call: renewLease took 5ms
17/03/31 23:15:16 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:15:16 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
[Stage 88:=======================================>                 (7 + 3) / 10][Stage 88:=============================================>           (8 + 2) / 10]17/03/31 23:15:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:15:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 88:===================================================>     (9 + 1) / 10]17/03/31 23:15:31 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=77, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=356352
17/03/31 23:15:31 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=371475 lastFlushOffset=356781 createNewBlock=false
17/03/31 23:15:31 DEBUG DFSClient: Queued packet 77
17/03/31 23:15:31 DEBUG DFSClient: Waiting for ack for: 77
17/03/31 23:15:31 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 77 offsetInBlock: 356352 lastPacketInBlock: false lastByteOffsetInBlock: 371475
17/03/31 23:15:32 WARN DFSClient: Slow ReadProcessor read fields took 57756ms (threshold=30000ms); ack: seqno: 77 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
[Stage 89:>                                                        (0 + 4) / 10]17/03/31 23:15:46 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:15:46 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:15:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:15:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #25
17/03/31 23:15:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #25
17/03/31 23:15:46 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/31 23:15:46 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:15:46 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
[Stage 89:=====>                                                   (1 + 4) / 10]17/03/31 23:15:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:15:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 89:===========>                                             (2 + 4) / 10][Stage 89:=================>                                       (3 + 3) / 10][Stage 89:======================>                                  (4 + 3) / 10][Stage 89:======================>                                  (4 + 4) / 10][Stage 89:============================>                            (5 + 4) / 10]17/03/31 23:16:16 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:16:16 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:16:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:16:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #26
17/03/31 23:16:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #26
17/03/31 23:16:16 DEBUG ProtobufRpcEngine: Call: renewLease took 5ms
17/03/31 23:16:16 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:16:16 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
[Stage 89:==================================>                      (6 + 4) / 10][Stage 89:=======================================>                 (7 + 3) / 10][Stage 89:=============================================>           (8 + 2) / 10]17/03/31 23:16:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:16:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/31 23:16:31 WARN TaskSetManager: Lost task 6.0 in stage 89.0 (TID 321, 172.21.15.173, executor 2): java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at org.apache.spark.graphx.impl.EdgePartitionBuilder.toEdgePartition(EdgePartitionBuilder.scala:43)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:110)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:105)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:845)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:845)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:88)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

17/03/31 23:16:31 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=78, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=371200
[Stage 89:===================================================>     (9 + 1) / 10]17/03/31 23:16:34 ERROR TaskSchedulerImpl: Lost executor 2 on 172.21.15.173: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/31 23:16:34 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=390647 lastFlushOffset=371475 createNewBlock=false
17/03/31 23:16:34 DEBUG DFSClient: Queued packet 78
17/03/31 23:16:34 DEBUG DFSClient: Waiting for ack for: 78
17/03/31 23:16:34 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 78 offsetInBlock: 371200 lastPacketInBlock: false lastByteOffsetInBlock: 390647
17/03/31 23:16:34 WARN DFSClient: Slow ReadProcessor read fields took 62305ms (threshold=30000ms); ack: seqno: 78 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
17/03/31 23:16:34 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=79, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=390144
17/03/31 23:16:34 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=390647 lastFlushOffset=390647 createNewBlock=false
17/03/31 23:16:34 DEBUG DFSClient: Waiting for ack for: 78
17/03/31 23:16:43 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=80, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=390144
17/03/31 23:16:43 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=390647 lastFlushOffset=390647 createNewBlock=false
17/03/31 23:16:43 DEBUG DFSClient: Waiting for ack for: 78
17/03/31 23:16:43 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=81, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=390144
17/03/31 23:16:43 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=390647 lastFlushOffset=390647 createNewBlock=false
17/03/31 23:16:43 DEBUG DFSClient: Waiting for ack for: 78
17/03/31 23:16:46 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:16:46 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:16:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:16:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #27
17/03/31 23:16:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #27
17/03/31 23:16:46 DEBUG ProtobufRpcEngine: Call: renewLease took 8ms
17/03/31 23:16:46 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:16:46 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
17/03/31 23:16:49 WARN TaskSetManager: Lost task 6.1 in stage 89.0 (TID 324, 172.21.15.173, executor 0): FetchFailed(BlockManagerId(2, 172.21.15.173, 60484, None), shuffleId=2, mapId=3, reduceId=6, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /172.21.15.173:60484
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:88)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to /172.21.15.173:60484
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:181)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: 拒绝连接: /172.21.15.173:60484
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:640)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
	... 2 more

)
17/03/31 23:16:49 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=82, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=390144
17/03/31 23:16:49 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=396521 lastFlushOffset=390647 createNewBlock=false
17/03/31 23:16:49 DEBUG DFSClient: Queued packet 82
17/03/31 23:16:49 DEBUG DFSClient: Waiting for ack for: 82
17/03/31 23:16:49 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 82 offsetInBlock: 390144 lastPacketInBlock: false lastByteOffsetInBlock: 396521
17/03/31 23:16:49 DEBUG DFSClient: DFSClient seqno: 82 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 77:>                                                         (0 + 2) / 2][Stage 77:=============================>                            (1 + 1) / 2]17/03/31 23:16:54 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=83, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=396288
17/03/31 23:16:54 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=400969 lastFlushOffset=396521 createNewBlock=false
17/03/31 23:16:54 DEBUG DFSClient: Queued packet 83
17/03/31 23:16:54 DEBUG DFSClient: Waiting for ack for: 83
17/03/31 23:16:54 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 83 offsetInBlock: 396288 lastPacketInBlock: false lastByteOffsetInBlock: 400969
17/03/31 23:16:54 DEBUG DFSClient: DFSClient seqno: 83 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:16:54 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=84, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=400896
17/03/31 23:16:54 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=400969 lastFlushOffset=400969 createNewBlock=false
17/03/31 23:16:54 DEBUG DFSClient: Waiting for ack for: 83
17/03/31 23:16:55 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=85, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=400896
17/03/31 23:16:55 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=406143 lastFlushOffset=400969 createNewBlock=false
17/03/31 23:16:55 DEBUG DFSClient: Queued packet 85
17/03/31 23:16:55 DEBUG DFSClient: Waiting for ack for: 85
17/03/31 23:16:55 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 85 offsetInBlock: 400896 lastPacketInBlock: false lastByteOffsetInBlock: 406143
17/03/31 23:16:55 DEBUG DFSClient: DFSClient seqno: 85 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:16:55 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=86, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=406016
17/03/31 23:16:55 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=406143 lastFlushOffset=406143 createNewBlock=false
17/03/31 23:16:55 DEBUG DFSClient: Waiting for ack for: 85
17/03/31 23:16:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:16:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 81:>                                                         (0 + 2) / 2]17/03/31 23:16:56 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=87, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=406016
17/03/31 23:16:56 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=411125 lastFlushOffset=406143 createNewBlock=false
17/03/31 23:16:56 DEBUG DFSClient: Queued packet 87
17/03/31 23:16:56 DEBUG DFSClient: Waiting for ack for: 87
17/03/31 23:16:56 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 87 offsetInBlock: 406016 lastPacketInBlock: false lastByteOffsetInBlock: 411125
17/03/31 23:16:56 DEBUG DFSClient: DFSClient seqno: 87 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 82:>                                                         (0 + 3) / 3][Stage 82:===================>                                      (1 + 2) / 3]17/03/31 23:16:59 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=88, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=410624
17/03/31 23:16:59 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=416646 lastFlushOffset=411125 createNewBlock=false
17/03/31 23:16:59 DEBUG DFSClient: Queued packet 88
17/03/31 23:16:59 DEBUG DFSClient: Waiting for ack for: 88
17/03/31 23:16:59 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 88 offsetInBlock: 410624 lastPacketInBlock: false lastByteOffsetInBlock: 416646
17/03/31 23:16:59 DEBUG DFSClient: DFSClient seqno: 88 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 83:>                                                         (0 + 3) / 3][Stage 83:===================>                                      (1 + 2) / 3][Stage 83:======================================>                   (2 + 1) / 3]17/03/31 23:17:08 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=89, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=416256
17/03/31 23:17:08 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=422371 lastFlushOffset=416646 createNewBlock=false
17/03/31 23:17:08 DEBUG DFSClient: Queued packet 89
17/03/31 23:17:08 DEBUG DFSClient: Waiting for ack for: 89
17/03/31 23:17:08 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 89 offsetInBlock: 416256 lastPacketInBlock: false lastByteOffsetInBlock: 422371
17/03/31 23:17:08 DEBUG DFSClient: DFSClient seqno: 89 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 84:>                                                         (0 + 2) / 2][Stage 84:=============================>                            (1 + 1) / 2]17/03/31 23:17:15 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=90, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=421888
17/03/31 23:17:15 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=427461 lastFlushOffset=422371 createNewBlock=false
17/03/31 23:17:15 DEBUG DFSClient: Queued packet 90
17/03/31 23:17:15 DEBUG DFSClient: Waiting for ack for: 90
17/03/31 23:17:15 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 90 offsetInBlock: 421888 lastPacketInBlock: false lastByteOffsetInBlock: 427461
17/03/31 23:17:15 DEBUG DFSClient: DFSClient seqno: 90 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 85:>                                                         (0 + 2) / 2]17/03/31 23:17:16 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:17:16 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:17:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:17:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #28
17/03/31 23:17:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #28
17/03/31 23:17:16 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/31 23:17:16 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:17:16 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
17/03/31 23:17:19 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=91, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=427008
17/03/31 23:17:19 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=427461 lastFlushOffset=427461 createNewBlock=false
17/03/31 23:17:19 DEBUG DFSClient: Waiting for ack for: 90
[Stage 86:>                                                         (0 + 2) / 2][Stage 86:=============================>                            (1 + 1) / 2]17/03/31 23:17:24 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=92, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=427008
17/03/31 23:17:24 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=432178 lastFlushOffset=427461 createNewBlock=false
17/03/31 23:17:24 DEBUG DFSClient: Queued packet 92
17/03/31 23:17:24 DEBUG DFSClient: Waiting for ack for: 92
17/03/31 23:17:24 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 92 offsetInBlock: 427008 lastPacketInBlock: false lastByteOffsetInBlock: 432178
17/03/31 23:17:24 DEBUG DFSClient: DFSClient seqno: 92 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 87:>                                                         (0 + 2) / 2]17/03/31 23:17:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:17:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 87:=============================>                            (1 + 1) / 2]17/03/31 23:17:31 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=93, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=432128
17/03/31 23:17:31 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=437235 lastFlushOffset=432178 createNewBlock=false
17/03/31 23:17:31 DEBUG DFSClient: Queued packet 93
17/03/31 23:17:31 DEBUG DFSClient: Waiting for ack for: 93
17/03/31 23:17:31 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 93 offsetInBlock: 432128 lastPacketInBlock: false lastByteOffsetInBlock: 437235
17/03/31 23:17:31 DEBUG DFSClient: DFSClient seqno: 93 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 88:>                                                         (0 + 4) / 4][Stage 88:==============>                                           (1 + 3) / 4][Stage 88:=============================>                            (2 + 2) / 4]17/03/31 23:17:46 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:17:46 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:17:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:17:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #29
17/03/31 23:17:46 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #29
17/03/31 23:17:46 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/31 23:17:46 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:17:46 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
[Stage 88:===========================================>              (3 + 1) / 4]17/03/31 23:17:49 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=94, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=436736
17/03/31 23:17:49 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=447227 lastFlushOffset=437235 createNewBlock=false
17/03/31 23:17:49 DEBUG DFSClient: Queued packet 94
17/03/31 23:17:49 DEBUG DFSClient: Waiting for ack for: 94
17/03/31 23:17:49 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 94 offsetInBlock: 436736 lastPacketInBlock: false lastByteOffsetInBlock: 447227
17/03/31 23:17:49 DEBUG DFSClient: DFSClient seqno: 94 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:17:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:17:56 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
                                                                                17/03/31 23:18:10 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=95, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=446976
17/03/31 23:18:10 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=450921 lastFlushOffset=447227 createNewBlock=false
17/03/31 23:18:10 DEBUG DFSClient: Queued packet 95
17/03/31 23:18:10 DEBUG DFSClient: Waiting for ack for: 95
17/03/31 23:18:10 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 95 offsetInBlock: 446976 lastPacketInBlock: false lastByteOffsetInBlock: 450921
17/03/31 23:18:10 DEBUG DFSClient: DFSClient seqno: 95 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:18:10 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=96, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=450560
17/03/31 23:18:10 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=450921 lastFlushOffset=450921 createNewBlock=false
17/03/31 23:18:10 DEBUG DFSClient: Waiting for ack for: 95
17/03/31 23:18:10 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=97, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=450560
17/03/31 23:18:10 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=458422 lastFlushOffset=450921 createNewBlock=false
17/03/31 23:18:10 DEBUG DFSClient: Queued packet 97
17/03/31 23:18:10 DEBUG DFSClient: Waiting for ack for: 97
17/03/31 23:18:10 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 97 offsetInBlock: 450560 lastPacketInBlock: false lastByteOffsetInBlock: 458422
17/03/31 23:18:12 DEBUG DFSClient: DFSClient seqno: 97 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 102:=====>                                                  (1 + 5) / 10][Stage 102:===========>                                            (2 + 4) / 10][Stage 102:===========>                                            (2 + 5) / 10][Stage 102:===========>                                            (2 + 6) / 10][Stage 102:================>                                       (3 + 5) / 10]17/03/31 23:18:16 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:18:16 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:18:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:18:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #30
17/03/31 23:18:16 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #30
17/03/31 23:18:16 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/31 23:18:16 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1678096954_1
17/03/31 23:18:16 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1678096954_1] with renew id 1 executed
[Stage 102:======================>                                 (4 + 4) / 10][Stage 102:======================>                                 (4 + 6) / 10][Stage 102:============================>                           (5 + 5) / 10]17/03/31 23:18:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:18:26 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 102:=================================>                      (6 + 4) / 10][Stage 102:=======================================>                (7 + 3) / 10][Stage 102:============================================>           (8 + 2) / 10][Stage 102:==================================================>     (9 + 1) / 10]                                                                                17/03/31 23:18:38 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=98, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=458240
17/03/31 23:18:38 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=467140 lastFlushOffset=458422 createNewBlock=false
17/03/31 23:18:38 DEBUG DFSClient: Queued packet 98
17/03/31 23:18:38 DEBUG DFSClient: Waiting for ack for: 98
17/03/31 23:18:38 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 98 offsetInBlock: 458240 lastPacketInBlock: false lastByteOffsetInBlock: 467140
17/03/31 23:18:38 DEBUG DFSClient: DFSClient seqno: 98 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:18:38 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=99, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=466944
17/03/31 23:18:38 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=467140 lastFlushOffset=467140 createNewBlock=false
17/03/31 23:18:38 DEBUG DFSClient: Waiting for ack for: 98
the err is 0.50
17/03/31 23:18:38 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=100, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=466944
17/03/31 23:18:38 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=467140 lastFlushOffset=467140 createNewBlock=false
17/03/31 23:18:38 DEBUG DFSClient: Waiting for ack for: 98
17/03/31 23:18:38 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=101, src=/eventLogs/app-20170331230645-0012.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=466944
17/03/31 23:18:38 DEBUG DFSClient: Queued packet 101
17/03/31 23:18:38 DEBUG DFSClient: Queued packet 102
17/03/31 23:18:38 DEBUG DFSClient: Waiting for ack for: 102
17/03/31 23:18:38 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 101 offsetInBlock: 466944 lastPacketInBlock: false lastByteOffsetInBlock: 470851
17/03/31 23:18:38 DEBUG DFSClient: DFSClient seqno: 101 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:18:38 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135 sending packet packet seqno: 102 offsetInBlock: 470851 lastPacketInBlock: true lastByteOffsetInBlock: 470851
17/03/31 23:18:38 DEBUG DFSClient: DFSClient seqno: 102 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 23:18:38 DEBUG DFSClient: Closing old block BP-519507147-172.21.15.90-1479901973323:blk_1073811957_71135
17/03/31 23:18:38 DEBUG Client: The ping interval is 60000 ms.
17/03/31 23:18:38 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 23:18:38 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 23:18:38 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #31
17/03/31 23:18:38 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #31
17/03/31 23:18:38 DEBUG ProtobufRpcEngine: Call: complete took 5ms
17/03/31 23:18:39 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #32
17/03/31 23:18:39 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #32
17/03/31 23:18:39 DEBUG ProtobufRpcEngine: Call: complete took 6ms
17/03/31 23:18:39 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #33
17/03/31 23:18:39 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #33
17/03/31 23:18:39 DEBUG ProtobufRpcEngine: Call: getFileInfo took 2ms
17/03/31 23:18:39 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #34
17/03/31 23:18:39 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #34
17/03/31 23:18:39 DEBUG ProtobufRpcEngine: Call: rename took 9ms
17/03/31 23:18:39 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop sending #35
17/03/31 23:18:39 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop got value #35
17/03/31 23:18:39 DEBUG ProtobufRpcEngine: Call: setTimes took 9ms
17/03/31 23:18:39 DEBUG PoolThreadCache: Freed 10 thread-local buffer(s) from thread: shuffle-server-6-4
17/03/31 23:18:39 DEBUG PoolThreadCache: Freed 32 thread-local buffer(s) from thread: shuffle-server-6-2
17/03/31 23:18:39 DEBUG PoolThreadCache: Freed 5 thread-local buffer(s) from thread: rpc-server-3-2
17/03/31 23:18:39 DEBUG PoolThreadCache: Freed 8 thread-local buffer(s) from thread: rpc-server-3-3
17/03/31 23:18:40 DEBUG Client: stopping client from cache: org.apache.hadoop.ipc.Client@b7d5486
17/03/31 23:18:40 DEBUG Client: removing client from cache: org.apache.hadoop.ipc.Client@b7d5486
17/03/31 23:18:40 DEBUG Client: stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@b7d5486
17/03/31 23:18:40 DEBUG Client: Stopping client
17/03/31 23:18:40 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 23:18:40 DEBUG Client: IPC Client (1714086345) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
