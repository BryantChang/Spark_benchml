17/03/31 10:07:56 WARN SparkContext: Support for Java 7 is deprecated as of Spark 2.0.0
17/03/31 10:07:56 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time, always=false, type=DEFAULT, sampleName=Ops)
17/03/31 10:07:56 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time, always=false, type=DEFAULT, sampleName=Ops)
17/03/31 10:07:56 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, value=[GetGroups], valueName=Time, always=false, type=DEFAULT, sampleName=Ops)
17/03/31 10:07:56 DEBUG MetricsSystemImpl: UgiMetrics, User and group related metrics
17/03/31 10:07:56 DEBUG Shell: setsid exited with exit code 0
17/03/31 10:07:56 DEBUG Groups:  Creating new Groups object
17/03/31 10:07:56 DEBUG NativeCodeLoader: Trying to load the custom-built native-hadoop library...
17/03/31 10:07:56 DEBUG NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
17/03/31 10:07:56 DEBUG NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
17/03/31 10:07:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/03/31 10:07:56 DEBUG PerformanceAdvisory: Falling back to shell based
17/03/31 10:07:56 DEBUG JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
17/03/31 10:07:57 DEBUG Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
17/03/31 10:07:57 DEBUG UserGroupInformation: hadoop login
17/03/31 10:07:57 DEBUG UserGroupInformation: hadoop login commit
17/03/31 10:07:57 DEBUG UserGroupInformation: using local user:UnixPrincipal: hadoop
17/03/31 10:07:57 DEBUG UserGroupInformation: Using user: "UnixPrincipal: hadoop" with name hadoop
17/03/31 10:07:57 DEBUG UserGroupInformation: User entry: "hadoop"
17/03/31 10:07:57 DEBUG UserGroupInformation: UGI loginUser:hadoop (auth:SIMPLE)
17/03/31 10:07:57 WARN SparkConf: Detected deprecated memory fraction settings: [spark.shuffle.memoryFraction, spark.storage.memoryFraction, spark.storage.unrollFraction]. As of Spark 1.6, execution and storage memory management are unified. All memory fractions used in the old model are now deprecated and no longer read. If you wish to use the old memory management, you may explicitly enable `spark.memory.useLegacyMode` (not recommended).
17/03/31 10:07:57 WARN SparkConf: 
SPARK_CLASSPATH was detected (set to '/home/hadoop/bryantchang/platforms/spark2.0/lib/mysql-connector-java-5.1.40-bin.jar:').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with --driver-class-path to augment the driver classpath
 - spark.executor.extraClassPath to augment the executor classpath
        
17/03/31 10:07:57 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/home/hadoop/bryantchang/platforms/spark2.0/lib/mysql-connector-java-5.1.40-bin.jar:' as a work-around.
17/03/31 10:07:57 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/home/hadoop/bryantchang/platforms/spark2.0/lib/mysql-connector-java-5.1.40-bin.jar:' as a work-around.
17/03/31 10:07:57 DEBUG InternalLoggerFactory: Using SLF4J as the default logging framework
17/03/31 10:07:57 DEBUG PlatformDependent0: java.nio.Buffer.address: available
17/03/31 10:07:57 DEBUG PlatformDependent0: sun.misc.Unsafe.theUnsafe: available
17/03/31 10:07:57 DEBUG PlatformDependent0: sun.misc.Unsafe.copyMemory: available
17/03/31 10:07:57 DEBUG PlatformDependent0: direct buffer constructor: available
17/03/31 10:07:57 DEBUG PlatformDependent0: java.nio.Bits.unaligned: available, true
17/03/31 10:07:57 DEBUG PlatformDependent0: java.nio.DirectByteBuffer.<init>(long, int): available
17/03/31 10:07:57 DEBUG Cleaner0: java.nio.ByteBuffer.cleaner(): available
17/03/31 10:07:57 DEBUG PlatformDependent: Java version: 7
17/03/31 10:07:57 DEBUG PlatformDependent: -Dio.netty.noUnsafe: false
17/03/31 10:07:57 DEBUG PlatformDependent: sun.misc.Unsafe: available
17/03/31 10:07:57 DEBUG PlatformDependent: -Dio.netty.noJavassist: false
17/03/31 10:07:57 DEBUG PlatformDependent: Javassist: available
17/03/31 10:07:57 DEBUG PlatformDependent: -Dio.netty.tmpdir: /tmp (java.io.tmpdir)
17/03/31 10:07:57 DEBUG PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)
17/03/31 10:07:57 DEBUG PlatformDependent: -Dio.netty.noPreferDirect: false
17/03/31 10:07:57 DEBUG PlatformDependent: io.netty.maxDirectMemory: 1342177280 bytes
17/03/31 10:07:57 DEBUG JavassistTypeParameterMatcherGenerator: Generated: io.netty.util.internal.__matchers__.org.apache.spark.network.protocol.MessageMatcher
17/03/31 10:07:57 DEBUG JavassistTypeParameterMatcherGenerator: Generated: io.netty.util.internal.__matchers__.io.netty.buffer.ByteBufMatcher
17/03/31 10:07:57 DEBUG MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 8
17/03/31 10:07:57 DEBUG NioEventLoop: -Dio.netty.noKeySetOptimization: false
17/03/31 10:07:57 DEBUG NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512
17/03/31 10:07:57 DEBUG PlatformDependent: org.jctools-core.MpscChunkedArrayQueue: available
17/03/31 10:07:57 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 8
17/03/31 10:07:57 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 8
17/03/31 10:07:57 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192
17/03/31 10:07:57 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 11
17/03/31 10:07:57 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 16777216
17/03/31 10:07:57 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.tinyCacheSize: 512
17/03/31 10:07:57 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256
17/03/31 10:07:57 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64
17/03/31 10:07:57 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768
17/03/31 10:07:57 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192
17/03/31 10:07:57 DEBUG ThreadLocalRandom: -Dio.netty.initialSeedUniquifier: 0x0906a90257c5e6f7 (took 0 ms)
17/03/31 10:07:57 DEBUG ByteBufUtil: -Dio.netty.allocator.type: unpooled
17/03/31 10:07:57 DEBUG ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 65536
17/03/31 10:07:57 DEBUG ByteBufUtil: -Dio.netty.maxThreadLocalCharBufferSize: 16384
17/03/31 10:07:57 DEBUG NetUtil: Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%1)
17/03/31 10:07:57 DEBUG NetUtil: /proc/sys/net/core/somaxconn: 128
17/03/31 10:07:57 DEBUG AbstractByteBuf: -Dio.netty.buffer.bytebuf.checkAccessible: true
17/03/31 10:07:57 DEBUG ResourceLeakDetector: -Dio.netty.leakDetection.level: simple
17/03/31 10:07:57 DEBUG ResourceLeakDetector: -Dio.netty.leakDetection.maxRecords: 4
17/03/31 10:07:57 DEBUG ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: io.netty.util.ResourceLeakDetector@1f9f200d
17/03/31 10:07:57 DEBUG Recycler: -Dio.netty.recycler.maxCapacity.default: 32768
17/03/31 10:07:57 DEBUG Recycler: -Dio.netty.recycler.maxSharedCapacityFactor: 2
17/03/31 10:07:57 DEBUG Recycler: -Dio.netty.recycler.linkCapacity: 16
17/03/31 10:07:57 DEBUG Recycler: -Dio.netty.recycler.ratio: 8
17/03/31 10:07:57 DEBUG BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false
17/03/31 10:07:57 DEBUG BlockReaderLocal: dfs.client.read.shortcircuit = false
17/03/31 10:07:57 DEBUG BlockReaderLocal: dfs.client.domain.socket.data.traffic = false
17/03/31 10:07:57 DEBUG BlockReaderLocal: dfs.domain.socket.path = 
17/03/31 10:07:57 DEBUG RetryUtils: multipleLinearRandomRetry = null
17/03/31 10:07:57 DEBUG Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcRequestWrapper, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@7f8e6e65
17/03/31 10:07:57 DEBUG Client: getting client out of cache: org.apache.hadoop.ipc.Client@1a6899fc
17/03/31 10:07:58 DEBUG PerformanceAdvisory: Both short-circuit local reads and UNIX domain socket are disabled.
17/03/31 10:07:58 DEBUG DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
17/03/31 10:07:58 DEBUG Client: The ping interval is 60000 ms.
17/03/31 10:07:58 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 10:07:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 10:07:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #0
17/03/31 10:07:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #0
17/03/31 10:07:58 DEBUG ProtobufRpcEngine: Call: getFileInfo took 26ms
17/03/31 10:07:58 DEBUG DFSClient: /eventLogs/app-20170331100757-0002.lz4.inprogress: masked=rw-r--r--
17/03/31 10:07:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #1
17/03/31 10:07:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #1
17/03/31 10:07:58 DEBUG ProtobufRpcEngine: Call: create took 10ms
17/03/31 10:07:58 DEBUG DFSClient: computePacketChunkSize: src=/eventLogs/app-20170331100757-0002.lz4.inprogress, chunkSize=516, chunksPerPacket=126, packetSize=65016
17/03/31 10:07:58 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1146653015_1] with renew id 1 started
17/03/31 10:07:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #2
17/03/31 10:07:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #2
17/03/31 10:07:58 DEBUG ProtobufRpcEngine: Call: setPermission took 8ms
17/03/31 10:07:58 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=0 lastFlushOffset=0 createNewBlock=false
17/03/31 10:07:58 DEBUG DFSClient: Waiting for ack for: -1
17/03/31 10:07:58 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=0 lastFlushOffset=0 createNewBlock=false
17/03/31 10:07:58 DEBUG DFSClient: Waiting for ack for: -1
17/03/31 10:07:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #3
17/03/31 10:07:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #3
17/03/31 10:07:58 DEBUG ProtobufRpcEngine: Call: getFileInfo took 1ms
17/03/31 10:07:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #4
17/03/31 10:07:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #4
17/03/31 10:07:58 DEBUG ProtobufRpcEngine: Call: getListing took 1ms
17/03/31 10:07:58 DEBUG FileInputFormat: Time taken to get FileStatuses: 15
17/03/31 10:07:58 INFO FileInputFormat: Total input paths to process : 10
17/03/31 10:07:58 DEBUG FileInputFormat: Total # of splits generated by getSplits: 13, TimeTaken: 19
17/03/31 10:07:58 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=0 lastFlushOffset=0 createNewBlock=false
17/03/31 10:07:58 DEBUG DFSClient: Waiting for ack for: -1
[Stage 0:>                                                         (0 + 0) / 10]17/03/31 10:08:02 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=0, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0
17/03/31 10:08:02 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=6336 lastFlushOffset=0 createNewBlock=false
17/03/31 10:08:02 DEBUG DFSClient: Queued packet 0
17/03/31 10:08:02 DEBUG DFSClient: Waiting for ack for: 0
17/03/31 10:08:02 DEBUG DFSClient: Allocating new block
17/03/31 10:08:02 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #5
17/03/31 10:08:02 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #5
17/03/31 10:08:02 DEBUG ProtobufRpcEngine: Call: addBlock took 24ms
17/03/31 10:08:02 DEBUG DFSClient: pipeline = DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]
17/03/31 10:08:02 DEBUG DFSClient: Connecting to datanode 172.21.15.173:50010
17/03/31 10:08:02 DEBUG DFSClient: Send buf size 124928
17/03/31 10:08:02 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #6
17/03/31 10:08:02 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #6
17/03/31 10:08:02 DEBUG ProtobufRpcEngine: Call: getServerDefaults took 1ms
17/03/31 10:08:02 DEBUG SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = /172.21.15.173, datanodeId = DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]
17/03/31 10:08:02 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 6336
17/03/31 10:08:02 DEBUG DFSClient: DFSClient seqno: 0 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:08:02 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #7
17/03/31 10:08:03 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #7
17/03/31 10:08:03 DEBUG ProtobufRpcEngine: Call: fsync took 25ms
17/03/31 10:08:03 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=1, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=6144
17/03/31 10:08:03 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=6336 lastFlushOffset=6336 createNewBlock=false
17/03/31 10:08:03 DEBUG DFSClient: Waiting for ack for: 0
17/03/31 10:08:03 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=2, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=6144
17/03/31 10:08:03 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=6336 lastFlushOffset=6336 createNewBlock=false
17/03/31 10:08:03 DEBUG DFSClient: Waiting for ack for: 0
17/03/31 10:08:03 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=3, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=6144
17/03/31 10:08:03 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=6336 lastFlushOffset=6336 createNewBlock=false
17/03/31 10:08:03 DEBUG DFSClient: Waiting for ack for: 0
[Stage 0:>                                                         (0 + 4) / 10][Stage 0:=====>                                                    (1 + 4) / 10][Stage 0:===========>                                              (2 + 4) / 10][Stage 0:=================>                                        (3 + 4) / 10][Stage 0:=======================>                                  (4 + 4) / 10][Stage 0:=============================>                            (5 + 4) / 10][Stage 0:==================================>                       (6 + 4) / 10][Stage 0:========================================>                 (7 + 3) / 10][Stage 0:==============================================>           (8 + 2) / 10][Stage 0:=========================================================(10 + 0) / 10]                                                                                17/03/31 10:08:10 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=4, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=6144
17/03/31 10:08:10 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=10358 lastFlushOffset=6336 createNewBlock=false
17/03/31 10:08:10 DEBUG DFSClient: Queued packet 4
17/03/31 10:08:10 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 4 offsetInBlock: 6144 lastPacketInBlock: false lastByteOffsetInBlock: 10358
17/03/31 10:08:10 DEBUG DFSClient: Waiting for ack for: 4
17/03/31 10:08:10 DEBUG DFSClient: DFSClient seqno: 4 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:08:10 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=5, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=10240
17/03/31 10:08:10 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=10358 lastFlushOffset=10358 createNewBlock=false
17/03/31 10:08:10 DEBUG DFSClient: Waiting for ack for: 4
17/03/31 10:08:10 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=6, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=10240
17/03/31 10:08:10 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=10358 lastFlushOffset=10358 createNewBlock=false
17/03/31 10:08:10 DEBUG DFSClient: Waiting for ack for: 4
[Stage 1:>                                                         (0 + 4) / 10][Stage 1:===========>                                              (2 + 4) / 10][Stage 1:=======================>                                  (4 + 4) / 10][Stage 1:=============================>                            (5 + 4) / 10][Stage 1:==============================================>           (8 + 2) / 10]17/03/31 10:08:12 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=7, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=10240
17/03/31 10:08:12 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=19886 lastFlushOffset=10358 createNewBlock=false
17/03/31 10:08:12 DEBUG DFSClient: Queued packet 7
17/03/31 10:08:12 DEBUG DFSClient: Waiting for ack for: 7
17/03/31 10:08:12 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 7 offsetInBlock: 10240 lastPacketInBlock: false lastByteOffsetInBlock: 19886
17/03/31 10:08:12 DEBUG DFSClient: DFSClient seqno: 7 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 2:>                                                         (0 + 4) / 10]17/03/31 10:08:12 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 10:08:12 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 2:===========>                                              (2 + 4) / 10][Stage 2:=======================>                                  (4 + 4) / 10][Stage 2:==================================>                       (6 + 4) / 10][Stage 2:==============================================>           (8 + 2) / 10]                                                                                17/03/31 10:08:16 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=8, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=19456
17/03/31 10:08:16 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=30637 lastFlushOffset=19886 createNewBlock=false
17/03/31 10:08:16 DEBUG DFSClient: Queued packet 8
17/03/31 10:08:16 DEBUG DFSClient: Waiting for ack for: 8
17/03/31 10:08:16 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 8 offsetInBlock: 19456 lastPacketInBlock: false lastByteOffsetInBlock: 30637
17/03/31 10:08:16 DEBUG DFSClient: DFSClient seqno: 8 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:08:16 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=9, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=30208
17/03/31 10:08:16 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=30637 lastFlushOffset=30637 createNewBlock=false
17/03/31 10:08:16 DEBUG DFSClient: Waiting for ack for: 8
17/03/31 10:08:16 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=10, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=30208
17/03/31 10:08:16 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=30637 lastFlushOffset=30637 createNewBlock=false
17/03/31 10:08:16 DEBUG DFSClient: Waiting for ack for: 8
[Stage 4:>                                                         (0 + 4) / 10][Stage 4:===========>                                              (2 + 4) / 10][Stage 4:=======================>                                  (4 + 4) / 10][Stage 4:==================================>                       (6 + 2) / 10][Stage 4:==============================================>           (8 + 2) / 10]17/03/31 10:08:24 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=11, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=30208
17/03/31 10:08:24 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=41859 lastFlushOffset=30637 createNewBlock=false
17/03/31 10:08:24 DEBUG DFSClient: Queued packet 11
17/03/31 10:08:24 DEBUG DFSClient: Waiting for ack for: 11
17/03/31 10:08:24 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 11 offsetInBlock: 30208 lastPacketInBlock: false lastByteOffsetInBlock: 41859
17/03/31 10:08:24 DEBUG DFSClient: DFSClient seqno: 11 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
                                                                                17/03/31 10:08:24 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=12, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=41472
17/03/31 10:08:24 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=50047 lastFlushOffset=41859 createNewBlock=false
17/03/31 10:08:24 DEBUG DFSClient: Queued packet 12
17/03/31 10:08:24 DEBUG DFSClient: Waiting for ack for: 12
17/03/31 10:08:24 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 12 offsetInBlock: 41472 lastPacketInBlock: false lastByteOffsetInBlock: 50047
17/03/31 10:08:24 DEBUG DFSClient: DFSClient seqno: 12 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:08:24 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=13, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=49664
17/03/31 10:08:24 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=50047 lastFlushOffset=50047 createNewBlock=false
17/03/31 10:08:24 DEBUG DFSClient: Waiting for ack for: 12
17/03/31 10:08:24 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=14, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=49664
17/03/31 10:08:24 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=50047 lastFlushOffset=50047 createNewBlock=false
17/03/31 10:08:24 DEBUG DFSClient: Waiting for ack for: 12
[Stage 7:>                                                         (0 + 4) / 10][Stage 7:=====>                                                    (1 + 4) / 10][Stage 7:=======================>                                  (4 + 4) / 10][Stage 7:=============================>                            (5 + 4) / 10][Stage 7:==================================>                       (6 + 4) / 10]17/03/31 10:08:28 DEBUG Client: The ping interval is 60000 ms.
17/03/31 10:08:28 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 10:08:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 10:08:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #8
17/03/31 10:08:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #8
17/03/31 10:08:28 DEBUG ProtobufRpcEngine: Call: renewLease took 3ms
17/03/31 10:08:28 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1146653015_1
17/03/31 10:08:28 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1146653015_1] with renew id 1 executed
[Stage 7:========================================>                 (7 + 3) / 10][Stage 7:==============================================>           (8 + 2) / 10][Stage 7:====================================================>     (9 + 1) / 10]                                                                                17/03/31 10:08:28 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=15, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=49664
17/03/31 10:08:28 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=63394 lastFlushOffset=50047 createNewBlock=false
17/03/31 10:08:28 DEBUG DFSClient: Queued packet 15
17/03/31 10:08:28 DEBUG DFSClient: Waiting for ack for: 15
17/03/31 10:08:28 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 15 offsetInBlock: 49664 lastPacketInBlock: false lastByteOffsetInBlock: 63394
17/03/31 10:08:28 DEBUG DFSClient: DFSClient seqno: 15 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:08:28 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=16, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=62976
17/03/31 10:08:28 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=63394 lastFlushOffset=63394 createNewBlock=false
17/03/31 10:08:28 DEBUG DFSClient: Waiting for ack for: 15
17/03/31 10:08:28 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=17, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=62976
17/03/31 10:08:28 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=63394 lastFlushOffset=63394 createNewBlock=false
17/03/31 10:08:28 DEBUG DFSClient: Waiting for ack for: 15
17/03/31 10:08:28 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=18, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=62976
17/03/31 10:08:28 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=63394 lastFlushOffset=63394 createNewBlock=false
17/03/31 10:08:28 DEBUG DFSClient: Waiting for ack for: 15
17/03/31 10:08:29 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=19, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=62976
17/03/31 10:08:29 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=71660 lastFlushOffset=63394 createNewBlock=false
17/03/31 10:08:29 DEBUG DFSClient: Queued packet 19
17/03/31 10:08:29 DEBUG DFSClient: Waiting for ack for: 19
17/03/31 10:08:29 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 19 offsetInBlock: 62976 lastPacketInBlock: false lastByteOffsetInBlock: 71660
17/03/31 10:08:29 DEBUG DFSClient: DFSClient seqno: 19 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 11:>                                                        (0 + 4) / 10][Stage 11:===========>                                             (2 + 4) / 10][Stage 11:======================>                                  (4 + 4) / 10][Stage 11:============================>                            (5 + 3) / 10][Stage 11:==================================>                      (6 + 2) / 10][Stage 11:=============================================>           (8 + 2) / 10][Stage 11:===================================================>     (9 + 1) / 10]17/03/31 10:08:34 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=20, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=71168
17/03/31 10:08:34 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=85476 lastFlushOffset=71660 createNewBlock=false
17/03/31 10:08:34 DEBUG DFSClient: Queued packet 20
17/03/31 10:08:34 DEBUG DFSClient: Waiting for ack for: 20
17/03/31 10:08:34 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 20 offsetInBlock: 71168 lastPacketInBlock: false lastByteOffsetInBlock: 85476
17/03/31 10:08:34 DEBUG DFSClient: DFSClient seqno: 20 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
                                                                                17/03/31 10:08:34 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=21, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=84992
17/03/31 10:08:34 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=94153 lastFlushOffset=85476 createNewBlock=false
17/03/31 10:08:34 DEBUG DFSClient: Queued packet 21
17/03/31 10:08:34 DEBUG DFSClient: Waiting for ack for: 21
17/03/31 10:08:34 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 21 offsetInBlock: 84992 lastPacketInBlock: false lastByteOffsetInBlock: 94153
17/03/31 10:08:34 DEBUG DFSClient: DFSClient seqno: 21 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:08:34 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=22, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=93696
17/03/31 10:08:34 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=94153 lastFlushOffset=94153 createNewBlock=false
17/03/31 10:08:34 DEBUG DFSClient: Waiting for ack for: 21
17/03/31 10:08:34 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=23, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=93696
17/03/31 10:08:34 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=94153 lastFlushOffset=94153 createNewBlock=false
17/03/31 10:08:34 DEBUG DFSClient: Waiting for ack for: 21
17/03/31 10:08:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=24, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=93696
17/03/31 10:08:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=103561 lastFlushOffset=94153 createNewBlock=false
17/03/31 10:08:35 DEBUG DFSClient: Queued packet 24
17/03/31 10:08:35 DEBUG DFSClient: Waiting for ack for: 24
17/03/31 10:08:35 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 24 offsetInBlock: 93696 lastPacketInBlock: false lastByteOffsetInBlock: 103561
17/03/31 10:08:35 DEBUG DFSClient: DFSClient seqno: 24 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 18:======================>                                  (4 + 4) / 10][Stage 18:============================>                            (5 + 4) / 10][Stage 18:==================================>                      (6 + 4) / 10][Stage 18:=============================================>           (8 + 2) / 10]17/03/31 10:08:38 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 10:08:38 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
                                                                                17/03/31 10:08:38 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=25, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=103424
17/03/31 10:08:38 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=117120 lastFlushOffset=103561 createNewBlock=false
17/03/31 10:08:38 DEBUG DFSClient: Queued packet 25
17/03/31 10:08:38 DEBUG DFSClient: Waiting for ack for: 25
17/03/31 10:08:38 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 25 offsetInBlock: 103424 lastPacketInBlock: false lastByteOffsetInBlock: 117120
17/03/31 10:08:38 DEBUG DFSClient: DFSClient seqno: 25 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:08:38 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=26, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=116736
17/03/31 10:08:38 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=117120 lastFlushOffset=117120 createNewBlock=false
17/03/31 10:08:38 DEBUG DFSClient: Waiting for ack for: 25
17/03/31 10:08:38 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=27, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=116736
17/03/31 10:08:38 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=117120 lastFlushOffset=117120 createNewBlock=false
17/03/31 10:08:38 DEBUG DFSClient: Waiting for ack for: 25
17/03/31 10:08:38 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=28, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=116736
17/03/31 10:08:38 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=117120 lastFlushOffset=117120 createNewBlock=false
17/03/31 10:08:38 DEBUG DFSClient: Waiting for ack for: 25
17/03/31 10:08:38 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=29, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=116736
17/03/31 10:08:38 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=121788 lastFlushOffset=117120 createNewBlock=false
17/03/31 10:08:38 DEBUG DFSClient: Queued packet 29
17/03/31 10:08:38 DEBUG DFSClient: Waiting for ack for: 29
17/03/31 10:08:38 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 29 offsetInBlock: 116736 lastPacketInBlock: false lastByteOffsetInBlock: 121788
17/03/31 10:08:38 DEBUG DFSClient: DFSClient seqno: 29 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 24:>                                                        (0 + 4) / 10][Stage 24:===========>                                             (2 + 4) / 10][Stage 24:======================>                                  (4 + 4) / 10][Stage 24:============================>                            (5 + 3) / 10][Stage 24:==================================>                      (6 + 2) / 10][Stage 24:=============================================>           (8 + 2) / 10]17/03/31 10:08:45 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=30, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=121344
17/03/31 10:08:45 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=133847 lastFlushOffset=121788 createNewBlock=false
17/03/31 10:08:45 DEBUG DFSClient: Queued packet 30
17/03/31 10:08:45 DEBUG DFSClient: Waiting for ack for: 30
17/03/31 10:08:45 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 30 offsetInBlock: 121344 lastPacketInBlock: false lastByteOffsetInBlock: 133847
17/03/31 10:08:45 DEBUG DFSClient: DFSClient seqno: 30 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 25:============================>                            (5 + 5) / 10]                                                                                17/03/31 10:08:46 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=31, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=133632
17/03/31 10:08:46 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=146107 lastFlushOffset=133847 createNewBlock=false
17/03/31 10:08:46 DEBUG DFSClient: Queued packet 31
17/03/31 10:08:46 DEBUG DFSClient: Waiting for ack for: 31
17/03/31 10:08:46 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 31 offsetInBlock: 133632 lastPacketInBlock: false lastByteOffsetInBlock: 146107
17/03/31 10:08:46 DEBUG DFSClient: DFSClient seqno: 31 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:08:46 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=32, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=145920
17/03/31 10:08:46 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=146107 lastFlushOffset=146107 createNewBlock=false
17/03/31 10:08:46 DEBUG DFSClient: Waiting for ack for: 31
17/03/31 10:08:46 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=33, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=145920
17/03/31 10:08:46 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=150086 lastFlushOffset=146107 createNewBlock=false
17/03/31 10:08:46 DEBUG DFSClient: Queued packet 33
17/03/31 10:08:46 DEBUG DFSClient: Waiting for ack for: 33
17/03/31 10:08:46 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 33 offsetInBlock: 145920 lastPacketInBlock: false lastByteOffsetInBlock: 150086
17/03/31 10:08:46 DEBUG DFSClient: DFSClient seqno: 33 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:08:47 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=34, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=150016
17/03/31 10:08:47 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=155185 lastFlushOffset=150086 createNewBlock=false
17/03/31 10:08:47 DEBUG DFSClient: Queued packet 34
17/03/31 10:08:47 DEBUG DFSClient: Waiting for ack for: 34
17/03/31 10:08:47 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 34 offsetInBlock: 150016 lastPacketInBlock: false lastByteOffsetInBlock: 155185
17/03/31 10:08:47 DEBUG DFSClient: DFSClient seqno: 34 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 33:>                                                        (0 + 4) / 10][Stage 33:===========>                                             (2 + 4) / 10][Stage 33:=================>                                       (3 + 4) / 10][Stage 33:======================>                                  (4 + 4) / 10][Stage 33:============================>                            (5 + 4) / 10][Stage 33:==================================>                      (6 + 4) / 10][Stage 33:=============================================>           (8 + 2) / 10]17/03/31 10:08:51 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=35, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=155136
[Stage 33:===================================================>     (9 + 1) / 10]                                                                                17/03/31 10:08:52 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=177563 lastFlushOffset=155185 createNewBlock=false
17/03/31 10:08:52 DEBUG DFSClient: Queued packet 35
17/03/31 10:08:52 DEBUG DFSClient: Waiting for ack for: 35
17/03/31 10:08:52 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 35 offsetInBlock: 155136 lastPacketInBlock: false lastByteOffsetInBlock: 177563
17/03/31 10:08:52 DEBUG DFSClient: DFSClient seqno: 35 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:08:52 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=36, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=177152
17/03/31 10:08:52 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=177563 lastFlushOffset=177563 createNewBlock=false
17/03/31 10:08:52 DEBUG DFSClient: Waiting for ack for: 35
17/03/31 10:08:52 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=37, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=177152
17/03/31 10:08:52 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=177563 lastFlushOffset=177563 createNewBlock=false
17/03/31 10:08:52 DEBUG DFSClient: Waiting for ack for: 35
17/03/31 10:08:52 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=38, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=177152
17/03/31 10:08:52 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=177563 lastFlushOffset=177563 createNewBlock=false
17/03/31 10:08:52 DEBUG DFSClient: Waiting for ack for: 35
17/03/31 10:08:52 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=39, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=177152
17/03/31 10:08:52 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=182261 lastFlushOffset=177563 createNewBlock=false
17/03/31 10:08:52 DEBUG DFSClient: Queued packet 39
17/03/31 10:08:52 DEBUG DFSClient: Waiting for ack for: 39
17/03/31 10:08:52 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 39 offsetInBlock: 177152 lastPacketInBlock: false lastByteOffsetInBlock: 182261
17/03/31 10:08:52 DEBUG DFSClient: DFSClient seqno: 39 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 41:>                                                        (0 + 4) / 10]17/03/31 10:08:58 DEBUG Client: The ping interval is 60000 ms.
17/03/31 10:08:58 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 10:08:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 10:08:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #9
17/03/31 10:08:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #9
17/03/31 10:08:58 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/31 10:08:58 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1146653015_1
17/03/31 10:08:58 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1146653015_1] with renew id 1 executed
17/03/31 10:09:08 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 10:09:08 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 41:=====>                                                   (1 + 4) / 10][Stage 41:===========>                                             (2 + 4) / 10][Stage 41:======================>                                  (4 + 4) / 10][Stage 41:============================>                            (5 + 4) / 10]17/03/31 10:09:28 DEBUG Client: The ping interval is 60000 ms.
17/03/31 10:09:28 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 10:09:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 10:09:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #10
17/03/31 10:09:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #10
17/03/31 10:09:28 DEBUG ProtobufRpcEngine: Call: renewLease took 5ms
17/03/31 10:09:28 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1146653015_1
17/03/31 10:09:28 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1146653015_1] with renew id 1 executed
[Stage 41:==================================>                      (6 + 4) / 10][Stage 41:=======================================>                 (7 + 3) / 10][Stage 41:=============================================>           (8 + 2) / 10][Stage 41:===================================================>     (9 + 1) / 10]17/03/31 10:09:38 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 10:09:38 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/31 10:09:38 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=40, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=181760
17/03/31 10:09:38 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=194420 lastFlushOffset=182261 createNewBlock=false
17/03/31 10:09:38 DEBUG DFSClient: Queued packet 40
17/03/31 10:09:38 DEBUG DFSClient: Waiting for ack for: 40
17/03/31 10:09:38 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 40 offsetInBlock: 181760 lastPacketInBlock: false lastByteOffsetInBlock: 194420
17/03/31 10:09:38 WARN DFSClient: Slow ReadProcessor read fields took 45650ms (threshold=30000ms); ack: seqno: 40 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
[Stage 42:>                                                        (0 + 4) / 10][Stage 42:======================>                                  (4 + 4) / 10][Stage 42:=======================================>                 (7 + 3) / 10][Stage 42:=============================================>           (8 + 2) / 10]                                                                                17/03/31 10:09:40 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=41, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=194048
17/03/31 10:09:40 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=207968 lastFlushOffset=194420 createNewBlock=false
17/03/31 10:09:40 DEBUG DFSClient: Queued packet 41
17/03/31 10:09:40 DEBUG DFSClient: Waiting for ack for: 41
17/03/31 10:09:40 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 41 offsetInBlock: 194048 lastPacketInBlock: false lastByteOffsetInBlock: 207968
17/03/31 10:09:40 DEBUG DFSClient: DFSClient seqno: 41 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:09:40 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=42, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=207872
17/03/31 10:09:40 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=207968 lastFlushOffset=207968 createNewBlock=false
17/03/31 10:09:40 DEBUG DFSClient: Waiting for ack for: 41
17/03/31 10:09:40 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=43, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=207872
17/03/31 10:09:40 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=211971 lastFlushOffset=207968 createNewBlock=false
17/03/31 10:09:40 DEBUG DFSClient: Queued packet 43
17/03/31 10:09:40 DEBUG DFSClient: Waiting for ack for: 43
17/03/31 10:09:40 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 43 offsetInBlock: 207872 lastPacketInBlock: false lastByteOffsetInBlock: 211971
17/03/31 10:09:40 DEBUG DFSClient: DFSClient seqno: 43 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 51:=======================================>                 (7 + 3) / 10]17/03/31 10:09:41 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=44, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=211968
17/03/31 10:09:41 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=216612 lastFlushOffset=211971 createNewBlock=false
17/03/31 10:09:41 DEBUG DFSClient: Queued packet 44
17/03/31 10:09:41 DEBUG DFSClient: Waiting for ack for: 44
17/03/31 10:09:41 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 44 offsetInBlock: 211968 lastPacketInBlock: false lastByteOffsetInBlock: 216612
17/03/31 10:09:41 DEBUG DFSClient: DFSClient seqno: 44 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 52:>                                                        (0 + 4) / 10][Stage 52:=====>                                                   (1 + 4) / 10][Stage 52:===========>                                             (2 + 4) / 10][Stage 52:=================>                                       (3 + 4) / 10][Stage 52:======================>                                  (4 + 4) / 10][Stage 52:============================>                            (5 + 3) / 10][Stage 52:==================================>                      (6 + 2) / 10][Stage 52:=======================================>                 (7 + 2) / 10][Stage 52:=============================================>           (8 + 2) / 10]                                                                                17/03/31 10:09:49 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=45, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=216576
17/03/31 10:09:49 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=232438 lastFlushOffset=216612 createNewBlock=false
17/03/31 10:09:49 DEBUG DFSClient: Queued packet 45
17/03/31 10:09:49 DEBUG DFSClient: Waiting for ack for: 45
17/03/31 10:09:49 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 45 offsetInBlock: 216576 lastPacketInBlock: false lastByteOffsetInBlock: 232438
17/03/31 10:09:49 DEBUG DFSClient: DFSClient seqno: 45 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:09:49 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=46, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=231936
17/03/31 10:09:49 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=232438 lastFlushOffset=232438 createNewBlock=false
17/03/31 10:09:49 DEBUG DFSClient: Waiting for ack for: 45
17/03/31 10:09:49 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=47, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=231936
17/03/31 10:09:49 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=232438 lastFlushOffset=232438 createNewBlock=false
17/03/31 10:09:49 DEBUG DFSClient: Waiting for ack for: 45
17/03/31 10:09:49 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=48, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=231936
17/03/31 10:09:49 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=232438 lastFlushOffset=232438 createNewBlock=false
17/03/31 10:09:49 DEBUG DFSClient: Waiting for ack for: 45
17/03/31 10:09:49 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=49, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=231936
17/03/31 10:09:49 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=240563 lastFlushOffset=232438 createNewBlock=false
17/03/31 10:09:49 DEBUG DFSClient: Queued packet 49
17/03/31 10:09:49 DEBUG DFSClient: Waiting for ack for: 49
17/03/31 10:09:49 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 49 offsetInBlock: 231936 lastPacketInBlock: false lastByteOffsetInBlock: 240563
17/03/31 10:09:49 DEBUG DFSClient: DFSClient seqno: 49 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 62:>                                                        (0 + 4) / 10][Stage 62:=====>                                                   (1 + 4) / 10][Stage 62:======================>                                  (4 + 4) / 10][Stage 62:============================>                            (5 + 3) / 10][Stage 62:==================================>                      (6 + 2) / 10][Stage 62:=======================================>                 (7 + 2) / 10][Stage 62:=============================================>           (8 + 2) / 10][Stage 62:===================================================>     (9 + 1) / 10]17/03/31 10:09:57 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=50, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=240128
17/03/31 10:09:57 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=250238 lastFlushOffset=240563 createNewBlock=false
17/03/31 10:09:57 DEBUG DFSClient: Queued packet 50
17/03/31 10:09:57 DEBUG DFSClient: Waiting for ack for: 50
17/03/31 10:09:57 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 50 offsetInBlock: 240128 lastPacketInBlock: false lastByteOffsetInBlock: 250238
17/03/31 10:09:57 DEBUG DFSClient: DFSClient seqno: 50 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:09:58 DEBUG Client: The ping interval is 60000 ms.
17/03/31 10:09:58 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 10:09:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 10:09:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #11
17/03/31 10:09:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #11
17/03/31 10:09:58 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/31 10:09:58 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1146653015_1
17/03/31 10:09:58 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1146653015_1] with renew id 1 executed
[Stage 63:>                                                        (0 + 4) / 10][Stage 63:=====>                                                   (1 + 4) / 10][Stage 63:===========>                                             (2 + 4) / 10][Stage 63:======================>                                  (4 + 4) / 10][Stage 63:==================================>                      (6 + 4) / 10][Stage 63:=======================================>                 (7 + 3) / 10][Stage 63:=============================================>           (8 + 2) / 10]                                                                                17/03/31 10:10:02 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=51, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=249856
17/03/31 10:10:02 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=262911 lastFlushOffset=250238 createNewBlock=false
17/03/31 10:10:02 DEBUG DFSClient: Queued packet 51
17/03/31 10:10:02 DEBUG DFSClient: Waiting for ack for: 51
17/03/31 10:10:02 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 51 offsetInBlock: 249856 lastPacketInBlock: false lastByteOffsetInBlock: 262911
17/03/31 10:10:02 DEBUG DFSClient: DFSClient seqno: 51 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:10:02 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=52, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=262656
17/03/31 10:10:02 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=262911 lastFlushOffset=262911 createNewBlock=false
17/03/31 10:10:02 DEBUG DFSClient: Waiting for ack for: 51
17/03/31 10:10:02 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=53, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=262656
17/03/31 10:10:02 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=270408 lastFlushOffset=262911 createNewBlock=false
17/03/31 10:10:02 DEBUG DFSClient: Queued packet 53
17/03/31 10:10:02 DEBUG DFSClient: Waiting for ack for: 53
17/03/31 10:10:02 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 53 offsetInBlock: 262656 lastPacketInBlock: false lastByteOffsetInBlock: 270408
17/03/31 10:10:02 DEBUG DFSClient: DFSClient seqno: 53 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 74:======================>                                  (4 + 4) / 10][Stage 74:============================>                            (5 + 3) / 10][Stage 74:=============================================>           (8 + 2) / 10][Stage 74:===================================================>     (9 + 1) / 10]17/03/31 10:10:04 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=54, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=270336
17/03/31 10:10:04 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=275545 lastFlushOffset=270408 createNewBlock=false
17/03/31 10:10:04 DEBUG DFSClient: Queued packet 54
17/03/31 10:10:04 DEBUG DFSClient: Waiting for ack for: 54
17/03/31 10:10:04 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 54 offsetInBlock: 270336 lastPacketInBlock: false lastByteOffsetInBlock: 275545
17/03/31 10:10:04 DEBUG DFSClient: DFSClient seqno: 54 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 75:>                                                        (0 + 4) / 10]17/03/31 10:10:08 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 10:10:08 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 75:=====>                                                   (1 + 4) / 10][Stage 75:===========>                                             (2 + 4) / 10][Stage 75:=================>                                       (3 + 4) / 10][Stage 75:======================>                                  (4 + 4) / 10][Stage 75:======================>                                  (4 + 6) / 10]17/03/31 10:10:19 WARN TaskSetManager: Lost task 5.0 in stage 75.0 (TID 225, 172.21.15.173, executor 0): java.lang.OutOfMemoryError: Java heap space
	at com.esotericsoftware.kryo.io.Input.readDoubles(Input.java:885)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$DoubleArraySerializer.read(DefaultArraySerializers.java:222)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$DoubleArraySerializer.read(DefaultArraySerializers.java:205)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)
	at com.twitter.chill.Tuple4Serializer.read(TupleSerializers.scala:70)
	at com.twitter.chill.Tuple4Serializer.read(TupleSerializers.scala:59)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:396)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:307)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:244)
	at org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:159)
	at org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:189)
	at org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:186)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)

[Stage 75:======================>                                  (4 + 4) / 10]17/03/31 10:10:21 WARN TaskSetManager: Lost task 8.0 in stage 75.0 (TID 228, 172.21.15.173, executor 0): java.io.FileNotFoundException: /tmp/spark-f3dc15b7-cbd2-4777-a10e-40b35a7f6f70/executor-98e72017-328e-45f2-8c3a-09208a35a004/blockmgr-837d177e-1791-4805-852d-78499f91f4de/3f/rdd_14_0 (没有那个文件或目录)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:171)
	at org.apache.spark.storage.DiskStore.put(DiskStore.scala:54)
	at org.apache.spark.storage.DiskStore.putBytes(DiskStore.scala:76)
	at org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:1268)
	at org.apache.spark.storage.memory.MemoryStore.org$apache$spark$storage$memory$MemoryStore$$dropBlock$1(MemoryStore.scala:526)
	at org.apache.spark.storage.memory.MemoryStore$$anonfun$evictBlocksToFreeSpace$2.apply(MemoryStore.scala:547)
	at org.apache.spark.storage.memory.MemoryStore$$anonfun$evictBlocksToFreeSpace$2.apply(MemoryStore.scala:541)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.storage.memory.MemoryStore.evictBlocksToFreeSpace(MemoryStore.scala:541)
	at org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:92)
	at org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:73)
	at org.apache.spark.memory.UnifiedMemoryManager.acquireStorageMemory(UnifiedMemoryManager.scala:178)
	at org.apache.spark.memory.UnifiedMemoryManager.acquireUnrollMemory(UnifiedMemoryManager.scala:185)
	at org.apache.spark.storage.memory.MemoryStore.reserveUnrollMemoryForThisTask(MemoryStore.scala:584)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:223)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:88)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/03/31 10:10:21 WARN TaskSetManager: Lost task 9.0 in stage 75.0 (TID 229, 172.21.15.173, executor 0): java.io.FileNotFoundException: /tmp/spark-f3dc15b7-cbd2-4777-a10e-40b35a7f6f70/executor-98e72017-328e-45f2-8c3a-09208a35a004/blockmgr-837d177e-1791-4805-852d-78499f91f4de/3b/rdd_14_4 (没有那个文件或目录)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:171)
	at org.apache.spark.storage.DiskStore.put(DiskStore.scala:54)
	at org.apache.spark.storage.DiskStore.putBytes(DiskStore.scala:76)
	at org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:1268)
	at org.apache.spark.storage.memory.MemoryStore.org$apache$spark$storage$memory$MemoryStore$$dropBlock$1(MemoryStore.scala:526)
	at org.apache.spark.storage.memory.MemoryStore$$anonfun$evictBlocksToFreeSpace$2.apply(MemoryStore.scala:547)
	at org.apache.spark.storage.memory.MemoryStore$$anonfun$evictBlocksToFreeSpace$2.apply(MemoryStore.scala:541)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.storage.memory.MemoryStore.evictBlocksToFreeSpace(MemoryStore.scala:541)
	at org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:92)
	at org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:73)
	at org.apache.spark.memory.UnifiedMemoryManager.acquireStorageMemory(UnifiedMemoryManager.scala:178)
	at org.apache.spark.memory.UnifiedMemoryManager.acquireUnrollMemory(UnifiedMemoryManager.scala:185)
	at org.apache.spark.storage.memory.MemoryStore.reserveUnrollMemoryForThisTask(MemoryStore.scala:584)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:223)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:88)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/03/31 10:10:21 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=55, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=275456
17/03/31 10:10:21 ERROR TaskSchedulerImpl: Lost executor 0 on 172.21.15.173: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/31 10:10:21 WARN TaskSetManager: Lost task 5.1 in stage 75.0 (TID 231, 172.21.15.173, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/31 10:10:21 WARN TaskSetManager: Lost task 4.1 in stage 75.0 (TID 230, 172.21.15.173, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/31 10:10:21 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=292851 lastFlushOffset=275545 createNewBlock=false
17/03/31 10:10:21 DEBUG DFSClient: Queued packet 55
17/03/31 10:10:21 DEBUG DFSClient: Waiting for ack for: 55
17/03/31 10:10:21 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 55 offsetInBlock: 275456 lastPacketInBlock: false lastByteOffsetInBlock: 292851
[Stage 75:======================>                                  (4 + 2) / 10]17/03/31 10:10:21 DEBUG DFSClient: DFSClient seqno: 55 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:10:21 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=56, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=292352
17/03/31 10:10:21 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=292851 lastFlushOffset=292851 createNewBlock=false
17/03/31 10:10:21 DEBUG DFSClient: Waiting for ack for: 55
17/03/31 10:10:25 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=57, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=292352
17/03/31 10:10:25 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=292851 lastFlushOffset=292851 createNewBlock=false
17/03/31 10:10:25 DEBUG DFSClient: Waiting for ack for: 55
[Stage 75:======================>                                  (4 + 4) / 10]17/03/31 10:10:25 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=58, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=292352
17/03/31 10:10:25 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=292851 lastFlushOffset=292851 createNewBlock=false
17/03/31 10:10:25 DEBUG DFSClient: Waiting for ack for: 55
17/03/31 10:10:26 WARN TaskSetManager: Lost task 4.2 in stage 75.0 (TID 232, 172.21.15.173, executor 2): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=4, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:88)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/31 10:10:26 WARN TaskSetManager: Lost task 5.2 in stage 75.0 (TID 233, 172.21.15.173, executor 2): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=5, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:88)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/31 10:10:26 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=59, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=292352
17/03/31 10:10:26 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=298275 lastFlushOffset=292851 createNewBlock=false
17/03/31 10:10:26 DEBUG DFSClient: Queued packet 59
17/03/31 10:10:26 DEBUG DFSClient: Waiting for ack for: 59
17/03/31 10:10:26 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 59 offsetInBlock: 292352 lastPacketInBlock: false lastByteOffsetInBlock: 298275
17/03/31 10:10:26 DEBUG DFSClient: DFSClient seqno: 59 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 64:>                                                         (0 + 0) / 5]17/03/31 10:10:27 WARN TaskSetManager: Lost task 9.1 in stage 75.0 (TID 234, 172.21.15.173, executor 2): FetchFailed(null, shuffleId=2, mapId=-1, reduceId=9, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 2
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:88)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/31 10:10:27 WARN TaskSetManager: Lost task 8.1 in stage 75.0 (TID 235, 172.21.15.173, executor 2): FetchFailed(null, shuffleId=2, mapId=-1, reduceId=8, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 2
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:88)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
[Stage 64:>                                                         (0 + 2) / 5]17/03/31 10:10:28 DEBUG Client: The ping interval is 60000 ms.
17/03/31 10:10:28 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 10:10:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 10:10:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #12
17/03/31 10:10:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #12
17/03/31 10:10:28 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/31 10:10:28 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1146653015_1
17/03/31 10:10:28 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1146653015_1] with renew id 1 executed
[Stage 64:===========>                                              (1 + 2) / 5][Stage 64:=======================>                                  (2 + 2) / 5][Stage 64:==================================>                       (3 + 2) / 5][Stage 64:==============================================>           (4 + 1) / 5]17/03/31 10:10:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=60, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=297984
17/03/31 10:10:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=307473 lastFlushOffset=298275 createNewBlock=false
17/03/31 10:10:32 DEBUG DFSClient: Queued packet 60
17/03/31 10:10:32 DEBUG DFSClient: Waiting for ack for: 60
17/03/31 10:10:32 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 60 offsetInBlock: 297984 lastPacketInBlock: false lastByteOffsetInBlock: 307473
17/03/31 10:10:32 DEBUG DFSClient: DFSClient seqno: 60 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 65:>                                                         (0 + 2) / 4][Stage 65:=============================>                            (2 + 2) / 4]17/03/31 10:10:35 WARN TaskSetManager: Lost task 7.0 in stage 75.0 (TID 227, 172.21.15.173, executor 1): FetchFailed(BlockManagerId(0, 172.21.15.173, 43284, None), shuffleId=9, mapId=4, reduceId=7, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /172.21.15.173:43284
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:88)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to /172.21.15.173:43284
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:181)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: 拒绝连接: /172.21.15.173:43284
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:640)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
	... 2 more

)
17/03/31 10:10:35 WARN TaskSetManager: Lost task 6.0 in stage 75.0 (TID 226, 172.21.15.173, executor 1): FetchFailed(BlockManagerId(0, 172.21.15.173, 43284, None), shuffleId=5, mapId=0, reduceId=6, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /172.21.15.173:43284
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:88)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to /172.21.15.173:43284
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:181)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: 拒绝连接: /172.21.15.173:43284
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:640)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
	... 2 more

)
17/03/31 10:10:37 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=61, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=307200
17/03/31 10:10:37 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=313172 lastFlushOffset=307473 createNewBlock=false
17/03/31 10:10:37 DEBUG DFSClient: Queued packet 61
17/03/31 10:10:37 DEBUG DFSClient: Waiting for ack for: 61
17/03/31 10:10:37 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 61 offsetInBlock: 307200 lastPacketInBlock: false lastByteOffsetInBlock: 313172
17/03/31 10:10:37 DEBUG DFSClient: DFSClient seqno: 61 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:10:37 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=62, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=312832
17/03/31 10:10:37 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=319252 lastFlushOffset=313172 createNewBlock=false
17/03/31 10:10:37 DEBUG DFSClient: Queued packet 62
17/03/31 10:10:37 DEBUG DFSClient: Waiting for ack for: 62
17/03/31 10:10:37 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 62 offsetInBlock: 312832 lastPacketInBlock: false lastByteOffsetInBlock: 319252
17/03/31 10:10:37 DEBUG DFSClient: DFSClient seqno: 62 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 67:=============================>                            (2 + 2) / 4]17/03/31 10:10:38 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 10:10:38 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/31 10:10:39 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=63, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=318976
17/03/31 10:10:39 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=324839 lastFlushOffset=319252 createNewBlock=false
17/03/31 10:10:39 DEBUG DFSClient: Queued packet 63
17/03/31 10:10:39 DEBUG DFSClient: Waiting for ack for: 63
17/03/31 10:10:39 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 63 offsetInBlock: 318976 lastPacketInBlock: false lastByteOffsetInBlock: 324839
17/03/31 10:10:39 DEBUG DFSClient: DFSClient seqno: 63 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 68:=============================================>           (8 + 2) / 10]17/03/31 10:10:40 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=64, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=324608
17/03/31 10:10:40 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=330798 lastFlushOffset=324839 createNewBlock=false
17/03/31 10:10:40 DEBUG DFSClient: Queued packet 64
17/03/31 10:10:40 DEBUG DFSClient: Waiting for ack for: 64
17/03/31 10:10:40 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 64 offsetInBlock: 324608 lastPacketInBlock: false lastByteOffsetInBlock: 330798
17/03/31 10:10:40 DEBUG DFSClient: DFSClient seqno: 64 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 69:=============================>                            (2 + 2) / 4]17/03/31 10:10:43 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=65, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=330752
17/03/31 10:10:43 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=336805 lastFlushOffset=330798 createNewBlock=false
17/03/31 10:10:43 DEBUG DFSClient: Queued packet 65
17/03/31 10:10:43 DEBUG DFSClient: Waiting for ack for: 65
17/03/31 10:10:43 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 65 offsetInBlock: 330752 lastPacketInBlock: false lastByteOffsetInBlock: 336805
17/03/31 10:10:43 DEBUG DFSClient: DFSClient seqno: 65 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 70:>                                                         (0 + 4) / 5][Stage 70:==============================================>           (4 + 1) / 5]17/03/31 10:10:44 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=66, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=336384
17/03/31 10:10:44 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=342347 lastFlushOffset=336805 createNewBlock=false
17/03/31 10:10:44 DEBUG DFSClient: Queued packet 66
17/03/31 10:10:44 DEBUG DFSClient: Waiting for ack for: 66
17/03/31 10:10:44 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 66 offsetInBlock: 336384 lastPacketInBlock: false lastByteOffsetInBlock: 342347
17/03/31 10:10:44 DEBUG DFSClient: DFSClient seqno: 66 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 71:>                                                         (0 + 2) / 4][Stage 71:>                                                         (0 + 4) / 4]17/03/31 10:10:58 DEBUG Client: The ping interval is 60000 ms.
17/03/31 10:10:58 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 10:10:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 10:10:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #13
17/03/31 10:10:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #13
17/03/31 10:10:58 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/31 10:10:58 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1146653015_1
17/03/31 10:10:58 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1146653015_1] with renew id 1 executed
[Stage 71:=============================>                            (2 + 2) / 4]17/03/31 10:11:08 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 10:11:08 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 71:===========================================>              (3 + 1) / 4]17/03/31 10:11:16 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=67, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=342016
17/03/31 10:11:16 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=348342 lastFlushOffset=342347 createNewBlock=false
17/03/31 10:11:16 DEBUG DFSClient: Queued packet 67
17/03/31 10:11:16 DEBUG DFSClient: Waiting for ack for: 67
17/03/31 10:11:16 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 67 offsetInBlock: 342016 lastPacketInBlock: false lastByteOffsetInBlock: 348342
17/03/31 10:11:16 WARN DFSClient: Slow ReadProcessor read fields took 32402ms (threshold=30000ms); ack: seqno: 67 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
[Stage 72:>                                                         (0 + 4) / 4][Stage 72:==============>                                           (1 + 3) / 4]17/03/31 10:11:18 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=68, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=348160
17/03/31 10:11:18 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=358693 lastFlushOffset=348342 createNewBlock=false
17/03/31 10:11:18 DEBUG DFSClient: Queued packet 68
17/03/31 10:11:18 DEBUG DFSClient: Waiting for ack for: 68
17/03/31 10:11:18 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 68 offsetInBlock: 348160 lastPacketInBlock: false lastByteOffsetInBlock: 358693
17/03/31 10:11:18 DEBUG DFSClient: DFSClient seqno: 68 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 73:>                                                         (0 + 4) / 4][Stage 73:==============>                                           (1 + 3) / 4][Stage 73:=============================>                            (2 + 2) / 4]17/03/31 10:11:20 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=69, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=358400
17/03/31 10:11:20 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=363695 lastFlushOffset=358693 createNewBlock=false
17/03/31 10:11:20 DEBUG DFSClient: Queued packet 69
17/03/31 10:11:20 DEBUG DFSClient: Waiting for ack for: 69
17/03/31 10:11:20 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 69 offsetInBlock: 358400 lastPacketInBlock: false lastByteOffsetInBlock: 363695
17/03/31 10:11:20 DEBUG DFSClient: DFSClient seqno: 69 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 74:>                                                         (0 + 4) / 6][Stage 74:=========>                                                (1 + 4) / 6][Stage 74:=============================>                            (3 + 3) / 6][Stage 74:======================================>                   (4 + 2) / 6][Stage 74:================================================>         (5 + 1) / 6]17/03/31 10:11:23 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=70, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=363520
17/03/31 10:11:23 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=368755 lastFlushOffset=363695 createNewBlock=false
17/03/31 10:11:23 DEBUG DFSClient: Queued packet 70
17/03/31 10:11:23 DEBUG DFSClient: Waiting for ack for: 70
17/03/31 10:11:23 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 70 offsetInBlock: 363520 lastPacketInBlock: false lastByteOffsetInBlock: 368755
17/03/31 10:11:23 DEBUG DFSClient: DFSClient seqno: 70 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 75:>                                                         (0 + 2) / 6]17/03/31 10:11:27 WARN TaskSetManager: Lost task 0.0 in stage 75.1 (TID 290, 172.21.15.173, executor 1): java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:331)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:364)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:364)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:356)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:322)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:322)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1303)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:362)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:369)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)

[Stage 75:=========>                                                (1 + 4) / 6]17/03/31 10:11:28 DEBUG Client: The ping interval is 60000 ms.
17/03/31 10:11:28 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 10:11:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 10:11:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #14
17/03/31 10:11:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #14
17/03/31 10:11:28 DEBUG ProtobufRpcEngine: Call: renewLease took 8ms
17/03/31 10:11:28 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1146653015_1
17/03/31 10:11:28 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1146653015_1] with renew id 1 executed
17/03/31 10:11:30 WARN TransportChannelHandler: Exception in connection from /172.21.15.173:55607
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:221)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:899)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:275)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)
17/03/31 10:11:30 ERROR TaskSchedulerImpl: Lost executor 1 on 172.21.15.173: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/31 10:11:30 WARN TaskSetManager: Lost task 5.0 in stage 75.1 (TID 295, 172.21.15.173, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/31 10:11:30 WARN TaskSetManager: Lost task 4.0 in stage 75.1 (TID 294, 172.21.15.173, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/31 10:11:30 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=71, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=368640
17/03/31 10:11:30 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=373577 lastFlushOffset=368755 createNewBlock=false
17/03/31 10:11:30 DEBUG DFSClient: Queued packet 71
17/03/31 10:11:30 DEBUG DFSClient: Waiting for ack for: 71
17/03/31 10:11:30 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 71 offsetInBlock: 368640 lastPacketInBlock: false lastByteOffsetInBlock: 373577
17/03/31 10:11:30 DEBUG DFSClient: DFSClient seqno: 71 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:11:30 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=72, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=373248
17/03/31 10:11:30 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=373577 lastFlushOffset=373577 createNewBlock=false
17/03/31 10:11:30 DEBUG DFSClient: Waiting for ack for: 71
[Stage 75:=========>                                                (1 + 2) / 6]17/03/31 10:11:33 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=73, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=373248
17/03/31 10:11:33 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=373577 lastFlushOffset=373577 createNewBlock=false
17/03/31 10:11:33 DEBUG DFSClient: Waiting for ack for: 71
17/03/31 10:11:33 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=74, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=373248
17/03/31 10:11:33 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=373577 lastFlushOffset=373577 createNewBlock=false
17/03/31 10:11:33 DEBUG DFSClient: Waiting for ack for: 71
[Stage 75:=========>                                                (1 + 4) / 6]17/03/31 10:11:33 WARN TaskSetManager: Lost task 4.1 in stage 75.1 (TID 296, 172.21.15.173, executor 3): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=8, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:88)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/31 10:11:33 WARN TaskSetManager: Lost task 5.1 in stage 75.1 (TID 297, 172.21.15.173, executor 3): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=9, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:88)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/31 10:11:33 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=75, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=373248
17/03/31 10:11:33 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=380061 lastFlushOffset=373577 createNewBlock=false
17/03/31 10:11:33 DEBUG DFSClient: Queued packet 75
17/03/31 10:11:33 DEBUG DFSClient: Waiting for ack for: 75
17/03/31 10:11:33 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 75 offsetInBlock: 373248 lastPacketInBlock: false lastByteOffsetInBlock: 380061
17/03/31 10:11:33 DEBUG DFSClient: DFSClient seqno: 75 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:11:34 WARN TaskSetManager: Lost task 0.1 in stage 75.1 (TID 298, 172.21.15.173, executor 3): FetchFailed(null, shuffleId=2, mapId=-1, reduceId=4, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 2
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:88)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
[Stage 64:>                                                         (0 + 2) / 5][Stage 64:===========>                                              (1 + 2) / 5][Stage 64:=======================>                                  (2 + 2) / 5]17/03/31 10:11:38 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 10:11:38 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 64:==================================>                       (3 + 2) / 5][Stage 64:==============================================>           (4 + 1) / 5]17/03/31 10:11:39 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=76, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=379904
17/03/31 10:11:39 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=384738 lastFlushOffset=380061 createNewBlock=false
17/03/31 10:11:39 DEBUG DFSClient: Queued packet 76
17/03/31 10:11:39 DEBUG DFSClient: Waiting for ack for: 76
17/03/31 10:11:39 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 76 offsetInBlock: 379904 lastPacketInBlock: false lastByteOffsetInBlock: 384738
17/03/31 10:11:39 DEBUG DFSClient: DFSClient seqno: 76 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 65:>                                                         (0 + 0) / 6][Stage 65:>                                                         (0 + 2) / 6][Stage 65:===================>                                      (2 + 2) / 6]17/03/31 10:11:45 WARN TaskSetManager: Lost task 2.0 in stage 75.1 (TID 292, 172.21.15.173, executor 2): FetchFailed(BlockManagerId(1, 172.21.15.173, 40180, None), shuffleId=2, mapId=1, reduceId=6, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /172.21.15.173:40180
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:88)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to /172.21.15.173:40180
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:181)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: 拒绝连接: /172.21.15.173:40180
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:640)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
	... 2 more

)
17/03/31 10:11:45 WARN TaskSetManager: Lost task 3.0 in stage 75.1 (TID 293, 172.21.15.173, executor 2): FetchFailed(BlockManagerId(1, 172.21.15.173, 40180, None), shuffleId=2, mapId=1, reduceId=7, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /172.21.15.173:40180
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:88)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to /172.21.15.173:40180
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:181)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: 拒绝连接: /172.21.15.173:40180
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:640)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
	... 2 more

)
[Stage 65:===================>                                      (2 + 4) / 6][Stage 65:=============================>                            (3 + 3) / 6][Stage 65:======================================>                   (4 + 2) / 6]17/03/31 10:11:47 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=77, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=384512
17/03/31 10:11:47 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=395015 lastFlushOffset=384738 createNewBlock=false
17/03/31 10:11:47 DEBUG DFSClient: Queued packet 77
17/03/31 10:11:47 DEBUG DFSClient: Waiting for ack for: 77
17/03/31 10:11:47 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 77 offsetInBlock: 384512 lastPacketInBlock: false lastByteOffsetInBlock: 395015
17/03/31 10:11:47 DEBUG DFSClient: DFSClient seqno: 77 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:11:48 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=78, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=394752
17/03/31 10:11:48 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=405537 lastFlushOffset=395015 createNewBlock=false
17/03/31 10:11:48 DEBUG DFSClient: Queued packet 78
17/03/31 10:11:48 DEBUG DFSClient: Waiting for ack for: 78
17/03/31 10:11:48 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 78 offsetInBlock: 394752 lastPacketInBlock: false lastByteOffsetInBlock: 405537
17/03/31 10:11:48 DEBUG DFSClient: DFSClient seqno: 78 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 67:======================================>                   (4 + 2) / 6][Stage 67:================================================>         (5 + 1) / 6]17/03/31 10:11:49 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=79, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=405504
17/03/31 10:11:49 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=410913 lastFlushOffset=405537 createNewBlock=false
17/03/31 10:11:49 DEBUG DFSClient: Queued packet 79
17/03/31 10:11:49 DEBUG DFSClient: Waiting for ack for: 79
17/03/31 10:11:49 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 79 offsetInBlock: 405504 lastPacketInBlock: false lastByteOffsetInBlock: 410913
17/03/31 10:11:49 DEBUG DFSClient: DFSClient seqno: 79 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:11:49 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=80, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=410624
17/03/31 10:11:49 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=416514 lastFlushOffset=410913 createNewBlock=false
17/03/31 10:11:49 DEBUG DFSClient: Queued packet 80
17/03/31 10:11:49 DEBUG DFSClient: Waiting for ack for: 80
17/03/31 10:11:49 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 80 offsetInBlock: 410624 lastPacketInBlock: false lastByteOffsetInBlock: 416514
17/03/31 10:11:49 DEBUG DFSClient: DFSClient seqno: 80 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 69:>                                                         (0 + 4) / 6][Stage 69:======================================>                   (4 + 2) / 6]17/03/31 10:11:50 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=81, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=416256
17/03/31 10:11:50 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=422329 lastFlushOffset=416514 createNewBlock=false
17/03/31 10:11:50 DEBUG DFSClient: Queued packet 81
17/03/31 10:11:50 DEBUG DFSClient: Waiting for ack for: 81
17/03/31 10:11:50 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 81 offsetInBlock: 416256 lastPacketInBlock: false lastByteOffsetInBlock: 422329
17/03/31 10:11:50 DEBUG DFSClient: DFSClient seqno: 81 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 70:==============>                                           (2 + 4) / 8][Stage 70:=============================>                            (4 + 4) / 8][Stage 70:===========================================>              (6 + 2) / 8]17/03/31 10:11:52 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=82, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=421888
17/03/31 10:11:52 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=428655 lastFlushOffset=422329 createNewBlock=false
17/03/31 10:11:52 DEBUG DFSClient: Queued packet 82
17/03/31 10:11:52 DEBUG DFSClient: Waiting for ack for: 82
17/03/31 10:11:52 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 82 offsetInBlock: 421888 lastPacketInBlock: false lastByteOffsetInBlock: 428655
17/03/31 10:11:52 DEBUG DFSClient: DFSClient seqno: 82 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 71:>                                                         (0 + 0) / 8][Stage 71:>                                                         (0 + 4) / 8]17/03/31 10:11:58 DEBUG Client: The ping interval is 60000 ms.
17/03/31 10:11:58 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 10:11:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 10:11:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #15
17/03/31 10:11:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #15
17/03/31 10:11:58 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/31 10:11:58 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1146653015_1
17/03/31 10:11:58 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1146653015_1] with renew id 1 executed
17/03/31 10:12:08 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 10:12:08 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 71:=======>                                                  (1 + 4) / 8][Stage 71:==============>                                           (2 + 3) / 8][Stage 71:==============>                                           (2 + 4) / 8][Stage 71:=====================>                                    (3 + 4) / 8]17/03/31 10:12:28 DEBUG Client: The ping interval is 60000 ms.
17/03/31 10:12:28 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 10:12:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 10:12:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #16
17/03/31 10:12:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #16
17/03/31 10:12:28 DEBUG ProtobufRpcEngine: Call: renewLease took 5ms
17/03/31 10:12:28 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1146653015_1
17/03/31 10:12:28 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1146653015_1] with renew id 1 executed
17/03/31 10:12:38 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 10:12:38 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/31 10:12:38 ERROR TaskSchedulerImpl: Lost executor 2 on 172.21.15.173: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/31 10:12:38 WARN TaskSetManager: Lost task 4.0 in stage 71.1 (TID 346, 172.21.15.173, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/31 10:12:38 WARN TaskSetManager: Lost task 3.0 in stage 71.1 (TID 344, 172.21.15.173, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/31 10:12:38 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=83, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=428544
17/03/31 10:12:38 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=434714 lastFlushOffset=428655 createNewBlock=false
17/03/31 10:12:38 DEBUG DFSClient: Queued packet 83
17/03/31 10:12:38 DEBUG DFSClient: Waiting for ack for: 83
17/03/31 10:12:38 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 83 offsetInBlock: 428544 lastPacketInBlock: false lastByteOffsetInBlock: 434714
17/03/31 10:12:38 WARN DFSClient: Slow ReadProcessor read fields took 46547ms (threshold=30000ms); ack: seqno: 83 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
17/03/31 10:12:38 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=84, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=434688
17/03/31 10:12:38 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=434714 lastFlushOffset=434714 createNewBlock=false
17/03/31 10:12:38 DEBUG DFSClient: Waiting for ack for: 83
[Stage 71:=====================>                                    (3 + 2) / 8]17/03/31 10:12:39 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=85, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=434688
17/03/31 10:12:39 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=434714 lastFlushOffset=434714 createNewBlock=false
17/03/31 10:12:39 DEBUG DFSClient: Waiting for ack for: 83
17/03/31 10:12:39 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=86, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=434688
17/03/31 10:12:39 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=434714 lastFlushOffset=434714 createNewBlock=false
17/03/31 10:12:39 DEBUG DFSClient: Waiting for ack for: 83
[Stage 71:=====================>                                    (3 + 4) / 8]17/03/31 10:12:40 WARN TaskSetManager: Lost task 4.1 in stage 71.1 (TID 352, 172.21.15.173, executor 4): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=6, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/31 10:12:40 WARN TaskSetManager: Lost task 3.1 in stage 71.1 (TID 351, 172.21.15.173, executor 4): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=5, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/31 10:12:40 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=87, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=434688
17/03/31 10:12:40 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=440813 lastFlushOffset=434714 createNewBlock=false
17/03/31 10:12:40 DEBUG DFSClient: Queued packet 87
17/03/31 10:12:40 DEBUG DFSClient: Waiting for ack for: 87
17/03/31 10:12:40 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 87 offsetInBlock: 434688 lastPacketInBlock: false lastByteOffsetInBlock: 440813
17/03/31 10:12:40 DEBUG DFSClient: DFSClient seqno: 87 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:12:40 WARN TaskSetManager: Lost task 7.0 in stage 71.1 (TID 353, 172.21.15.173, executor 4): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=9, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
[Stage 64:>                                                         (0 + 2) / 5][Stage 64:=======================>                                  (2 + 2) / 5][Stage 64:==================================>                       (3 + 2) / 5][Stage 64:==============================================>           (4 + 1) / 5]17/03/31 10:12:45 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=88, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=440320
17/03/31 10:12:45 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=445772 lastFlushOffset=440813 createNewBlock=false
17/03/31 10:12:45 DEBUG DFSClient: Queued packet 88
17/03/31 10:12:45 DEBUG DFSClient: Waiting for ack for: 88
17/03/31 10:12:45 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 88 offsetInBlock: 440320 lastPacketInBlock: false lastByteOffsetInBlock: 445772
17/03/31 10:12:45 DEBUG DFSClient: DFSClient seqno: 88 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 65:>                                                         (0 + 0) / 6][Stage 65:>                                                         (0 + 2) / 6]17/03/31 10:12:50 WARN TaskSetManager: Lost task 5.0 in stage 71.1 (TID 349, 172.21.15.173, executor 3): FetchFailed(BlockManagerId(2, 172.21.15.173, 58691, None), shuffleId=0, mapId=2, reduceId=7, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /172.21.15.173:58691
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.graphx.impl.GraphImpl$$anonfun$4.apply(GraphImpl.scala:108)
	at org.apache.spark.graphx.impl.GraphImpl$$anonfun$4.apply(GraphImpl.scala:106)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:845)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:845)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to /172.21.15.173:58691
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:181)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: 拒绝连接: /172.21.15.173:58691
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:640)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
	... 2 more

)
[Stage 65:>                                                         (0 + 3) / 6]17/03/31 10:12:51 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=89, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=445440
17/03/31 10:12:51 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=445772 lastFlushOffset=445772 createNewBlock=false
17/03/31 10:12:51 DEBUG DFSClient: Waiting for ack for: 88
[Stage 65:=========>                                                (1 + 3) / 6][Stage 65:=============================>                            (3 + 3) / 6][Stage 65:======================================>                   (4 + 2) / 6]17/03/31 10:12:55 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=90, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=445440
17/03/31 10:12:55 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=455441 lastFlushOffset=445772 createNewBlock=false
17/03/31 10:12:55 DEBUG DFSClient: Queued packet 90
17/03/31 10:12:55 DEBUG DFSClient: Waiting for ack for: 90
17/03/31 10:12:55 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 90 offsetInBlock: 445440 lastPacketInBlock: false lastByteOffsetInBlock: 455441
17/03/31 10:12:55 DEBUG DFSClient: DFSClient seqno: 90 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:12:55 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=91, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=455168
17/03/31 10:12:55 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=460724 lastFlushOffset=455441 createNewBlock=false
17/03/31 10:12:55 DEBUG DFSClient: Queued packet 91
17/03/31 10:12:55 DEBUG DFSClient: Waiting for ack for: 91
17/03/31 10:12:55 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 91 offsetInBlock: 455168 lastPacketInBlock: false lastByteOffsetInBlock: 460724
17/03/31 10:12:55 DEBUG DFSClient: DFSClient seqno: 91 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 67:=============================>                            (4 + 3) / 8][Stage 67:===========================================>              (6 + 2) / 8][Stage 67:==================================================>       (7 + 1) / 8]17/03/31 10:12:57 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=92, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=460288
17/03/31 10:12:57 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=470875 lastFlushOffset=460724 createNewBlock=false
17/03/31 10:12:57 DEBUG DFSClient: Queued packet 92
17/03/31 10:12:57 DEBUG DFSClient: Waiting for ack for: 92
17/03/31 10:12:57 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 92 offsetInBlock: 460288 lastPacketInBlock: false lastByteOffsetInBlock: 470875
17/03/31 10:12:57 DEBUG DFSClient: DFSClient seqno: 92 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:12:58 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=93, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=470528
17/03/31 10:12:58 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=475709 lastFlushOffset=470875 createNewBlock=false
17/03/31 10:12:58 DEBUG DFSClient: Queued packet 93
17/03/31 10:12:58 DEBUG DFSClient: Waiting for ack for: 93
17/03/31 10:12:58 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 93 offsetInBlock: 470528 lastPacketInBlock: false lastByteOffsetInBlock: 475709
17/03/31 10:12:58 DEBUG Client: The ping interval is 60000 ms.
17/03/31 10:12:58 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 10:12:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 10:12:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #17
17/03/31 10:12:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #17
17/03/31 10:12:58 DEBUG ProtobufRpcEngine: Call: renewLease took 3ms
17/03/31 10:12:58 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1146653015_1
17/03/31 10:12:58 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1146653015_1] with renew id 1 executed
17/03/31 10:12:58 DEBUG DFSClient: DFSClient seqno: 93 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 69:=====================>                                    (3 + 3) / 8][Stage 69:=============================>                            (4 + 3) / 8][Stage 69:==================================================>       (7 + 1) / 8]17/03/31 10:12:59 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=94, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=475648
17/03/31 10:12:59 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=487173 lastFlushOffset=475709 createNewBlock=false
17/03/31 10:12:59 DEBUG DFSClient: Queued packet 94
17/03/31 10:12:59 DEBUG DFSClient: Waiting for ack for: 94
17/03/31 10:12:59 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 94 offsetInBlock: 475648 lastPacketInBlock: false lastByteOffsetInBlock: 487173
17/03/31 10:12:59 DEBUG DFSClient: DFSClient seqno: 94 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 70:=============================>                            (3 + 3) / 6]17/03/31 10:13:00 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=95, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=486912
17/03/31 10:13:00 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=493134 lastFlushOffset=487173 createNewBlock=false
17/03/31 10:13:00 DEBUG DFSClient: Queued packet 95
17/03/31 10:13:00 DEBUG DFSClient: Waiting for ack for: 95
17/03/31 10:13:00 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 95 offsetInBlock: 486912 lastPacketInBlock: false lastByteOffsetInBlock: 493134
17/03/31 10:13:00 DEBUG DFSClient: DFSClient seqno: 95 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 71:>                                                         (0 + 3) / 7][Stage 71:========>                                                 (1 + 3) / 7]17/03/31 10:13:05 WARN TaskSetManager: Lost task 6.0 in stage 71.1 (TID 350, 172.21.15.173, executor 3): FetchFailed(BlockManagerId(2, 172.21.15.173, 58691, None), shuffleId=0, mapId=2, reduceId=8, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /172.21.15.173:58691
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.graphx.impl.GraphImpl$$anonfun$4.apply(GraphImpl.scala:108)
	at org.apache.spark.graphx.impl.GraphImpl$$anonfun$4.apply(GraphImpl.scala:106)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:845)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:845)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to /172.21.15.173:58691
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:181)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: 拒绝连接: /172.21.15.173:58691
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:640)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
	... 2 more

)
[Stage 71:================>                                         (2 + 4) / 7]17/03/31 10:13:08 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 10:13:08 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/31 10:13:28 DEBUG Client: The ping interval is 60000 ms.
17/03/31 10:13:28 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 10:13:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 10:13:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #18
17/03/31 10:13:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #18
17/03/31 10:13:28 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/31 10:13:28 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1146653015_1
17/03/31 10:13:28 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1146653015_1] with renew id 1 executed
[Stage 71:=================================>                        (4 + 3) / 7]17/03/31 10:13:30 WARN TaskSetManager: Lost task 4.0 in stage 71.2 (TID 407, 172.21.15.173, executor 3): java.lang.OutOfMemoryError: GC overhead limit exceeded
	at org.apache.spark.graphx.lib.SVDPlusPlus$$anonfun$run$1$$anonfun$13.apply(SVDPlusPlus.scala:156)
	at org.apache.spark.graphx.lib.SVDPlusPlus$$anonfun$run$1$$anonfun$13.apply(SVDPlusPlus.scala:154)
	at org.apache.spark.graphx.impl.AggregatingEdgeContext.send(EdgePartition.scala:536)
	at org.apache.spark.graphx.impl.AggregatingEdgeContext.sendToDst(EdgePartition.scala:531)
	at org.apache.spark.graphx.lib.SVDPlusPlus$.org$apache$spark$graphx$lib$SVDPlusPlus$$sendMsgTrainF$1(SVDPlusPlus.scala:122)
	at org.apache.spark.graphx.lib.SVDPlusPlus$$anonfun$run$1$$anonfun$12.apply(SVDPlusPlus.scala:153)
	at org.apache.spark.graphx.lib.SVDPlusPlus$$anonfun$run$1$$anonfun$12.apply(SVDPlusPlus.scala:153)
	at org.apache.spark.graphx.impl.EdgePartition.aggregateMessagesEdgeScan(EdgePartition.scala:409)
	at org.apache.spark.graphx.impl.GraphImpl$$anonfun$13$$anonfun$apply$3.apply(GraphImpl.scala:237)
	at org.apache.spark.graphx.impl.GraphImpl$$anonfun$13$$anonfun$apply$3.apply(GraphImpl.scala:207)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/03/31 10:13:31 ERROR TaskSchedulerImpl: Lost executor 3 on 172.21.15.173: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/31 10:13:31 WARN TaskSetManager: Lost task 5.0 in stage 71.2 (TID 408, 172.21.15.173, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/31 10:13:31 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=96, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=493056
17/03/31 10:13:31 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=500052 lastFlushOffset=493134 createNewBlock=false
17/03/31 10:13:31 DEBUG DFSClient: Queued packet 96
17/03/31 10:13:31 DEBUG DFSClient: Waiting for ack for: 96
17/03/31 10:13:31 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 96 offsetInBlock: 493056 lastPacketInBlock: false lastByteOffsetInBlock: 500052
17/03/31 10:13:31 WARN DFSClient: Slow ReadProcessor read fields took 30536ms (threshold=30000ms); ack: seqno: 96 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
17/03/31 10:13:31 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=97, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=499712
17/03/31 10:13:31 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=500052 lastFlushOffset=500052 createNewBlock=false
17/03/31 10:13:31 DEBUG DFSClient: Waiting for ack for: 96
[Stage 71:=================================>                        (4 + 0) / 7][Stage 71:=========================================>                (5 + 0) / 7]17/03/31 10:13:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=98, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=499712
17/03/31 10:13:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=500052 lastFlushOffset=500052 createNewBlock=false
17/03/31 10:13:32 DEBUG DFSClient: Waiting for ack for: 96
17/03/31 10:13:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=99, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=499712
17/03/31 10:13:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=500052 lastFlushOffset=500052 createNewBlock=false
17/03/31 10:13:32 DEBUG DFSClient: Waiting for ack for: 96
[Stage 71:=========================================>                (5 + 2) / 7]17/03/31 10:13:37 WARN TaskSetManager: Lost task 2.1 in stage 71.2 (TID 412, 172.21.15.173, executor 5): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=5, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/31 10:13:37 WARN TaskSetManager: Lost task 3.1 in stage 71.2 (TID 413, 172.21.15.173, executor 5): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=6, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/31 10:13:37 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=100, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=499712
17/03/31 10:13:37 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=506680 lastFlushOffset=500052 createNewBlock=false
17/03/31 10:13:37 DEBUG DFSClient: Queued packet 100
17/03/31 10:13:37 DEBUG DFSClient: Waiting for ack for: 100
17/03/31 10:13:37 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 100 offsetInBlock: 499712 lastPacketInBlock: false lastByteOffsetInBlock: 506680
17/03/31 10:13:37 DEBUG DFSClient: DFSClient seqno: 100 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 64:>                                                         (0 + 0) / 5][Stage 64:>                                                         (0 + 2) / 5]17/03/31 10:13:38 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 10:13:38 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 64:===========>                                              (1 + 2) / 5][Stage 64:=======================>                                  (2 + 2) / 5][Stage 64:==================================>                       (3 + 2) / 5][Stage 64:==============================================>           (4 + 1) / 5]17/03/31 10:13:42 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=101, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=506368
17/03/31 10:13:42 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=511953 lastFlushOffset=506680 createNewBlock=false
17/03/31 10:13:42 DEBUG DFSClient: Queued packet 101
17/03/31 10:13:42 DEBUG DFSClient: Waiting for ack for: 101
17/03/31 10:13:42 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 101 offsetInBlock: 506368 lastPacketInBlock: false lastByteOffsetInBlock: 511953
17/03/31 10:13:42 DEBUG DFSClient: DFSClient seqno: 101 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 65:>                                                         (0 + 0) / 6]17/03/31 10:13:45 WARN TaskSetManager: Lost task 4.1 in stage 71.2 (TID 410, 172.21.15.173, executor 4): FetchFailed(BlockManagerId(3, 172.21.15.173, 35516, None), shuffleId=2, mapId=5, reduceId=7, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /172.21.15.173:35516
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to /172.21.15.173:35516
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:181)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: 拒绝连接: /172.21.15.173:35516
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:640)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
	... 2 more

)
[Stage 65:>                                                         (0 + 1) / 6][Stage 65:=========>                                                (1 + 1) / 6][Stage 65:=========>                                                (1 + 3) / 6][Stage 65:===================>                                      (2 + 3) / 6]17/03/31 10:13:47 WARN TaskSetManager: Lost task 5.1 in stage 71.2 (TID 411, 172.21.15.173, executor 4): FetchFailed(BlockManagerId(3, 172.21.15.173, 35516, None), shuffleId=2, mapId=1, reduceId=8, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /172.21.15.173:35516
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to /172.21.15.173:35516
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:181)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: 拒绝连接: /172.21.15.173:35516
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:640)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
	... 2 more

)
[Stage 65:===================>                                      (2 + 4) / 6][Stage 65:======================================>                   (4 + 2) / 6][Stage 65:================================================>         (5 + 1) / 6]17/03/31 10:13:50 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=102, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=511488
17/03/31 10:13:50 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=523545 lastFlushOffset=511953 createNewBlock=false
17/03/31 10:13:50 DEBUG DFSClient: Queued packet 102
17/03/31 10:13:50 DEBUG DFSClient: Waiting for ack for: 102
17/03/31 10:13:50 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 102 offsetInBlock: 511488 lastPacketInBlock: false lastByteOffsetInBlock: 523545
17/03/31 10:13:50 DEBUG DFSClient: DFSClient seqno: 102 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:13:51 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=103, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=523264
17/03/31 10:13:51 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=528893 lastFlushOffset=523545 createNewBlock=false
17/03/31 10:13:51 DEBUG DFSClient: Queued packet 103
17/03/31 10:13:51 DEBUG DFSClient: Waiting for ack for: 103
17/03/31 10:13:51 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 103 offsetInBlock: 523264 lastPacketInBlock: false lastByteOffsetInBlock: 528893
17/03/31 10:13:51 DEBUG DFSClient: DFSClient seqno: 103 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 67:=======================>                                  (2 + 2) / 5][Stage 67:==================================>                       (3 + 2) / 5][Stage 67:==============================================>           (4 + 1) / 5]17/03/31 10:13:52 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=104, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=528384
17/03/31 10:13:52 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=534606 lastFlushOffset=528893 createNewBlock=false
17/03/31 10:13:52 DEBUG DFSClient: Queued packet 104
17/03/31 10:13:52 DEBUG DFSClient: Waiting for ack for: 104
17/03/31 10:13:52 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 104 offsetInBlock: 528384 lastPacketInBlock: false lastByteOffsetInBlock: 534606
17/03/31 10:13:52 DEBUG DFSClient: DFSClient seqno: 104 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:13:52 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=105, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=534528
17/03/31 10:13:52 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=540058 lastFlushOffset=534606 createNewBlock=false
17/03/31 10:13:52 DEBUG DFSClient: Queued packet 105
17/03/31 10:13:52 DEBUG DFSClient: Waiting for ack for: 105
17/03/31 10:13:52 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 105 offsetInBlock: 534528 lastPacketInBlock: false lastByteOffsetInBlock: 540058
17/03/31 10:13:52 DEBUG DFSClient: DFSClient seqno: 105 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 69:>                                                         (0 + 4) / 5][Stage 69:=======================>                                  (2 + 2) / 5][Stage 69:==================================>                       (3 + 2) / 5][Stage 69:==============================================>           (4 + 1) / 5]17/03/31 10:13:53 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=106, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=539648
17/03/31 10:13:53 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=546128 lastFlushOffset=540058 createNewBlock=false
17/03/31 10:13:53 DEBUG DFSClient: Queued packet 106
17/03/31 10:13:53 DEBUG DFSClient: Waiting for ack for: 106
17/03/31 10:13:53 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 106 offsetInBlock: 539648 lastPacketInBlock: false lastByteOffsetInBlock: 546128
17/03/31 10:13:53 DEBUG DFSClient: DFSClient seqno: 106 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 70:>                                                         (0 + 4) / 6][Stage 70:===================>                                      (2 + 4) / 6][Stage 70:======================================>                   (4 + 2) / 6]17/03/31 10:13:55 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=107, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=545792
17/03/31 10:13:55 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=552030 lastFlushOffset=546128 createNewBlock=false
17/03/31 10:13:55 DEBUG DFSClient: Queued packet 107
17/03/31 10:13:55 DEBUG DFSClient: Waiting for ack for: 107
17/03/31 10:13:55 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 107 offsetInBlock: 545792 lastPacketInBlock: false lastByteOffsetInBlock: 552030
17/03/31 10:13:55 DEBUG DFSClient: DFSClient seqno: 107 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 71:>                                                         (0 + 4) / 7]17/03/31 10:13:58 DEBUG Client: The ping interval is 60000 ms.
17/03/31 10:13:58 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 10:13:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 10:13:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #19
17/03/31 10:13:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #19
17/03/31 10:13:58 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/31 10:13:58 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1146653015_1
17/03/31 10:13:58 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1146653015_1] with renew id 1 executed
17/03/31 10:14:08 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 10:14:08 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 71:================>                                         (2 + 4) / 7]17/03/31 10:14:28 DEBUG Client: The ping interval is 60000 ms.
17/03/31 10:14:28 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 10:14:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 10:14:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #20
17/03/31 10:14:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #20
17/03/31 10:14:28 DEBUG ProtobufRpcEngine: Call: renewLease took 5ms
17/03/31 10:14:28 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1146653015_1
17/03/31 10:14:28 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1146653015_1] with renew id 1 executed
17/03/31 10:14:38 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 10:14:38 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/31 10:14:58 DEBUG Client: The ping interval is 60000 ms.
17/03/31 10:14:58 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 10:14:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 10:14:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #21
17/03/31 10:14:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #21
17/03/31 10:14:58 DEBUG ProtobufRpcEngine: Call: renewLease took 5ms
17/03/31 10:14:58 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1146653015_1
17/03/31 10:14:58 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1146653015_1] with renew id 1 executed
[Stage 71:========================>                                 (3 + 4) / 7][Stage 71:=================================>                        (4 + 3) / 7]17/03/31 10:15:02 WARN TaskSetManager: Lost task 3.0 in stage 71.3 (TID 455, 172.21.15.173, executor 4): java.lang.OutOfMemoryError: GC overhead limit exceeded

17/03/31 10:15:02 WARN TaskSetManager: Lost task 3.1 in stage 71.3 (TID 459, 172.21.15.173, executor 4): FetchFailed(BlockManagerId(4, 172.21.15.173, 36239, None), shuffleId=2, mapId=1, reduceId=5, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-f3dc15b7-cbd2-4777-a10e-40b35a7f6f70/executor-98e72017-328e-45f2-8c3a-09208a35a004/blockmgr-59586212-c50d-402a-8688-6a368669c2a5/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: /tmp/spark-f3dc15b7-cbd2-4777-a10e-40b35a7f6f70/executor-98e72017-328e-45f2-8c3a-09208a35a004/blockmgr-59586212-c50d-402a-8688-6a368669c2a5/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:199)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:302)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:269)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:303)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:132)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	... 7 more

)
17/03/31 10:15:02 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=108, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=551936
17/03/31 10:15:02 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=564225 lastFlushOffset=552030 createNewBlock=false
17/03/31 10:15:02 DEBUG DFSClient: Queued packet 108
17/03/31 10:15:02 DEBUG DFSClient: Waiting for ack for: 108
17/03/31 10:15:02 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 108 offsetInBlock: 551936 lastPacketInBlock: false lastByteOffsetInBlock: 564225
17/03/31 10:15:02 WARN DFSClient: Slow ReadProcessor read fields took 67805ms (threshold=30000ms); ack: seqno: 108 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
17/03/31 10:15:02 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=109, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=564224
17/03/31 10:15:02 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=564225 lastFlushOffset=564225 createNewBlock=false
17/03/31 10:15:02 DEBUG DFSClient: Waiting for ack for: 108
17/03/31 10:15:03 WARN TransportChannelHandler: Exception in connection from /172.21.15.173:55720
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:221)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:899)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:275)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)
17/03/31 10:15:03 ERROR TaskSchedulerImpl: Lost executor 4 on 172.21.15.173: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/31 10:15:03 WARN TaskSetManager: Lost task 0.0 in stage 71.3 (TID 453, 172.21.15.173, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/31 10:15:03 WARN TaskSetManager: Lost task 1.0 in stage 64.5 (TID 461, 172.21.15.173, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/31 10:15:03 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=110, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=564224
17/03/31 10:15:03 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=564225 lastFlushOffset=564225 createNewBlock=false
17/03/31 10:15:03 DEBUG DFSClient: Waiting for ack for: 108
[Stage 64:>                                                         (0 + 1) / 5]17/03/31 10:15:04 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=111, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=564224
17/03/31 10:15:04 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=564225 lastFlushOffset=564225 createNewBlock=false
17/03/31 10:15:04 DEBUG DFSClient: Waiting for ack for: 108
[Stage 64:>                                                         (0 + 3) / 5]17/03/31 10:15:04 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=112, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=564224
17/03/31 10:15:04 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=564225 lastFlushOffset=564225 createNewBlock=false
17/03/31 10:15:04 DEBUG DFSClient: Waiting for ack for: 108
[Stage 64:===========>                                              (1 + 3) / 5][Stage 64:=======================>                                  (2 + 3) / 5][Stage 64:==================================>                       (3 + 2) / 5]17/03/31 10:15:08 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 10:15:08 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/31 10:15:08 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=113, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=564224
17/03/31 10:15:08 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=569957 lastFlushOffset=564225 createNewBlock=false
17/03/31 10:15:08 DEBUG DFSClient: Queued packet 113
17/03/31 10:15:08 DEBUG DFSClient: Waiting for ack for: 113
17/03/31 10:15:08 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 113 offsetInBlock: 564224 lastPacketInBlock: false lastByteOffsetInBlock: 569957
17/03/31 10:15:08 DEBUG DFSClient: DFSClient seqno: 113 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 65:==============>                                           (2 + 3) / 8][Stage 65:=====================>                                    (3 + 3) / 8][Stage 65:====================================>                     (5 + 3) / 8][Stage 65:===========================================>              (6 + 2) / 8]17/03/31 10:15:13 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=114, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=569856
17/03/31 10:15:13 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=582515 lastFlushOffset=569957 createNewBlock=false
17/03/31 10:15:13 DEBUG DFSClient: Queued packet 114
17/03/31 10:15:13 DEBUG DFSClient: Waiting for ack for: 114
17/03/31 10:15:13 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 114 offsetInBlock: 569856 lastPacketInBlock: false lastByteOffsetInBlock: 582515
17/03/31 10:15:13 DEBUG DFSClient: DFSClient seqno: 114 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:15:14 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=115, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=582144
17/03/31 10:15:14 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=587974 lastFlushOffset=582515 createNewBlock=false
17/03/31 10:15:14 DEBUG DFSClient: Queued packet 115
17/03/31 10:15:14 DEBUG DFSClient: Waiting for ack for: 115
17/03/31 10:15:14 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 115 offsetInBlock: 582144 lastPacketInBlock: false lastByteOffsetInBlock: 587974
17/03/31 10:15:14 DEBUG DFSClient: DFSClient seqno: 115 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 67:=====================>                                    (3 + 3) / 8][Stage 67:====================================>                     (5 + 1) / 8][Stage 67:==================================================>       (7 + 1) / 8]17/03/31 10:15:15 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=116, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=587776
17/03/31 10:15:15 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=593720 lastFlushOffset=587974 createNewBlock=false
17/03/31 10:15:15 DEBUG DFSClient: Queued packet 116
17/03/31 10:15:15 DEBUG DFSClient: Waiting for ack for: 116
17/03/31 10:15:15 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 116 offsetInBlock: 587776 lastPacketInBlock: false lastByteOffsetInBlock: 593720
17/03/31 10:15:15 DEBUG DFSClient: DFSClient seqno: 116 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:15:15 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=117, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=593408
17/03/31 10:15:15 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=599665 lastFlushOffset=593720 createNewBlock=false
17/03/31 10:15:15 DEBUG DFSClient: Queued packet 117
17/03/31 10:15:15 DEBUG DFSClient: Waiting for ack for: 117
17/03/31 10:15:15 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 117 offsetInBlock: 593408 lastPacketInBlock: false lastByteOffsetInBlock: 599665
17/03/31 10:15:15 DEBUG DFSClient: DFSClient seqno: 117 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 69:=====================>                                    (3 + 3) / 8][Stage 69:===========================================>              (6 + 1) / 8][Stage 69:==================================================>       (7 + 1) / 8]17/03/31 10:15:17 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=118, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=599552
17/03/31 10:15:17 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=610985 lastFlushOffset=599665 createNewBlock=false
17/03/31 10:15:17 DEBUG DFSClient: Queued packet 118
17/03/31 10:15:17 DEBUG DFSClient: Waiting for ack for: 118
17/03/31 10:15:17 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 118 offsetInBlock: 599552 lastPacketInBlock: false lastByteOffsetInBlock: 610985
17/03/31 10:15:17 DEBUG DFSClient: DFSClient seqno: 118 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 70:==============>                                           (2 + 3) / 8][Stage 70:=====================>                                    (3 + 3) / 8]17/03/31 10:15:18 WARN TaskSetManager: Lost task 6.0 in stage 71.3 (TID 458, 172.21.15.173, executor 5): FetchFailed(null, shuffleId=5, mapId=-1, reduceId=8, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 5
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
[Stage 70:=====================>                                    (3 + 4) / 8][Stage 70:===========================================>              (6 + 2) / 8][Stage 70:==================================================>       (7 + 1) / 8]17/03/31 10:15:19 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=119, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=610816
17/03/31 10:15:19 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=617160 lastFlushOffset=610985 createNewBlock=false
17/03/31 10:15:19 DEBUG DFSClient: Queued packet 119
17/03/31 10:15:19 DEBUG DFSClient: Waiting for ack for: 119
17/03/31 10:15:19 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 119 offsetInBlock: 610816 lastPacketInBlock: false lastByteOffsetInBlock: 617160
17/03/31 10:15:19 DEBUG DFSClient: DFSClient seqno: 119 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 71:>                                                         (0 + 4) / 6]17/03/31 10:15:28 DEBUG Client: The ping interval is 60000 ms.
17/03/31 10:15:28 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 10:15:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 10:15:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #22
17/03/31 10:15:28 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #22
17/03/31 10:15:28 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/31 10:15:28 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1146653015_1
17/03/31 10:15:28 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1146653015_1] with renew id 1 executed
17/03/31 10:15:38 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 10:15:38 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 71:=========>                                                (1 + 4) / 6][Stage 71:===================>                                      (2 + 4) / 6][Stage 71:=============================>                            (3 + 3) / 6][Stage 71:======================================>                   (4 + 2) / 6]17/03/31 10:15:58 DEBUG Client: The ping interval is 60000 ms.
17/03/31 10:15:58 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 10:15:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 10:15:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #23
17/03/31 10:15:58 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #23
17/03/31 10:15:58 DEBUG ProtobufRpcEngine: Call: renewLease took 5ms
17/03/31 10:15:58 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_1146653015_1
17/03/31 10:15:58 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1146653015_1] with renew id 1 executed
17/03/31 10:16:08 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 10:16:08 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/31 10:16:08 WARN TaskSetManager: Lost task 2.0 in stage 71.4 (TID 515, 172.21.15.173, executor 5): java.lang.OutOfMemoryError: GC overhead limit exceeded
	at org.apache.spark.graphx.lib.SVDPlusPlus$$anonfun$run$1$$anonfun$13.apply(SVDPlusPlus.scala:158)
	at org.apache.spark.graphx.lib.SVDPlusPlus$$anonfun$run$1$$anonfun$13.apply(SVDPlusPlus.scala:154)
	at org.apache.spark.graphx.impl.AggregatingEdgeContext.send(EdgePartition.scala:536)
	at org.apache.spark.graphx.impl.AggregatingEdgeContext.sendToDst(EdgePartition.scala:531)
	at org.apache.spark.graphx.lib.SVDPlusPlus$.org$apache$spark$graphx$lib$SVDPlusPlus$$sendMsgTrainF$1(SVDPlusPlus.scala:122)
	at org.apache.spark.graphx.lib.SVDPlusPlus$$anonfun$run$1$$anonfun$12.apply(SVDPlusPlus.scala:153)
	at org.apache.spark.graphx.lib.SVDPlusPlus$$anonfun$run$1$$anonfun$12.apply(SVDPlusPlus.scala:153)
	at org.apache.spark.graphx.impl.EdgePartition.aggregateMessagesEdgeScan(EdgePartition.scala:409)
	at org.apache.spark.graphx.impl.GraphImpl$$anonfun$13$$anonfun$apply$3.apply(GraphImpl.scala:237)
	at org.apache.spark.graphx.impl.GraphImpl$$anonfun$13$$anonfun$apply$3.apply(GraphImpl.scala:207)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/03/31 10:16:09 ERROR TaskSchedulerImpl: Lost executor 5 on 172.21.15.173: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/31 10:16:09 WARN TaskSetManager: Lost task 3.0 in stage 71.4 (TID 517, 172.21.15.173, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/31 10:16:09 WARN TaskSetManager: Lost task 2.1 in stage 71.4 (TID 520, 172.21.15.173, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/31 10:16:09 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=120, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=616960
17/03/31 10:16:09 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=623458 lastFlushOffset=617160 createNewBlock=false
17/03/31 10:16:09 DEBUG DFSClient: Queued packet 120
17/03/31 10:16:09 DEBUG DFSClient: Waiting for ack for: 120
17/03/31 10:16:09 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 120 offsetInBlock: 616960 lastPacketInBlock: false lastByteOffsetInBlock: 623458
17/03/31 10:16:09 WARN DFSClient: Slow ReadProcessor read fields took 50041ms (threshold=30000ms); ack: seqno: 120 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
17/03/31 10:16:09 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=121, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=623104
17/03/31 10:16:09 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=623458 lastFlushOffset=623458 createNewBlock=false
17/03/31 10:16:09 DEBUG DFSClient: Waiting for ack for: 120
17/03/31 10:16:09 WARN TaskSetManager: Lost task 2.2 in stage 71.4 (TID 521, 172.21.15.173, executor 6): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=2, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/31 10:16:09 WARN TaskSetManager: Lost task 3.1 in stage 71.4 (TID 522, 172.21.15.173, executor 6): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=5, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/31 10:16:09 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=122, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=623104
17/03/31 10:16:09 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=629819 lastFlushOffset=623458 createNewBlock=false
17/03/31 10:16:09 DEBUG DFSClient: Queued packet 122
17/03/31 10:16:09 DEBUG DFSClient: Waiting for ack for: 122
17/03/31 10:16:09 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 122 offsetInBlock: 623104 lastPacketInBlock: false lastByteOffsetInBlock: 629819
17/03/31 10:16:09 DEBUG DFSClient: DFSClient seqno: 122 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 71 (mapPartitions at GraphImpl.scala:207) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697) 	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693) 	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338) 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285) 	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338) 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54) 	at org.apache.spark.scheduler.Task.run(Task.scala:114) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1456)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1444)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1443)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1443)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1273)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1668)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1626)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1615)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2016)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2113)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1027)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:364)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1009)
	at org.apache.spark.graphx.impl.EdgeRDDImpl.count(EdgeRDDImpl.scala:90)
	at org.apache.spark.graphx.lib.SVDPlusPlus$.org$apache$spark$graphx$lib$SVDPlusPlus$$materialize(SVDPlusPlus.scala:210)
	at org.apache.spark.graphx.lib.SVDPlusPlus$.run(SVDPlusPlus.scala:196)
	at src.main.scala.SVDPlusPlusApp$.main(SVDPlusPlusApp.scala:102)
	at src.main.scala.SVDPlusPlusApp.main(SVDPlusPlusApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:728)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
17/03/31 10:16:09 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=123, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=629760
17/03/31 10:16:09 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=629819 lastFlushOffset=629819 createNewBlock=false
17/03/31 10:16:09 DEBUG DFSClient: Waiting for ack for: 122
17/03/31 10:16:09 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=124, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=629760
17/03/31 10:16:09 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=629819 lastFlushOffset=629819 createNewBlock=false
17/03/31 10:16:09 DEBUG DFSClient: Waiting for ack for: 122
17/03/31 10:16:09 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=125, src=/eventLogs/app-20170331100757-0002.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=629760
17/03/31 10:16:09 DEBUG DFSClient: Queued packet 125
17/03/31 10:16:09 DEBUG DFSClient: Queued packet 126
17/03/31 10:16:09 DEBUG DFSClient: Waiting for ack for: 126
17/03/31 10:16:09 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 125 offsetInBlock: 629760 lastPacketInBlock: false lastByteOffsetInBlock: 634336
17/03/31 10:16:09 DEBUG DFSClient: DFSClient seqno: 125 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:16:09 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111 sending packet packet seqno: 126 offsetInBlock: 634336 lastPacketInBlock: true lastByteOffsetInBlock: 634336
17/03/31 10:16:09 DEBUG DFSClient: DFSClient seqno: 126 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/31 10:16:09 DEBUG DFSClient: Closing old block BP-519507147-172.21.15.90-1479901973323:blk_1073811933_71111
17/03/31 10:16:09 DEBUG Client: The ping interval is 60000 ms.
17/03/31 10:16:09 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/31 10:16:09 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/31 10:16:09 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #24
17/03/31 10:16:09 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #24
17/03/31 10:16:09 DEBUG ProtobufRpcEngine: Call: complete took 12ms
17/03/31 10:16:09 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #25
17/03/31 10:16:09 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #25
17/03/31 10:16:09 DEBUG ProtobufRpcEngine: Call: getFileInfo took 1ms
17/03/31 10:16:09 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #26
17/03/31 10:16:09 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #26
17/03/31 10:16:09 DEBUG ProtobufRpcEngine: Call: rename took 11ms
17/03/31 10:16:09 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop sending #27
17/03/31 10:16:09 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop got value #27
17/03/31 10:16:09 DEBUG ProtobufRpcEngine: Call: setTimes took 10ms
17/03/31 10:16:09 DEBUG PoolThreadCache: Freed 31 thread-local buffer(s) from thread: shuffle-server-6-2
17/03/31 10:16:09 DEBUG PoolThreadCache: Freed 32 thread-local buffer(s) from thread: rpc-server-3-4
17/03/31 10:16:09 DEBUG Client: stopping client from cache: org.apache.hadoop.ipc.Client@1a6899fc
17/03/31 10:16:09 DEBUG Client: removing client from cache: org.apache.hadoop.ipc.Client@1a6899fc
17/03/31 10:16:09 DEBUG Client: stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@1a6899fc
17/03/31 10:16:09 DEBUG Client: Stopping client
17/03/31 10:16:09 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/31 10:16:09 DEBUG Client: IPC Client (1407420403) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
