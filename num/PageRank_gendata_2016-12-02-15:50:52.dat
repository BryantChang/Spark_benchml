Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/12/02 15:50:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/12/02 15:50:53 WARN SparkConf: Detected deprecated memory fraction settings: [spark.storage.memoryFraction]. As of Spark 1.6, execution and storage memory management are unified. All memory fractions used in the old model are now deprecated and no longer read. If you wish to use the old memory management, you may explicitly enable `spark.memory.useLegacyMode` (not recommended).
16/12/02 15:50:54 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
16/12/02 15:50:54 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
16/12/02 15:50:54 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
16/12/02 15:50:54 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
16/12/02 15:50:54 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
16/12/02 15:50:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
16/12/02 15:51:10 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[Stage 0:>                                                         (0 + 0) / 10]16/12/02 15:51:25 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
16/12/02 15:51:40 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
16/12/02 15:51:55 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
16/12/02 15:52:11 WARN DFSClient: Slow ReadProcessor read fields took 52419ms (threshold=30000ms); ack: seqno: 3 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
[Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10]16/12/02 15:55:53 WARN HeartbeatReceiver: Removing executor 0 with no recent heartbeats: 147835 ms exceeds timeout 120000 ms
16/12/02 15:55:53 ERROR TaskSchedulerImpl: Lost executor 0 on 172.21.15.173: Executor heartbeat timed out after 147835 ms
16/12/02 15:55:53 WARN TaskSetManager: Lost task 2.0 in stage 0.0 (TID 2, 172.21.15.173): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 147835 ms
16/12/02 15:55:53 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, 172.21.15.173): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 147835 ms
16/12/02 15:55:53 WARN TaskSetManager: Lost task 3.0 in stage 0.0 (TID 3, 172.21.15.173): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 147835 ms
16/12/02 15:55:53 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, 172.21.15.173): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 147835 ms
[Stage 0:>                                                         (0 + 0) / 10]16/12/02 15:55:54 WARN DFSClient: Slow ReadProcessor read fields took 71281ms (threshold=30000ms); ack: seqno: 5 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
[Stage 0:>                                                         (0 + 0) / 10][Stage 0:>                                                         (0 + 0) / 10][Stage 0:>                                                         (0 + 0) / 10][Stage 0:>                                                         (0 + 0) / 10][Stage 0:>                                                         (0 + 0) / 10][Stage 0:>                                                         (0 + 0) / 10]16/12/02 16:01:59 WARN DFSClient: Slow ReadProcessor read fields took 65116ms (threshold=30000ms); ack: seqno: 6 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
[Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10]16/12/02 16:03:06 WARN DFSClient: Slow ReadProcessor read fields took 49191ms (threshold=30000ms); ack: seqno: 9 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
16/12/02 16:03:12 ERROR TaskSchedulerImpl: Lost executor 0 on 172.21.15.173: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
[Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10]16/12/02 16:07:53 WARN HeartbeatReceiver: Removing executor 1 with no recent heartbeats: 177361 ms exceeds timeout 120000 ms
16/12/02 16:07:53 ERROR TaskSchedulerImpl: Lost executor 1 on 172.21.15.173: Executor heartbeat timed out after 177361 ms
16/12/02 16:07:53 WARN TaskSetManager: Lost task 3.1 in stage 0.0 (TID 5, 172.21.15.173): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 177361 ms
16/12/02 16:07:53 WARN TaskSetManager: Lost task 0.1 in stage 0.0 (TID 4, 172.21.15.173): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 177361 ms
16/12/02 16:07:53 WARN TaskSetManager: Lost task 2.1 in stage 0.0 (TID 7, 172.21.15.173): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 177361 ms
16/12/02 16:07:53 WARN TaskSetManager: Lost task 1.1 in stage 0.0 (TID 6, 172.21.15.173): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 177361 ms
[Stage 0:>                                                         (0 + 0) / 10]16/12/02 16:07:57 WARN DFSClient: Slow ReadProcessor read fields took 132851ms (threshold=30000ms); ack: seqno: 12 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
[Stage 0:>                                                         (0 + 0) / 10][Stage 0:>                                                         (0 + 0) / 10][Stage 0:>                                                         (0 + 0) / 10][Stage 0:>                                                         (0 + 0) / 10][Stage 0:>                                                         (0 + 0) / 10][Stage 0:>                                                         (0 + 0) / 10][Stage 0:>                                                         (0 + 0) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10]16/12/02 16:26:53 WARN HeartbeatReceiver: Removing executor 2 with no recent heartbeats: 173172 ms exceeds timeout 120000 ms
16/12/02 16:26:53 ERROR TaskSchedulerImpl: Lost executor 2 on 172.21.15.173: Executor heartbeat timed out after 173172 ms
16/12/02 16:26:53 WARN TaskSetManager: Lost task 1.2 in stage 0.0 (TID 8, 172.21.15.173): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 173172 ms
16/12/02 16:26:53 WARN TaskSetManager: Lost task 3.2 in stage 0.0 (TID 11, 172.21.15.173): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 173172 ms
16/12/02 16:26:53 WARN TaskSetManager: Lost task 0.2 in stage 0.0 (TID 10, 172.21.15.173): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 173172 ms
16/12/02 16:26:53 WARN TaskSetManager: Lost task 2.2 in stage 0.0 (TID 9, 172.21.15.173): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 173172 ms
[Stage 0:>                                                         (0 + 0) / 10]16/12/02 16:26:54 WARN DFSClient: Slow ReadProcessor read fields took 64961ms (threshold=30000ms); ack: seqno: 15 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
[Stage 0:>                                                         (0 + 0) / 10][Stage 0:>                                                         (0 + 0) / 10][Stage 0:>                                                         (0 + 0) / 10][Stage 0:>                                                         (0 + 0) / 10][Stage 0:>                                                         (0 + 0) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 4) / 10]16/12/02 16:37:28 ERROR TaskSchedulerImpl: Lost an executor 1 (already removed): Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
16/12/02 16:37:28 WARN DFSClient: Slow ReadProcessor read fields took 146015ms (threshold=30000ms); ack: seqno: 18 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
16/12/02 16:37:53 WARN HeartbeatReceiver: Removing executor 3 with no recent heartbeats: 163958 ms exceeds timeout 120000 ms
16/12/02 16:37:53 ERROR TaskSchedulerImpl: Lost executor 3 on 172.21.15.173: Executor heartbeat timed out after 163958 ms
16/12/02 16:37:53 WARN TaskSetManager: Lost task 3.3 in stage 0.0 (TID 14, 172.21.15.173): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 163958 ms
16/12/02 16:37:53 ERROR TaskSetManager: Task 3 in stage 0.0 failed 4 times; aborting job
16/12/02 16:37:53 WARN TaskSetManager: Lost task 0.3 in stage 0.0 (TID 13, 172.21.15.173): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 163958 ms
16/12/02 16:37:53 WARN TaskSetManager: Lost task 2.3 in stage 0.0 (TID 12, 172.21.15.173): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 163958 ms
16/12/02 16:37:53 WARN TaskSetManager: Lost task 1.3 in stage 0.0 (TID 15, 172.21.15.173): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 163958 ms
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 0.0 failed 4 times, most recent failure: Lost task 3.3 in stage 0.0 (TID 14, 172.21.15.173): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 163958 ms
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1219)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1161)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1064)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1030)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1030)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1030)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:956)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:956)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:956)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:955)
	at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1459)
	at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1438)
	at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1438)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
	at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1438)
	at src.main.scala.pageRankDataGen$.main(pageRankDataGen.scala:53)
	at src.main.scala.pageRankDataGen.main(pageRankDataGen.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
