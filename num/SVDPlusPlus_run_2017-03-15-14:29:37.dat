17/03/15 14:29:38 WARN SparkContext: Support for Java 7 is deprecated as of Spark 2.0.0
17/03/15 14:29:38 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time, about=, type=DEFAULT, always=false, sampleName=Ops)
17/03/15 14:29:38 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time, about=, type=DEFAULT, always=false, sampleName=Ops)
17/03/15 14:29:38 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(value=[GetGroups], valueName=Time, about=, type=DEFAULT, always=false, sampleName=Ops)
17/03/15 14:29:38 DEBUG MetricsSystemImpl: UgiMetrics, User and group related metrics
17/03/15 14:29:38 DEBUG Shell: setsid exited with exit code 0
17/03/15 14:29:38 DEBUG Groups:  Creating new Groups object
17/03/15 14:29:38 DEBUG NativeCodeLoader: Trying to load the custom-built native-hadoop library...
17/03/15 14:29:38 DEBUG NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
17/03/15 14:29:38 DEBUG NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
17/03/15 14:29:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/03/15 14:29:38 DEBUG PerformanceAdvisory: Falling back to shell based
17/03/15 14:29:38 DEBUG JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
17/03/15 14:29:38 DEBUG Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
17/03/15 14:29:38 DEBUG UserGroupInformation: hadoop login
17/03/15 14:29:38 DEBUG UserGroupInformation: hadoop login commit
17/03/15 14:29:38 DEBUG UserGroupInformation: using local user:UnixPrincipal: hadoop
17/03/15 14:29:38 DEBUG UserGroupInformation: Using user: "UnixPrincipal: hadoop" with name hadoop
17/03/15 14:29:38 DEBUG UserGroupInformation: User entry: "hadoop"
17/03/15 14:29:38 DEBUG UserGroupInformation: UGI loginUser:hadoop (auth:SIMPLE)
17/03/15 14:29:38 WARN SparkConf: Detected deprecated memory fraction settings: [spark.storage.memoryFraction]. As of Spark 1.6, execution and storage memory management are unified. All memory fractions used in the old model are now deprecated and no longer read. If you wish to use the old memory management, you may explicitly enable `spark.memory.useLegacyMode` (not recommended).
17/03/15 14:29:38 WARN SparkConf: 
SPARK_CLASSPATH was detected (set to '/home/hadoop/bryantchang/platforms/spark2.0/lib/mysql-connector-java-5.1.40-bin.jar:').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with --driver-class-path to augment the driver classpath
 - spark.executor.extraClassPath to augment the executor classpath
        
17/03/15 14:29:38 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/home/hadoop/bryantchang/platforms/spark2.0/lib/mysql-connector-java-5.1.40-bin.jar:' as a work-around.
17/03/15 14:29:38 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/home/hadoop/bryantchang/platforms/spark2.0/lib/mysql-connector-java-5.1.40-bin.jar:' as a work-around.
17/03/15 14:29:38 DEBUG InternalLoggerFactory: Using SLF4J as the default logging framework
17/03/15 14:29:38 DEBUG PlatformDependent0: java.nio.Buffer.address: available
17/03/15 14:29:38 DEBUG PlatformDependent0: sun.misc.Unsafe.theUnsafe: available
17/03/15 14:29:38 DEBUG PlatformDependent0: sun.misc.Unsafe.copyMemory: available
17/03/15 14:29:38 DEBUG PlatformDependent0: direct buffer constructor: available
17/03/15 14:29:38 DEBUG PlatformDependent0: java.nio.Bits.unaligned: available, true
17/03/15 14:29:38 DEBUG PlatformDependent0: java.nio.DirectByteBuffer.<init>(long, int): available
17/03/15 14:29:38 DEBUG Cleaner0: java.nio.ByteBuffer.cleaner(): available
17/03/15 14:29:38 DEBUG PlatformDependent: Java version: 7
17/03/15 14:29:38 DEBUG PlatformDependent: -Dio.netty.noUnsafe: false
17/03/15 14:29:38 DEBUG PlatformDependent: sun.misc.Unsafe: available
17/03/15 14:29:38 DEBUG PlatformDependent: -Dio.netty.noJavassist: false
17/03/15 14:29:38 DEBUG PlatformDependent: Javassist: available
17/03/15 14:29:38 DEBUG PlatformDependent: -Dio.netty.tmpdir: /tmp (java.io.tmpdir)
17/03/15 14:29:38 DEBUG PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)
17/03/15 14:29:38 DEBUG PlatformDependent: -Dio.netty.noPreferDirect: false
17/03/15 14:29:38 DEBUG PlatformDependent: io.netty.maxDirectMemory: 954728448 bytes
17/03/15 14:29:38 DEBUG JavassistTypeParameterMatcherGenerator: Generated: io.netty.util.internal.__matchers__.org.apache.spark.network.protocol.MessageMatcher
17/03/15 14:29:38 DEBUG JavassistTypeParameterMatcherGenerator: Generated: io.netty.util.internal.__matchers__.io.netty.buffer.ByteBufMatcher
17/03/15 14:29:38 DEBUG MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 8
17/03/15 14:29:38 DEBUG NioEventLoop: -Dio.netty.noKeySetOptimization: false
17/03/15 14:29:38 DEBUG NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512
17/03/15 14:29:38 DEBUG PlatformDependent: org.jctools-core.MpscChunkedArrayQueue: available
17/03/15 14:29:38 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 8
17/03/15 14:29:38 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 8
17/03/15 14:29:38 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192
17/03/15 14:29:38 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 11
17/03/15 14:29:38 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 16777216
17/03/15 14:29:38 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.tinyCacheSize: 512
17/03/15 14:29:38 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256
17/03/15 14:29:38 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64
17/03/15 14:29:38 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768
17/03/15 14:29:38 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192
17/03/15 14:29:38 DEBUG ThreadLocalRandom: -Dio.netty.initialSeedUniquifier: 0x1f60b85a471f4618 (took 0 ms)
17/03/15 14:29:38 DEBUG ByteBufUtil: -Dio.netty.allocator.type: unpooled
17/03/15 14:29:38 DEBUG ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 65536
17/03/15 14:29:38 DEBUG ByteBufUtil: -Dio.netty.maxThreadLocalCharBufferSize: 16384
17/03/15 14:29:38 DEBUG NetUtil: Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%1)
17/03/15 14:29:38 DEBUG NetUtil: /proc/sys/net/core/somaxconn: 128
17/03/15 14:29:39 DEBUG AbstractByteBuf: -Dio.netty.buffer.bytebuf.checkAccessible: true
17/03/15 14:29:39 DEBUG ResourceLeakDetector: -Dio.netty.leakDetection.level: simple
17/03/15 14:29:39 DEBUG ResourceLeakDetector: -Dio.netty.leakDetection.maxRecords: 4
17/03/15 14:29:39 DEBUG ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: io.netty.util.ResourceLeakDetector@4579480f
17/03/15 14:29:39 DEBUG Recycler: -Dio.netty.recycler.maxCapacity.default: 32768
17/03/15 14:29:39 DEBUG Recycler: -Dio.netty.recycler.maxSharedCapacityFactor: 2
17/03/15 14:29:39 DEBUG Recycler: -Dio.netty.recycler.linkCapacity: 16
17/03/15 14:29:39 DEBUG Recycler: -Dio.netty.recycler.ratio: 8
17/03/15 14:29:39 DEBUG BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false
17/03/15 14:29:39 DEBUG BlockReaderLocal: dfs.client.read.shortcircuit = false
17/03/15 14:29:39 DEBUG BlockReaderLocal: dfs.client.domain.socket.data.traffic = false
17/03/15 14:29:39 DEBUG BlockReaderLocal: dfs.domain.socket.path = 
17/03/15 14:29:39 DEBUG RetryUtils: multipleLinearRandomRetry = null
17/03/15 14:29:39 DEBUG Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcRequestWrapper, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@1710791a
17/03/15 14:29:39 DEBUG Client: getting client out of cache: org.apache.hadoop.ipc.Client@1ca04be
17/03/15 14:29:39 DEBUG PerformanceAdvisory: Both short-circuit local reads and UNIX domain socket are disabled.
17/03/15 14:29:39 DEBUG DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
17/03/15 14:29:39 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:29:39 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:29:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:29:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #0
17/03/15 14:29:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #0
17/03/15 14:29:39 DEBUG ProtobufRpcEngine: Call: getFileInfo took 25ms
17/03/15 14:29:39 DEBUG DFSClient: /eventLogs/app-20170315142939-0020.lz4.inprogress: masked=rw-r--r--
17/03/15 14:29:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #1
17/03/15 14:29:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #1
17/03/15 14:29:39 DEBUG ProtobufRpcEngine: Call: create took 20ms
17/03/15 14:29:39 DEBUG DFSClient: computePacketChunkSize: src=/eventLogs/app-20170315142939-0020.lz4.inprogress, chunkSize=516, chunksPerPacket=126, packetSize=65016
17/03/15 14:29:39 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 started
17/03/15 14:29:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #2
17/03/15 14:29:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #2
17/03/15 14:29:39 DEBUG ProtobufRpcEngine: Call: setPermission took 4ms
17/03/15 14:29:39 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=0 lastFlushOffset=0 createNewBlock=false
17/03/15 14:29:39 DEBUG DFSClient: Waiting for ack for: -1
17/03/15 14:29:39 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=0 lastFlushOffset=0 createNewBlock=false
17/03/15 14:29:39 DEBUG DFSClient: Waiting for ack for: -1
17/03/15 14:29:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #3
17/03/15 14:29:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #3
17/03/15 14:29:40 DEBUG ProtobufRpcEngine: Call: getFileInfo took 1ms
17/03/15 14:29:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #4
17/03/15 14:29:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #4
17/03/15 14:29:40 DEBUG ProtobufRpcEngine: Call: getListing took 2ms
17/03/15 14:29:40 DEBUG FileInputFormat: Time taken to get FileStatuses: 23
17/03/15 14:29:40 INFO FileInputFormat: Total input paths to process : 10
17/03/15 14:29:40 DEBUG FileInputFormat: Total # of splits generated by getSplits: 12, TimeTaken: 27
17/03/15 14:29:40 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=0 lastFlushOffset=0 createNewBlock=false
17/03/15 14:29:40 DEBUG DFSClient: Waiting for ack for: -1
17/03/15 14:29:40 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=0, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0
17/03/15 14:29:40 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=6313 lastFlushOffset=0 createNewBlock=false
17/03/15 14:29:40 DEBUG DFSClient: Queued packet 0
17/03/15 14:29:40 DEBUG DFSClient: Waiting for ack for: 0
17/03/15 14:29:40 DEBUG DFSClient: Allocating new block
17/03/15 14:29:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #5
17/03/15 14:29:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #5
17/03/15 14:29:40 DEBUG ProtobufRpcEngine: Call: addBlock took 11ms
17/03/15 14:29:40 DEBUG DFSClient: pipeline = DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]
17/03/15 14:29:40 DEBUG DFSClient: Connecting to datanode 172.21.15.173:50010
17/03/15 14:29:40 DEBUG DFSClient: Send buf size 124928
17/03/15 14:29:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #6
17/03/15 14:29:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #6
17/03/15 14:29:40 DEBUG ProtobufRpcEngine: Call: getServerDefaults took 0ms
17/03/15 14:29:40 DEBUG SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = /172.21.15.173, datanodeId = DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]
17/03/15 14:29:40 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 6313
17/03/15 14:29:40 DEBUG DFSClient: DFSClient seqno: 0 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:29:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #7
17/03/15 14:29:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #7
17/03/15 14:29:40 DEBUG ProtobufRpcEngine: Call: fsync took 6ms
17/03/15 14:29:40 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=1, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=6144
17/03/15 14:29:40 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=6313 lastFlushOffset=6313 createNewBlock=false
17/03/15 14:29:40 DEBUG DFSClient: Waiting for ack for: 0
[Stage 0:>                                                         (0 + 4) / 10][Stage 0:>                                                         (0 + 7) / 10][Stage 0:=================>                                        (3 + 4) / 10][Stage 0:=======================>                                  (4 + 4) / 10][Stage 0:=============================>                            (5 + 4) / 10][Stage 0:==================================>                       (6 + 4) / 10][Stage 0:========================================>                 (7 + 3) / 10][Stage 0:==============================================>           (8 + 2) / 10][Stage 0:====================================================>     (9 + 1) / 10]                                                                                17/03/15 14:29:50 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=2, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=6144
17/03/15 14:29:50 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=10421 lastFlushOffset=6313 createNewBlock=false
17/03/15 14:29:50 DEBUG DFSClient: Queued packet 2
17/03/15 14:29:50 DEBUG DFSClient: Waiting for ack for: 2
17/03/15 14:29:50 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 2 offsetInBlock: 6144 lastPacketInBlock: false lastByteOffsetInBlock: 10421
17/03/15 14:29:50 DEBUG DFSClient: DFSClient seqno: 2 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:29:50 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=3, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=10240
17/03/15 14:29:50 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=10421 lastFlushOffset=10421 createNewBlock=false
17/03/15 14:29:50 DEBUG DFSClient: Waiting for ack for: 2
17/03/15 14:29:50 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=4, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=10240
17/03/15 14:29:50 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=10421 lastFlushOffset=10421 createNewBlock=false
17/03/15 14:29:50 DEBUG DFSClient: Waiting for ack for: 2
17/03/15 14:29:50 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:29:50 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 1:>                                                         (0 + 4) / 10][Stage 1:=================>                                        (3 + 4) / 10][Stage 1:=======================>                                  (4 + 4) / 10][Stage 1:==============================================>           (8 + 2) / 10][Stage 1:====================================================>     (9 + 1) / 10]17/03/15 14:29:52 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=5, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=10240
17/03/15 14:29:52 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=14886 lastFlushOffset=10421 createNewBlock=false
17/03/15 14:29:52 DEBUG DFSClient: Queued packet 5
17/03/15 14:29:52 DEBUG DFSClient: Waiting for ack for: 5
17/03/15 14:29:52 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 5 offsetInBlock: 10240 lastPacketInBlock: false lastByteOffsetInBlock: 14886
17/03/15 14:29:52 DEBUG DFSClient: DFSClient seqno: 5 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 2:>                                                         (0 + 4) / 10][Stage 2:=======================>                                  (4 + 4) / 10][Stage 2:==============================================>           (8 + 2) / 10]                                                                                17/03/15 14:29:59 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=6, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=14848
17/03/15 14:29:59 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=27276 lastFlushOffset=14886 createNewBlock=false
17/03/15 14:29:59 DEBUG DFSClient: Queued packet 6
17/03/15 14:29:59 DEBUG DFSClient: Waiting for ack for: 6
17/03/15 14:29:59 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 6 offsetInBlock: 14848 lastPacketInBlock: false lastByteOffsetInBlock: 27276
17/03/15 14:29:59 DEBUG DFSClient: DFSClient seqno: 6 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:29:59 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=7, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=27136
17/03/15 14:29:59 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=27276 lastFlushOffset=27276 createNewBlock=false
17/03/15 14:29:59 DEBUG DFSClient: Waiting for ack for: 6
17/03/15 14:29:59 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=8, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=27136
17/03/15 14:29:59 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=27276 lastFlushOffset=27276 createNewBlock=false
17/03/15 14:29:59 DEBUG DFSClient: Waiting for ack for: 6
[Stage 4:>                                                         (0 + 4) / 10][Stage 4:=======================>                                  (4 + 4) / 10]17/03/15 14:30:09 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:30:09 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:30:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:30:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #8
17/03/15 14:30:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #8
17/03/15 14:30:09 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 14:30:09 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:30:09 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
[Stage 4:========================================>                 (7 + 3) / 10][Stage 4:==============================================>           (8 + 2) / 10]17/03/15 14:30:11 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=9, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=27136
17/03/15 14:30:11 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=40356 lastFlushOffset=27276 createNewBlock=false
17/03/15 14:30:11 DEBUG DFSClient: Queued packet 9
17/03/15 14:30:11 DEBUG DFSClient: Waiting for ack for: 9
17/03/15 14:30:11 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 9 offsetInBlock: 27136 lastPacketInBlock: false lastByteOffsetInBlock: 40356
17/03/15 14:30:11 DEBUG DFSClient: DFSClient seqno: 9 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
                                                                                17/03/15 14:30:11 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=10, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=39936
17/03/15 14:30:11 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=44759 lastFlushOffset=40356 createNewBlock=false
17/03/15 14:30:11 DEBUG DFSClient: Queued packet 10
17/03/15 14:30:11 DEBUG DFSClient: Waiting for ack for: 10
17/03/15 14:30:11 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 10 offsetInBlock: 39936 lastPacketInBlock: false lastByteOffsetInBlock: 44759
17/03/15 14:30:11 DEBUG DFSClient: DFSClient seqno: 10 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:30:11 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=11, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=44544
17/03/15 14:30:11 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=44759 lastFlushOffset=44759 createNewBlock=false
17/03/15 14:30:11 DEBUG DFSClient: Waiting for ack for: 10
17/03/15 14:30:11 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=12, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=44544
17/03/15 14:30:11 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=49082 lastFlushOffset=44759 createNewBlock=false
17/03/15 14:30:11 DEBUG DFSClient: Queued packet 12
17/03/15 14:30:11 DEBUG DFSClient: Waiting for ack for: 12
17/03/15 14:30:11 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 12 offsetInBlock: 44544 lastPacketInBlock: false lastByteOffsetInBlock: 49082
17/03/15 14:30:11 DEBUG DFSClient: DFSClient seqno: 12 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 7:>                                                         (0 + 4) / 10][Stage 7:===========>                                              (2 + 4) / 10][Stage 7:=======================>                                  (4 + 4) / 10]17/03/15 14:30:19 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:30:19 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 7:=============================>                            (5 + 4) / 10][Stage 7:========================================>                 (7 + 3) / 10][Stage 7:==============================================>           (8 + 2) / 10]                                                                                17/03/15 14:30:23 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=13, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=48640
17/03/15 14:30:23 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=55828 lastFlushOffset=49082 createNewBlock=false
17/03/15 14:30:23 DEBUG DFSClient: Queued packet 13
17/03/15 14:30:23 DEBUG DFSClient: Waiting for ack for: 13
17/03/15 14:30:23 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 13 offsetInBlock: 48640 lastPacketInBlock: false lastByteOffsetInBlock: 55828
17/03/15 14:30:23 DEBUG DFSClient: DFSClient seqno: 13 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:30:23 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=14, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=55808
17/03/15 14:30:23 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=55828 lastFlushOffset=55828 createNewBlock=false
17/03/15 14:30:23 DEBUG DFSClient: Waiting for ack for: 13
17/03/15 14:30:23 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=15, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=55808
17/03/15 14:30:23 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=55828 lastFlushOffset=55828 createNewBlock=false
17/03/15 14:30:23 DEBUG DFSClient: Waiting for ack for: 13
17/03/15 14:30:23 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=16, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=55808
17/03/15 14:30:23 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=59618 lastFlushOffset=55828 createNewBlock=false
17/03/15 14:30:23 DEBUG DFSClient: Queued packet 16
17/03/15 14:30:23 DEBUG DFSClient: Waiting for ack for: 16
17/03/15 14:30:23 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 16 offsetInBlock: 55808 lastPacketInBlock: false lastByteOffsetInBlock: 59618
17/03/15 14:30:23 DEBUG DFSClient: DFSClient seqno: 16 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:30:23 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=17, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=59392
17/03/15 14:30:23 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=67315 lastFlushOffset=59618 createNewBlock=false
17/03/15 14:30:23 DEBUG DFSClient: Queued packet 17
17/03/15 14:30:23 DEBUG DFSClient: Waiting for ack for: 17
17/03/15 14:30:23 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 17 offsetInBlock: 59392 lastPacketInBlock: false lastByteOffsetInBlock: 67315
17/03/15 14:30:23 DEBUG DFSClient: DFSClient seqno: 17 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 11:>                                                        (0 + 4) / 10][Stage 11:=================>                                       (3 + 4) / 10][Stage 11:=======================================>                 (7 + 3) / 10]17/03/15 14:30:25 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=18, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=67072
17/03/15 14:30:25 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=72819 lastFlushOffset=67315 createNewBlock=false
17/03/15 14:30:25 DEBUG DFSClient: Queued packet 18
17/03/15 14:30:25 DEBUG DFSClient: Waiting for ack for: 18
17/03/15 14:30:25 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 18 offsetInBlock: 67072 lastPacketInBlock: false lastByteOffsetInBlock: 72819
17/03/15 14:30:25 DEBUG DFSClient: DFSClient seqno: 18 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
                                                                                17/03/15 14:30:25 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=19, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=72704
17/03/15 14:30:25 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=81975 lastFlushOffset=72819 createNewBlock=false
17/03/15 14:30:25 DEBUG DFSClient: Queued packet 19
17/03/15 14:30:25 DEBUG DFSClient: Waiting for ack for: 19
17/03/15 14:30:25 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 19 offsetInBlock: 72704 lastPacketInBlock: false lastByteOffsetInBlock: 81975
17/03/15 14:30:25 DEBUG DFSClient: DFSClient seqno: 19 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:30:25 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=20, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=81920
17/03/15 14:30:25 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=81975 lastFlushOffset=81975 createNewBlock=false
17/03/15 14:30:25 DEBUG DFSClient: Waiting for ack for: 19
17/03/15 14:30:25 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=21, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=81920
17/03/15 14:30:25 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=86700 lastFlushOffset=81975 createNewBlock=false
17/03/15 14:30:25 DEBUG DFSClient: Queued packet 21
17/03/15 14:30:25 DEBUG DFSClient: Waiting for ack for: 21
17/03/15 14:30:25 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 21 offsetInBlock: 81920 lastPacketInBlock: false lastByteOffsetInBlock: 86700
17/03/15 14:30:25 DEBUG DFSClient: DFSClient seqno: 21 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:30:26 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=22, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=86528
17/03/15 14:30:26 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=91939 lastFlushOffset=86700 createNewBlock=false
17/03/15 14:30:26 DEBUG DFSClient: Queued packet 22
17/03/15 14:30:26 DEBUG DFSClient: Waiting for ack for: 22
17/03/15 14:30:26 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 22 offsetInBlock: 86528 lastPacketInBlock: false lastByteOffsetInBlock: 91939
17/03/15 14:30:26 DEBUG DFSClient: DFSClient seqno: 22 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 18:===================================================>     (9 + 1) / 10]                                                                                17/03/15 14:30:27 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=23, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=91648
17/03/15 14:30:27 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=104922 lastFlushOffset=91939 createNewBlock=false
17/03/15 14:30:27 DEBUG DFSClient: Queued packet 23
17/03/15 14:30:27 DEBUG DFSClient: Waiting for ack for: 23
17/03/15 14:30:27 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 23 offsetInBlock: 91648 lastPacketInBlock: false lastByteOffsetInBlock: 104922
17/03/15 14:30:27 DEBUG DFSClient: DFSClient seqno: 23 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:30:27 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=24, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=104448
17/03/15 14:30:27 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=104922 lastFlushOffset=104922 createNewBlock=false
17/03/15 14:30:27 DEBUG DFSClient: Waiting for ack for: 23
17/03/15 14:30:27 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=25, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=104448
17/03/15 14:30:27 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=104922 lastFlushOffset=104922 createNewBlock=false
17/03/15 14:30:27 DEBUG DFSClient: Waiting for ack for: 23
17/03/15 14:30:27 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=26, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=104448
17/03/15 14:30:27 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=104922 lastFlushOffset=104922 createNewBlock=false
17/03/15 14:30:27 DEBUG DFSClient: Waiting for ack for: 23
17/03/15 14:30:27 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=27, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=104448
17/03/15 14:30:27 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=108935 lastFlushOffset=104922 createNewBlock=false
17/03/15 14:30:27 DEBUG DFSClient: Queued packet 27
17/03/15 14:30:27 DEBUG DFSClient: Waiting for ack for: 27
17/03/15 14:30:27 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 27 offsetInBlock: 104448 lastPacketInBlock: false lastByteOffsetInBlock: 108935
17/03/15 14:30:27 DEBUG DFSClient: DFSClient seqno: 27 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 24:>                                                        (0 + 4) / 10][Stage 24:=================>                                       (3 + 4) / 10][Stage 24:======================>                                  (4 + 4) / 10][Stage 24:==================================>                      (6 + 4) / 10][Stage 24:=============================================>           (8 + 2) / 10]17/03/15 14:30:29 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=28, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=108544
17/03/15 14:30:29 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=114548 lastFlushOffset=108935 createNewBlock=false
17/03/15 14:30:29 DEBUG DFSClient: Queued packet 28
17/03/15 14:30:29 DEBUG DFSClient: Waiting for ack for: 28
17/03/15 14:30:29 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 28 offsetInBlock: 108544 lastPacketInBlock: false lastByteOffsetInBlock: 114548
17/03/15 14:30:29 DEBUG DFSClient: DFSClient seqno: 28 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 25:======================>                                  (4 + 4) / 10][Stage 25:=============================================>           (8 + 2) / 10]                                                                                17/03/15 14:30:30 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=29, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=114176
17/03/15 14:30:30 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=123500 lastFlushOffset=114548 createNewBlock=false
17/03/15 14:30:30 DEBUG DFSClient: Queued packet 29
17/03/15 14:30:30 DEBUG DFSClient: Waiting for ack for: 29
17/03/15 14:30:30 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 29 offsetInBlock: 114176 lastPacketInBlock: false lastByteOffsetInBlock: 123500
17/03/15 14:30:30 DEBUG DFSClient: DFSClient seqno: 29 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:30:30 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=30, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=123392
17/03/15 14:30:30 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=123500 lastFlushOffset=123500 createNewBlock=false
17/03/15 14:30:30 DEBUG DFSClient: Waiting for ack for: 29
17/03/15 14:30:30 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=31, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=123392
17/03/15 14:30:30 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=128019 lastFlushOffset=123500 createNewBlock=false
17/03/15 14:30:30 DEBUG DFSClient: Queued packet 31
17/03/15 14:30:30 DEBUG DFSClient: Waiting for ack for: 31
17/03/15 14:30:30 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 31 offsetInBlock: 123392 lastPacketInBlock: false lastByteOffsetInBlock: 128019
17/03/15 14:30:30 DEBUG DFSClient: DFSClient seqno: 31 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:30:31 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=32, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=128000
17/03/15 14:30:31 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=136399 lastFlushOffset=128019 createNewBlock=false
17/03/15 14:30:31 DEBUG DFSClient: Queued packet 32
17/03/15 14:30:31 DEBUG DFSClient: Waiting for ack for: 32
17/03/15 14:30:31 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 32 offsetInBlock: 128000 lastPacketInBlock: false lastByteOffsetInBlock: 136399
17/03/15 14:30:31 DEBUG DFSClient: DFSClient seqno: 32 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 33:>                                                        (0 + 4) / 10][Stage 33:======================>                                  (4 + 4) / 10][Stage 33:=============================================>           (8 + 2) / 10]                                                                                17/03/15 14:30:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=33, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=136192
17/03/15 14:30:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=152401 lastFlushOffset=136399 createNewBlock=false
17/03/15 14:30:35 DEBUG DFSClient: Queued packet 33
17/03/15 14:30:35 DEBUG DFSClient: Waiting for ack for: 33
17/03/15 14:30:35 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 33 offsetInBlock: 136192 lastPacketInBlock: false lastByteOffsetInBlock: 152401
17/03/15 14:30:35 DEBUG DFSClient: DFSClient seqno: 33 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:30:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=34, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=152064
17/03/15 14:30:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=152401 lastFlushOffset=152401 createNewBlock=false
17/03/15 14:30:35 DEBUG DFSClient: Waiting for ack for: 33
17/03/15 14:30:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=35, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=152064
17/03/15 14:30:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=152401 lastFlushOffset=152401 createNewBlock=false
17/03/15 14:30:35 DEBUG DFSClient: Waiting for ack for: 33
17/03/15 14:30:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=36, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=152064
17/03/15 14:30:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=152401 lastFlushOffset=152401 createNewBlock=false
17/03/15 14:30:35 DEBUG DFSClient: Waiting for ack for: 33
17/03/15 14:30:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=37, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=152064
17/03/15 14:30:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=156504 lastFlushOffset=152401 createNewBlock=false
17/03/15 14:30:35 DEBUG DFSClient: Queued packet 37
17/03/15 14:30:35 DEBUG DFSClient: Waiting for ack for: 37
17/03/15 14:30:35 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 37 offsetInBlock: 152064 lastPacketInBlock: false lastByteOffsetInBlock: 156504
17/03/15 14:30:35 DEBUG DFSClient: DFSClient seqno: 37 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 41:>                                                        (0 + 4) / 10]17/03/15 14:30:39 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:30:39 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:30:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:30:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #9
17/03/15 14:30:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #9
17/03/15 14:30:39 DEBUG ProtobufRpcEngine: Call: renewLease took 8ms
17/03/15 14:30:39 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:30:39 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
17/03/15 14:30:49 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:30:49 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/15 14:31:09 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:31:09 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:31:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:31:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #10
17/03/15 14:31:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #10
17/03/15 14:31:09 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 14:31:09 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:31:09 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
17/03/15 14:31:19 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:31:19 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 41:>                                                        (0 + 4) / 10]17/03/15 14:31:39 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:31:39 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:31:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:31:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #11
17/03/15 14:31:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #11
17/03/15 14:31:39 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 14:31:39 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:31:39 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
[Stage 41:=====>                                                   (1 + 4) / 10][Stage 41:=================>                                       (3 + 4) / 10][Stage 41:======================>                                  (4 + 4) / 10]17/03/15 14:31:49 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:31:49 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/15 14:32:09 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:32:09 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:32:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:32:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #12
17/03/15 14:32:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #12
17/03/15 14:32:09 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/15 14:32:09 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:32:09 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
[Stage 41:==================================>                      (6 + 4) / 10][Stage 41:=======================================>                 (7 + 3) / 10][Stage 41:=============================================>           (8 + 2) / 10]17/03/15 14:32:19 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:32:19 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 41:===================================================>     (9 + 1) / 10]17/03/15 14:32:21 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=38, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=156160
17/03/15 14:32:21 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=164915 lastFlushOffset=156504 createNewBlock=false
17/03/15 14:32:21 DEBUG DFSClient: Queued packet 38
17/03/15 14:32:21 DEBUG DFSClient: Waiting for ack for: 38
17/03/15 14:32:21 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 38 offsetInBlock: 156160 lastPacketInBlock: false lastByteOffsetInBlock: 164915
17/03/15 14:32:21 WARN DFSClient: Slow ReadProcessor read fields took 105558ms (threshold=30000ms); ack: seqno: 38 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
[Stage 42:>                                                        (0 + 4) / 10][Stage 42:=====>                                                   (1 + 4) / 10][Stage 42:===========>                                             (2 + 4) / 10][Stage 42:======================>                                  (4 + 4) / 10][Stage 42:==================================>                      (6 + 4) / 10][Stage 42:=======================================>                 (7 + 3) / 10][Stage 42:=============================================>           (8 + 2) / 10]                                                                                17/03/15 14:32:25 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=39, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=164864
17/03/15 14:32:25 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=177409 lastFlushOffset=164915 createNewBlock=false
17/03/15 14:32:25 DEBUG DFSClient: Queued packet 39
17/03/15 14:32:25 DEBUG DFSClient: Waiting for ack for: 39
17/03/15 14:32:25 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 39 offsetInBlock: 164864 lastPacketInBlock: false lastByteOffsetInBlock: 177409
17/03/15 14:32:25 DEBUG DFSClient: DFSClient seqno: 39 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:32:25 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=40, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=177152
17/03/15 14:32:25 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=177409 lastFlushOffset=177409 createNewBlock=false
17/03/15 14:32:25 DEBUG DFSClient: Waiting for ack for: 39
17/03/15 14:32:25 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=41, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=177152
17/03/15 14:32:25 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=181338 lastFlushOffset=177409 createNewBlock=false
17/03/15 14:32:25 DEBUG DFSClient: Queued packet 41
17/03/15 14:32:25 DEBUG DFSClient: Waiting for ack for: 41
17/03/15 14:32:25 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 41 offsetInBlock: 177152 lastPacketInBlock: false lastByteOffsetInBlock: 181338
17/03/15 14:32:25 DEBUG DFSClient: DFSClient seqno: 41 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 51:>                                                        (0 + 4) / 10][Stage 51:======================>                                  (4 + 4) / 10][Stage 51:=============================================>           (8 + 2) / 10][Stage 51:===================================================>     (9 + 1) / 10]17/03/15 14:32:30 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=42, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=181248
17/03/15 14:32:30 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=185613 lastFlushOffset=181338 createNewBlock=false
17/03/15 14:32:30 DEBUG DFSClient: Queued packet 42
17/03/15 14:32:30 DEBUG DFSClient: Waiting for ack for: 42
17/03/15 14:32:30 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 42 offsetInBlock: 181248 lastPacketInBlock: false lastByteOffsetInBlock: 185613
17/03/15 14:32:30 DEBUG DFSClient: DFSClient seqno: 42 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 52:>                                                        (0 + 4) / 10][Stage 52:=====>                                                   (1 + 4) / 10][Stage 52:===========>                                             (2 + 4) / 10][Stage 52:======================>                                  (4 + 4) / 10]17/03/15 14:32:39 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:32:39 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:32:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:32:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #13
17/03/15 14:32:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #13
17/03/15 14:32:39 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 14:32:39 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:32:39 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
[Stage 52:============================>                            (5 + 4) / 10][Stage 52:==================================>                      (6 + 4) / 10]17/03/15 14:32:45 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=43, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=185344
[Stage 52:=======================================>                 (7 + 3) / 10][Stage 52:===================================================>     (9 + 1) / 10]                                                                                17/03/15 14:32:47 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=206815 lastFlushOffset=185613 createNewBlock=false
17/03/15 14:32:47 DEBUG DFSClient: Queued packet 43
17/03/15 14:32:47 DEBUG DFSClient: Waiting for ack for: 43
17/03/15 14:32:47 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 43 offsetInBlock: 185344 lastPacketInBlock: false lastByteOffsetInBlock: 206815
17/03/15 14:32:47 DEBUG DFSClient: DFSClient seqno: 43 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:32:47 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=44, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=206336
17/03/15 14:32:47 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=206815 lastFlushOffset=206815 createNewBlock=false
17/03/15 14:32:47 DEBUG DFSClient: Waiting for ack for: 43
17/03/15 14:32:47 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=45, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=206336
17/03/15 14:32:47 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=206815 lastFlushOffset=206815 createNewBlock=false
17/03/15 14:32:47 DEBUG DFSClient: Waiting for ack for: 43
17/03/15 14:32:47 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=46, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=206336
17/03/15 14:32:47 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=206815 lastFlushOffset=206815 createNewBlock=false
17/03/15 14:32:47 DEBUG DFSClient: Waiting for ack for: 43
17/03/15 14:32:47 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=47, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=206336
17/03/15 14:32:47 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=214793 lastFlushOffset=206815 createNewBlock=false
17/03/15 14:32:47 DEBUG DFSClient: Queued packet 47
17/03/15 14:32:47 DEBUG DFSClient: Waiting for ack for: 47
17/03/15 14:32:47 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 47 offsetInBlock: 206336 lastPacketInBlock: false lastByteOffsetInBlock: 214793
17/03/15 14:32:47 DEBUG DFSClient: DFSClient seqno: 47 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 62:>                                                        (0 + 4) / 10]17/03/15 14:32:49 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:32:49 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 62:=====>                                                   (1 + 4) / 10][Stage 62:===========>                                             (2 + 4) / 10][Stage 62:=================>                                       (3 + 4) / 10][Stage 62:======================>                                  (4 + 4) / 10][Stage 62:=======================================>                 (7 + 3) / 10]17/03/15 14:33:01 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=48, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=214528
17/03/15 14:33:01 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=224164 lastFlushOffset=214793 createNewBlock=false
17/03/15 14:33:01 DEBUG DFSClient: Queued packet 48
17/03/15 14:33:01 DEBUG DFSClient: Waiting for ack for: 48
17/03/15 14:33:01 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 48 offsetInBlock: 214528 lastPacketInBlock: false lastByteOffsetInBlock: 224164
17/03/15 14:33:01 DEBUG DFSClient: DFSClient seqno: 48 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 63:>                                                        (0 + 4) / 10][Stage 63:======================>                                  (4 + 4) / 10][Stage 63:============================>                            (5 + 4) / 10][Stage 63:==================================>                      (6 + 4) / 10][Stage 63:=======================================>                 (7 + 3) / 10][Stage 63:=============================================>           (8 + 2) / 10]                                                                                17/03/15 14:33:08 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=49, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=223744
17/03/15 14:33:08 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=236859 lastFlushOffset=224164 createNewBlock=false
17/03/15 14:33:08 DEBUG DFSClient: Queued packet 49
17/03/15 14:33:08 DEBUG DFSClient: Waiting for ack for: 49
17/03/15 14:33:08 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 49 offsetInBlock: 223744 lastPacketInBlock: false lastByteOffsetInBlock: 236859
17/03/15 14:33:08 DEBUG DFSClient: DFSClient seqno: 49 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:33:08 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=50, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=236544
17/03/15 14:33:08 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=236859 lastFlushOffset=236859 createNewBlock=false
17/03/15 14:33:08 DEBUG DFSClient: Waiting for ack for: 49
17/03/15 14:33:08 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=51, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=236544
17/03/15 14:33:08 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=240891 lastFlushOffset=236859 createNewBlock=false
17/03/15 14:33:08 DEBUG DFSClient: Queued packet 51
17/03/15 14:33:08 DEBUG DFSClient: Waiting for ack for: 51
17/03/15 14:33:08 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 51 offsetInBlock: 236544 lastPacketInBlock: false lastByteOffsetInBlock: 240891
17/03/15 14:33:08 DEBUG DFSClient: DFSClient seqno: 51 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 74:======================>                                  (4 + 4) / 10]17/03/15 14:33:09 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:33:09 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:33:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:33:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #14
17/03/15 14:33:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #14
17/03/15 14:33:09 DEBUG ProtobufRpcEngine: Call: renewLease took 8ms
17/03/15 14:33:09 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:33:09 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
[Stage 74:============================>                            (5 + 4) / 10][Stage 74:=============================================>           (8 + 2) / 10][Stage 74:===================================================>     (9 + 1) / 10]17/03/15 14:33:10 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=52, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=240640
17/03/15 14:33:10 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=249370 lastFlushOffset=240891 createNewBlock=false
17/03/15 14:33:10 DEBUG DFSClient: Queued packet 52
17/03/15 14:33:10 DEBUG DFSClient: Waiting for ack for: 52
17/03/15 14:33:10 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 52 offsetInBlock: 240640 lastPacketInBlock: false lastByteOffsetInBlock: 249370
17/03/15 14:33:10 DEBUG DFSClient: DFSClient seqno: 52 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 75:>                                                        (0 + 4) / 10][Stage 75:=====>                                                   (1 + 4) / 10][Stage 75:===========>                                             (2 + 4) / 10]17/03/15 14:33:19 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:33:19 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 75:=================>                                       (3 + 4) / 10][Stage 75:======================>                                  (4 + 4) / 10][Stage 75:============================>                            (5 + 4) / 10][Stage 75:==================================>                      (6 + 4) / 10][Stage 75:=======================================>                 (7 + 3) / 10]17/03/15 14:33:30 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=53, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=249344
[Stage 75:=============================================>           (8 + 2) / 10][Stage 75:===================================================>     (9 + 1) / 10]                                                                                17/03/15 14:33:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=265465 lastFlushOffset=249370 createNewBlock=false
17/03/15 14:33:32 DEBUG DFSClient: Queued packet 53
17/03/15 14:33:32 DEBUG DFSClient: Waiting for ack for: 53
17/03/15 14:33:32 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 53 offsetInBlock: 249344 lastPacketInBlock: false lastByteOffsetInBlock: 265465
17/03/15 14:33:32 DEBUG DFSClient: DFSClient seqno: 53 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:33:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=54, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=265216
17/03/15 14:33:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=265465 lastFlushOffset=265465 createNewBlock=false
17/03/15 14:33:32 DEBUG DFSClient: Waiting for ack for: 53
17/03/15 14:33:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=55, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=265216
17/03/15 14:33:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=265465 lastFlushOffset=265465 createNewBlock=false
17/03/15 14:33:32 DEBUG DFSClient: Waiting for ack for: 53
17/03/15 14:33:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=56, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=265216
17/03/15 14:33:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=265465 lastFlushOffset=265465 createNewBlock=false
17/03/15 14:33:32 DEBUG DFSClient: Waiting for ack for: 53
17/03/15 14:33:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=57, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=265216
17/03/15 14:33:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=276031 lastFlushOffset=265465 createNewBlock=false
17/03/15 14:33:32 DEBUG DFSClient: Queued packet 57
17/03/15 14:33:32 DEBUG DFSClient: Waiting for ack for: 57
17/03/15 14:33:32 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 57 offsetInBlock: 265216 lastPacketInBlock: false lastByteOffsetInBlock: 276031
17/03/15 14:33:32 DEBUG DFSClient: DFSClient seqno: 57 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 87:>                                                        (0 + 4) / 10]17/03/15 14:33:39 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:33:39 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:33:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:33:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #15
17/03/15 14:33:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #15
17/03/15 14:33:39 DEBUG ProtobufRpcEngine: Call: renewLease took 8ms
17/03/15 14:33:39 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:33:39 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
17/03/15 14:33:49 WARN TaskSetManager: Lost task 0.0 in stage 87.0 (TID 230, 172.21.15.173, executor 0): java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:331)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:364)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:364)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:356)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:322)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:322)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1303)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:362)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:369)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)

17/03/15 14:33:49 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:33:49 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 87:=====>                                                   (1 + 4) / 10]17/03/15 14:33:53 ERROR TaskSchedulerImpl: Lost executor 0 on 172.21.15.173: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 14:33:53 WARN TaskSetManager: Lost task 2.1 in stage 87.0 (TID 236, 172.21.15.173, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 14:33:53 WARN TaskSetManager: Lost task 5.0 in stage 87.0 (TID 235, 172.21.15.173, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 14:33:53 WARN TaskSetManager: Lost task 4.0 in stage 87.0 (TID 234, 172.21.15.173, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 14:33:53 WARN TaskSetManager: Lost task 1.0 in stage 87.0 (TID 231, 172.21.15.173, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 14:33:53 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=58, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=275968
17/03/15 14:33:53 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=281438 lastFlushOffset=276031 createNewBlock=false
17/03/15 14:33:53 DEBUG DFSClient: Queued packet 58
17/03/15 14:33:53 DEBUG DFSClient: Waiting for ack for: 58
17/03/15 14:33:53 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 58 offsetInBlock: 275968 lastPacketInBlock: false lastByteOffsetInBlock: 281438
17/03/15 14:33:53 DEBUG DFSClient: DFSClient seqno: 58 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:33:53 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=59, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=281088
17/03/15 14:33:53 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=281438 lastFlushOffset=281438 createNewBlock=false
17/03/15 14:33:53 DEBUG DFSClient: Waiting for ack for: 58
[Stage 87:=====>                                                  (1 + -1) / 10]17/03/15 14:33:58 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=60, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=281088
17/03/15 14:33:58 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=281438 lastFlushOffset=281438 createNewBlock=false
17/03/15 14:33:58 DEBUG DFSClient: Waiting for ack for: 58
[Stage 87:=====>                                                   (1 + 3) / 10]17/03/15 14:33:58 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=61, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=281088
17/03/15 14:33:58 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=281438 lastFlushOffset=281438 createNewBlock=false
17/03/15 14:33:58 DEBUG DFSClient: Waiting for ack for: 58
17/03/15 14:33:59 WARN TaskSetManager: Lost task 1.1 in stage 87.0 (TID 237, 172.21.15.173, executor 1): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=1, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/15 14:33:59 WARN TaskSetManager: Lost task 2.2 in stage 87.0 (TID 240, 172.21.15.173, executor 1): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=2, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/15 14:33:59 WARN TaskSetManager: Lost task 4.1 in stage 87.0 (TID 238, 172.21.15.173, executor 1): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=4, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/15 14:33:59 WARN TaskSetManager: Lost task 5.1 in stage 87.0 (TID 239, 172.21.15.173, executor 1): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=5, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/15 14:33:59 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=62, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=281088
17/03/15 14:33:59 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=286621 lastFlushOffset=281438 createNewBlock=false
17/03/15 14:33:59 DEBUG DFSClient: Queued packet 62
17/03/15 14:33:59 DEBUG DFSClient: Waiting for ack for: 62
17/03/15 14:33:59 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 62 offsetInBlock: 281088 lastPacketInBlock: false lastByteOffsetInBlock: 286621
17/03/15 14:33:59 DEBUG DFSClient: DFSClient seqno: 62 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:33:59 WARN TaskSetManager: Lost task 3.1 in stage 87.0 (TID 241, 172.21.15.173, executor 1): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=3, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/15 14:33:59 WARN TaskSetManager: Lost task 0.1 in stage 87.0 (TID 242, 172.21.15.173, executor 1): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=0, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
[Stage 76:>                                                        (0 + 4) / 10][Stage 76:=================>                                       (3 + 4) / 10][Stage 76:======================>                                  (4 + 4) / 10][Stage 76:============================>                            (5 + 4) / 10][Stage 76:==================================>                      (6 + 4) / 10][Stage 76:=======================================>                 (7 + 3) / 10][Stage 76:=============================================>           (8 + 2) / 10]17/03/15 14:34:09 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:34:09 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:34:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:34:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #16
17/03/15 14:34:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #16
17/03/15 14:34:09 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 14:34:09 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:34:09 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
[Stage 76:===================================================>     (9 + 1) / 10]17/03/15 14:34:11 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=63, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=286208
17/03/15 14:34:11 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=298878 lastFlushOffset=286621 createNewBlock=false
17/03/15 14:34:11 DEBUG DFSClient: Queued packet 63
17/03/15 14:34:11 DEBUG DFSClient: Waiting for ack for: 63
17/03/15 14:34:11 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 63 offsetInBlock: 286208 lastPacketInBlock: false lastByteOffsetInBlock: 298878
17/03/15 14:34:11 DEBUG DFSClient: DFSClient seqno: 63 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 77:>                                                        (0 + 4) / 10][Stage 77:======================>                                  (4 + 4) / 10]17/03/15 14:34:19 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:34:19 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 77:=============================================>           (8 + 2) / 10]17/03/15 14:34:23 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=64, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=298496
17/03/15 14:34:23 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=307494 lastFlushOffset=298878 createNewBlock=false
17/03/15 14:34:23 DEBUG DFSClient: Queued packet 64
17/03/15 14:34:23 DEBUG DFSClient: Waiting for ack for: 64
17/03/15 14:34:23 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 64 offsetInBlock: 298496 lastPacketInBlock: false lastByteOffsetInBlock: 307494
17/03/15 14:34:23 DEBUG DFSClient: DFSClient seqno: 64 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:34:23 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=65, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=307200
17/03/15 14:34:23 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=312529 lastFlushOffset=307494 createNewBlock=false
17/03/15 14:34:23 DEBUG DFSClient: Queued packet 65
17/03/15 14:34:23 DEBUG DFSClient: Waiting for ack for: 65
17/03/15 14:34:23 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 65 offsetInBlock: 307200 lastPacketInBlock: false lastByteOffsetInBlock: 312529
17/03/15 14:34:23 DEBUG DFSClient: DFSClient seqno: 65 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 79:======================>                                  (4 + 4) / 10][Stage 79:=============================================>           (8 + 2) / 10]17/03/15 14:34:24 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=66, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=312320
17/03/15 14:34:24 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=322635 lastFlushOffset=312529 createNewBlock=false
17/03/15 14:34:24 DEBUG DFSClient: Queued packet 66
17/03/15 14:34:24 DEBUG DFSClient: Waiting for ack for: 66
17/03/15 14:34:24 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 66 offsetInBlock: 312320 lastPacketInBlock: false lastByteOffsetInBlock: 322635
17/03/15 14:34:24 DEBUG DFSClient: DFSClient seqno: 66 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 80:=============================================>           (8 + 2) / 10]17/03/15 14:34:25 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=67, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=322560
17/03/15 14:34:25 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=328453 lastFlushOffset=322635 createNewBlock=false
17/03/15 14:34:25 DEBUG DFSClient: Queued packet 67
17/03/15 14:34:25 DEBUG DFSClient: Waiting for ack for: 67
17/03/15 14:34:25 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 67 offsetInBlock: 322560 lastPacketInBlock: false lastByteOffsetInBlock: 328453
17/03/15 14:34:25 DEBUG DFSClient: DFSClient seqno: 67 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 81:>                                                        (0 + 4) / 10][Stage 81:======================>                                  (4 + 4) / 10][Stage 81:=============================================>           (8 + 2) / 10]17/03/15 14:34:28 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=68, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=328192
17/03/15 14:34:28 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=338460 lastFlushOffset=328453 createNewBlock=false
17/03/15 14:34:28 DEBUG DFSClient: Queued packet 68
17/03/15 14:34:28 DEBUG DFSClient: Waiting for ack for: 68
17/03/15 14:34:28 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 68 offsetInBlock: 328192 lastPacketInBlock: false lastByteOffsetInBlock: 338460
17/03/15 14:34:28 DEBUG DFSClient: DFSClient seqno: 68 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 82:>                                                        (0 + 4) / 10][Stage 82:======================>                                  (4 + 4) / 10][Stage 82:=============================================>           (8 + 2) / 10]17/03/15 14:34:30 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=69, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=338432
17/03/15 14:34:30 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=344103 lastFlushOffset=338460 createNewBlock=false
17/03/15 14:34:30 DEBUG DFSClient: Queued packet 69
17/03/15 14:34:30 DEBUG DFSClient: Waiting for ack for: 69
17/03/15 14:34:30 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 69 offsetInBlock: 338432 lastPacketInBlock: false lastByteOffsetInBlock: 344103
17/03/15 14:34:30 DEBUG DFSClient: DFSClient seqno: 69 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 83:>                                                        (0 + 4) / 10]17/03/15 14:34:39 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:34:39 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:34:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:34:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #17
17/03/15 14:34:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #17
17/03/15 14:34:39 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 14:34:39 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:34:39 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
17/03/15 14:34:49 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:34:49 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/15 14:35:09 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:35:09 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:35:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:35:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #18
17/03/15 14:35:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #18
17/03/15 14:35:09 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/15 14:35:09 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:35:09 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
17/03/15 14:35:19 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:35:19 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 83:>                                                        (0 + 4) / 10]17/03/15 14:35:39 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:35:39 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:35:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:35:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #19
17/03/15 14:35:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #19
17/03/15 14:35:39 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/15 14:35:39 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:35:39 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
17/03/15 14:35:49 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:35:49 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 83:======================>                                  (4 + 4) / 10]17/03/15 14:36:09 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:36:09 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:36:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:36:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #20
17/03/15 14:36:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #20
17/03/15 14:36:09 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 14:36:09 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:36:09 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
17/03/15 14:36:19 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:36:19 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/15 14:36:39 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:36:39 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:36:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:36:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #21
17/03/15 14:36:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #21
17/03/15 14:36:39 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 14:36:39 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:36:39 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
17/03/15 14:36:49 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:36:49 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 83:======================>                                  (4 + 4) / 10]17/03/15 14:37:00 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: -1 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 0
17/03/15 14:37:00 DEBUG DFSClient: DFSClient seqno: -1 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 83:==================================>                      (6 + 4) / 10][Stage 83:=============================================>           (8 + 2) / 10][Stage 83:===================================================>     (9 + 1) / 10]17/03/15 14:37:08 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=70, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=344064
17/03/15 14:37:08 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=354722 lastFlushOffset=344103 createNewBlock=false
17/03/15 14:37:08 DEBUG DFSClient: Queued packet 70
17/03/15 14:37:08 DEBUG DFSClient: Waiting for ack for: 70
17/03/15 14:37:08 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 70 offsetInBlock: 344064 lastPacketInBlock: false lastByteOffsetInBlock: 354722
17/03/15 14:37:09 DEBUG DFSClient: DFSClient seqno: 70 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 84:>                                                        (0 + 4) / 10]17/03/15 14:37:09 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:37:09 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:37:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:37:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #22
17/03/15 14:37:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #22
17/03/15 14:37:09 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 14:37:09 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:37:09 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
[Stage 84:=====>                                                   (1 + 4) / 10][Stage 84:=================>                                       (3 + 4) / 10][Stage 84:======================>                                  (4 + 4) / 10][Stage 84:============================>                            (5 + 4) / 10][Stage 84:==================================>                      (6 + 4) / 10][Stage 84:=======================================>                 (7 + 3) / 10][Stage 84:=============================================>           (8 + 2) / 10]17/03/15 14:37:19 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:37:19 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 84:===================================================>     (9 + 1) / 10]17/03/15 14:37:21 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=71, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=354304
17/03/15 14:37:21 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=360514 lastFlushOffset=354722 createNewBlock=false
17/03/15 14:37:21 DEBUG DFSClient: Queued packet 71
17/03/15 14:37:21 DEBUG DFSClient: Waiting for ack for: 71
17/03/15 14:37:21 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 71 offsetInBlock: 354304 lastPacketInBlock: false lastByteOffsetInBlock: 360514
17/03/15 14:37:21 DEBUG DFSClient: DFSClient seqno: 71 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 85:>                                                        (0 + 4) / 10][Stage 85:=====>                                                   (1 + 4) / 10][Stage 85:===========>                                             (2 + 4) / 10][Stage 85:==================================>                      (6 + 4) / 10]17/03/15 14:37:32 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=72, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=360448
17/03/15 14:37:32 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=371120 lastFlushOffset=360514 createNewBlock=false
17/03/15 14:37:32 DEBUG DFSClient: Queued packet 72
17/03/15 14:37:32 DEBUG DFSClient: Waiting for ack for: 72
17/03/15 14:37:32 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 72 offsetInBlock: 360448 lastPacketInBlock: false lastByteOffsetInBlock: 371120
17/03/15 14:37:32 DEBUG DFSClient: DFSClient seqno: 72 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 86:>                                                        (0 + 4) / 10][Stage 86:=====>                                                   (1 + 4) / 10][Stage 86:=================>                                       (3 + 4) / 10][Stage 86:======================>                                  (4 + 4) / 10]17/03/15 14:37:39 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:37:39 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:37:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:37:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #23
17/03/15 14:37:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #23
17/03/15 14:37:39 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 14:37:39 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:37:39 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
[Stage 86:============================>                            (5 + 4) / 10][Stage 86:==================================>                      (6 + 4) / 10][Stage 86:=============================================>           (8 + 2) / 10][Stage 86:===================================================>     (9 + 1) / 10]17/03/15 14:37:47 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=73, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=370688
17/03/15 14:37:47 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=380912 lastFlushOffset=371120 createNewBlock=false
17/03/15 14:37:47 DEBUG DFSClient: Queued packet 73
17/03/15 14:37:47 DEBUG DFSClient: Waiting for ack for: 73
17/03/15 14:37:47 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 73 offsetInBlock: 370688 lastPacketInBlock: false lastByteOffsetInBlock: 380912
17/03/15 14:37:47 DEBUG DFSClient: DFSClient seqno: 73 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 87:>                                                        (0 + 4) / 10]17/03/15 14:37:49 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:37:49 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 87:======================>                                  (4 + 4) / 10]17/03/15 14:38:09 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:38:09 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:38:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:38:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #24
17/03/15 14:38:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #24
17/03/15 14:38:09 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/15 14:38:09 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:38:09 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
17/03/15 14:38:19 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:38:19 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/15 14:38:21 WARN TaskSetManager: Lost task 4.0 in stage 87.1 (TID 357, 172.21.15.173, executor 1): java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:331)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:364)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:364)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:356)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:322)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:322)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1303)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:362)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:369)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)

17/03/15 14:38:21 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=74, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=380416
17/03/15 14:38:22 WARN TaskSetManager: Lost task 7.0 in stage 87.1 (TID 360, 172.21.15.173, executor 1): java.io.FileNotFoundException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-f4881aa3-8d2a-4277-9e2a-b4e770a955a7/blockmgr-c5843f99-d16d-4236-913f-a61cec845e51/1f/temp_shuffle_52b2335a-974c-4b92-a4c7-9ebc36733a9a (没有那个文件或目录)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:229)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:152)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/03/15 14:38:23 WARN TaskSetManager: Lost task 6.1 in stage 87.1 (TID 363, 172.21.15.173, executor 1): org.apache.spark.SparkException: Block rdd_14_6 was not found even though it's read-locked
	at org.apache.spark.storage.BlockManager.handleLocalReadFailure(BlockManager.scala:436)
	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:478)
	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:630)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:688)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/03/15 14:38:24 ERROR TaskSchedulerImpl: Lost executor 1 on 172.21.15.173: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 14:38:24 WARN TaskSetManager: Lost task 7.1 in stage 87.1 (TID 364, 172.21.15.173, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 14:38:24 WARN TaskSetManager: Lost task 5.0 in stage 87.1 (TID 358, 172.21.15.173, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 14:38:24 WARN TaskSetManager: Lost task 8.0 in stage 87.1 (TID 361, 172.21.15.173, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 14:38:24 WARN TaskSetManager: Lost task 4.1 in stage 87.1 (TID 362, 172.21.15.173, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 14:38:24 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=402940 lastFlushOffset=380912 createNewBlock=false
17/03/15 14:38:24 DEBUG DFSClient: Queued packet 74
17/03/15 14:38:24 DEBUG DFSClient: Waiting for ack for: 74
17/03/15 14:38:24 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 74 offsetInBlock: 380416 lastPacketInBlock: false lastByteOffsetInBlock: 402940
17/03/15 14:38:24 WARN DFSClient: Slow ReadProcessor read fields took 37299ms (threshold=30000ms); ack: seqno: 74 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[172.21.15.173:50010,DS-bcd3842f-7acc-473a-9a24-360950418375,DISK]]
17/03/15 14:38:24 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=75, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=402432
17/03/15 14:38:24 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=402940 lastFlushOffset=402940 createNewBlock=false
17/03/15 14:38:24 DEBUG DFSClient: Waiting for ack for: 74
[Stage 87:======================>                                 (4 + -4) / 10]17/03/15 14:38:29 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=76, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=402432
17/03/15 14:38:29 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=409886 lastFlushOffset=402940 createNewBlock=false
17/03/15 14:38:29 DEBUG DFSClient: Queued packet 76
17/03/15 14:38:29 DEBUG DFSClient: Waiting for ack for: 76
17/03/15 14:38:29 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 76 offsetInBlock: 402432 lastPacketInBlock: false lastByteOffsetInBlock: 409886
17/03/15 14:38:29 DEBUG DFSClient: DFSClient seqno: 76 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:38:29 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=77, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=409600
17/03/15 14:38:29 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=409886 lastFlushOffset=409886 createNewBlock=false
17/03/15 14:38:29 DEBUG DFSClient: Waiting for ack for: 76
[Stage 87:======================>                                  (4 + 0) / 10]17/03/15 14:38:30 WARN TaskSetManager: Lost task 8.1 in stage 87.1 (TID 366, 172.21.15.173, executor 2): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=8, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/15 14:38:30 WARN TaskSetManager: Lost task 5.1 in stage 87.1 (TID 367, 172.21.15.173, executor 2): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=5, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/15 14:38:30 WARN TaskSetManager: Lost task 7.2 in stage 87.1 (TID 368, 172.21.15.173, executor 2): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=7, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/15 14:38:30 WARN TaskSetManager: Lost task 4.2 in stage 87.1 (TID 365, 172.21.15.173, executor 2): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=4, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/15 14:38:30 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=78, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=409600
17/03/15 14:38:30 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=414709 lastFlushOffset=409886 createNewBlock=false
17/03/15 14:38:30 DEBUG DFSClient: Queued packet 78
17/03/15 14:38:30 DEBUG DFSClient: Waiting for ack for: 78
17/03/15 14:38:30 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 78 offsetInBlock: 409600 lastPacketInBlock: false lastByteOffsetInBlock: 414709
17/03/15 14:38:30 DEBUG DFSClient: DFSClient seqno: 78 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:38:30 WARN TaskSetManager: Lost task 3.1 in stage 87.1 (TID 369, 172.21.15.173, executor 2): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=3, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
[Stage 76:>                                                        (0 + 4) / 10][Stage 76:=================>                                       (3 + 4) / 10][Stage 76:============================>                            (5 + 4) / 10]17/03/15 14:38:39 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:38:39 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:38:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:38:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #25
17/03/15 14:38:39 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #25
17/03/15 14:38:39 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 14:38:39 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:38:39 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
[Stage 76:==================================>                      (6 + 4) / 10][Stage 76:=======================================>                 (7 + 3) / 10][Stage 76:=============================================>           (8 + 2) / 10][Stage 76:===================================================>     (9 + 1) / 10]17/03/15 14:38:42 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=79, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=414208
17/03/15 14:38:42 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=421954 lastFlushOffset=414709 createNewBlock=false
17/03/15 14:38:42 DEBUG DFSClient: Queued packet 79
17/03/15 14:38:42 DEBUG DFSClient: Waiting for ack for: 79
17/03/15 14:38:42 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 79 offsetInBlock: 414208 lastPacketInBlock: false lastByteOffsetInBlock: 421954
17/03/15 14:38:42 DEBUG DFSClient: DFSClient seqno: 79 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 77:>                                                        (0 + 4) / 10][Stage 77:======================>                                  (4 + 4) / 10]17/03/15 14:38:49 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:38:49 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 77:==================================>                      (6 + 4) / 10][Stage 77:=============================================>           (8 + 2) / 10]17/03/15 14:38:54 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=80, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=421888
17/03/15 14:38:54 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=434898 lastFlushOffset=421954 createNewBlock=false
17/03/15 14:38:54 DEBUG DFSClient: Queued packet 80
17/03/15 14:38:54 DEBUG DFSClient: Waiting for ack for: 80
17/03/15 14:38:54 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 80 offsetInBlock: 421888 lastPacketInBlock: false lastByteOffsetInBlock: 434898
17/03/15 14:38:54 DEBUG DFSClient: DFSClient seqno: 80 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:38:54 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=81, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=434688
17/03/15 14:38:54 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=439968 lastFlushOffset=434898 createNewBlock=false
17/03/15 14:38:54 DEBUG DFSClient: Queued packet 81
17/03/15 14:38:54 DEBUG DFSClient: Waiting for ack for: 81
17/03/15 14:38:54 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 81 offsetInBlock: 434688 lastPacketInBlock: false lastByteOffsetInBlock: 439968
17/03/15 14:38:54 DEBUG DFSClient: DFSClient seqno: 81 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 79:>                                                        (0 + 4) / 10][Stage 79:======================>                                  (4 + 4) / 10][Stage 79:=============================================>           (8 + 2) / 10]17/03/15 14:38:55 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=82, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=439808
17/03/15 14:38:55 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=445462 lastFlushOffset=439968 createNewBlock=false
17/03/15 14:38:55 DEBUG DFSClient: Queued packet 82
17/03/15 14:38:55 DEBUG DFSClient: Waiting for ack for: 82
17/03/15 14:38:55 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 82 offsetInBlock: 439808 lastPacketInBlock: false lastByteOffsetInBlock: 445462
17/03/15 14:38:55 DEBUG DFSClient: DFSClient seqno: 82 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 80:=============================================>           (8 + 2) / 10]17/03/15 14:38:56 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=83, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=445440
17/03/15 14:38:56 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=455133 lastFlushOffset=445462 createNewBlock=false
17/03/15 14:38:56 DEBUG DFSClient: Queued packet 83
17/03/15 14:38:56 DEBUG DFSClient: Waiting for ack for: 83
17/03/15 14:38:56 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 83 offsetInBlock: 445440 lastPacketInBlock: false lastByteOffsetInBlock: 455133
17/03/15 14:38:56 DEBUG DFSClient: DFSClient seqno: 83 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 81:>                                                        (0 + 4) / 10][Stage 81:======================>                                  (4 + 4) / 10][Stage 81:=============================================>           (8 + 2) / 10]17/03/15 14:38:59 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=84, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=454656
17/03/15 14:38:59 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=460882 lastFlushOffset=455133 createNewBlock=false
17/03/15 14:38:59 DEBUG DFSClient: Queued packet 84
17/03/15 14:38:59 DEBUG DFSClient: Waiting for ack for: 84
17/03/15 14:38:59 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 84 offsetInBlock: 454656 lastPacketInBlock: false lastByteOffsetInBlock: 460882
17/03/15 14:38:59 DEBUG DFSClient: DFSClient seqno: 84 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 82:>                                                        (0 + 4) / 10][Stage 82:======================>                                  (4 + 4) / 10][Stage 82:=============================================>           (8 + 2) / 10][Stage 82:===================================================>     (9 + 1) / 10]17/03/15 14:39:01 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=85, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=460800
17/03/15 14:39:01 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=471586 lastFlushOffset=460882 createNewBlock=false
17/03/15 14:39:01 DEBUG DFSClient: Queued packet 85
17/03/15 14:39:01 DEBUG DFSClient: Waiting for ack for: 85
17/03/15 14:39:01 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 85 offsetInBlock: 460800 lastPacketInBlock: false lastByteOffsetInBlock: 471586
17/03/15 14:39:01 DEBUG DFSClient: DFSClient seqno: 85 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 83:>                                                        (0 + 4) / 10]17/03/15 14:39:09 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:39:09 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:39:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:39:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #26
17/03/15 14:39:09 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #26
17/03/15 14:39:09 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 14:39:09 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:39:09 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
17/03/15 14:39:19 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:39:19 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/15 14:39:40 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:39:40 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:39:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:39:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #27
17/03/15 14:39:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #27
17/03/15 14:39:40 DEBUG ProtobufRpcEngine: Call: renewLease took 9ms
17/03/15 14:39:40 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:39:40 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
17/03/15 14:39:50 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:39:50 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 83:>                                                        (0 + 4) / 10]17/03/15 14:40:10 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:40:10 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:40:10 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:40:10 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #28
17/03/15 14:40:10 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #28
17/03/15 14:40:10 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 14:40:10 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:40:10 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
17/03/15 14:40:20 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:40:20 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 83:======================>                                  (4 + 4) / 10]17/03/15 14:40:40 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:40:40 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:40:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:40:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #29
17/03/15 14:40:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #29
17/03/15 14:40:40 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 14:40:40 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:40:40 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
17/03/15 14:40:50 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:40:50 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/15 14:41:10 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:41:10 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:41:10 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:41:10 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #30
17/03/15 14:41:10 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #30
17/03/15 14:41:10 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 14:41:10 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:41:10 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
17/03/15 14:41:20 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:41:20 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 83:======================>                                  (4 + 4) / 10]17/03/15 14:41:31 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: -1 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 0
17/03/15 14:41:31 DEBUG DFSClient: DFSClient seqno: -1 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 83:==================================>                      (6 + 4) / 10][Stage 83:=============================================>           (8 + 2) / 10]17/03/15 14:41:37 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=86, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=471552
17/03/15 14:41:37 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=477153 lastFlushOffset=471586 createNewBlock=false
17/03/15 14:41:37 DEBUG DFSClient: Queued packet 86
17/03/15 14:41:37 DEBUG DFSClient: Waiting for ack for: 86
17/03/15 14:41:37 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 86 offsetInBlock: 471552 lastPacketInBlock: false lastByteOffsetInBlock: 477153
17/03/15 14:41:37 DEBUG DFSClient: DFSClient seqno: 86 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 84:>                                                        (0 + 4) / 10]17/03/15 14:41:40 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:41:40 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:41:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:41:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #31
17/03/15 14:41:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #31
17/03/15 14:41:40 DEBUG ProtobufRpcEngine: Call: renewLease took 6ms
17/03/15 14:41:40 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:41:40 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
[Stage 84:======================>                                  (4 + 4) / 10][Stage 84:============================>                            (5 + 4) / 10][Stage 84:=======================================>                 (7 + 3) / 10][Stage 84:=============================================>           (8 + 2) / 10][Stage 84:===================================================>     (9 + 1) / 10]17/03/15 14:41:50 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:41:50 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/15 14:41:50 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=87, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=476672
17/03/15 14:41:50 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=487731 lastFlushOffset=477153 createNewBlock=false
17/03/15 14:41:50 DEBUG DFSClient: Queued packet 87
17/03/15 14:41:50 DEBUG DFSClient: Waiting for ack for: 87
17/03/15 14:41:50 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 87 offsetInBlock: 476672 lastPacketInBlock: false lastByteOffsetInBlock: 487731
17/03/15 14:41:50 DEBUG DFSClient: DFSClient seqno: 87 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 85:>                                                        (0 + 4) / 10][Stage 85:===========>                                             (2 + 4) / 10][Stage 85:======================>                                  (4 + 4) / 10][Stage 85:============================>                            (5 + 4) / 10][Stage 85:==================================>                      (6 + 4) / 10][Stage 85:=======================================>                 (7 + 3) / 10][Stage 85:=============================================>           (8 + 2) / 10][Stage 85:===================================================>     (9 + 1) / 10]17/03/15 14:42:00 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=88, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=487424
17/03/15 14:42:00 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=493495 lastFlushOffset=487731 createNewBlock=false
17/03/15 14:42:00 DEBUG DFSClient: Queued packet 88
17/03/15 14:42:00 DEBUG DFSClient: Waiting for ack for: 88
17/03/15 14:42:00 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 88 offsetInBlock: 487424 lastPacketInBlock: false lastByteOffsetInBlock: 493495
17/03/15 14:42:00 DEBUG DFSClient: DFSClient seqno: 88 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 86:>                                                        (0 + 4) / 10][Stage 86:=====>                                                   (1 + 4) / 10][Stage 86:===========>                                             (2 + 4) / 10][Stage 86:======================>                                  (4 + 4) / 10]17/03/15 14:42:10 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:42:10 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:42:10 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:42:10 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #32
17/03/15 14:42:10 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #32
17/03/15 14:42:10 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 14:42:10 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:42:10 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
[Stage 86:============================>                            (5 + 4) / 10][Stage 86:==================================>                      (6 + 4) / 10][Stage 86:=======================================>                 (7 + 3) / 10][Stage 86:=============================================>           (8 + 2) / 10][Stage 86:===================================================>     (9 + 1) / 10]17/03/15 14:42:16 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=89, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=493056
17/03/15 14:42:16 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=508733 lastFlushOffset=493495 createNewBlock=false
17/03/15 14:42:16 DEBUG DFSClient: Queued packet 89
17/03/15 14:42:16 DEBUG DFSClient: Waiting for ack for: 89
17/03/15 14:42:16 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 89 offsetInBlock: 493056 lastPacketInBlock: false lastByteOffsetInBlock: 508733
17/03/15 14:42:16 DEBUG DFSClient: DFSClient seqno: 89 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 87:>                                                        (0 + 4) / 10]17/03/15 14:42:20 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:42:20 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/15 14:42:29 WARN TaskSetManager: Lost task 1.0 in stage 87.2 (TID 481, 172.21.15.173, executor 2): java.lang.OutOfMemoryError: Java heap space
	at com.esotericsoftware.kryo.io.Input.readDoubles(Input.java:885)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$DoubleArraySerializer.read(DefaultArraySerializers.java:222)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$DoubleArraySerializer.read(DefaultArraySerializers.java:205)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)
	at com.twitter.chill.Tuple4Serializer.read(TupleSerializers.scala:69)
	at com.twitter.chill.Tuple4Serializer.read(TupleSerializers.scala:59)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:396)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:307)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:244)
	at org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:159)
	at org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:189)
	at org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:186)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)

17/03/15 14:42:30 WARN TaskSetManager: Lost task 4.0 in stage 87.2 (TID 484, 172.21.15.173, executor 2): FetchFailed(BlockManagerId(2, 172.21.15.173, 45243, None), shuffleId=2, mapId=3, reduceId=4, message=
org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-f4881aa3-8d2a-4277-9e2a-b4e770a955a7/blockmgr-df8265d8-47c4-4e4e-a3ec-551fa1c783be/27/shuffle_2_3_0.data, offset=109040, length=27260}
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:356)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-f4881aa3-8d2a-4277-9e2a-b4e770a955a7/blockmgr-df8265d8-47c4-4e4e-a3ec-551fa1c783be/27/shuffle_2_3_0.data, offset=109040, length=27260}
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:113)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:349)
	... 39 more
Caused by: java.io.FileNotFoundException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-f4881aa3-8d2a-4277-9e2a-b4e770a955a7/blockmgr-df8265d8-47c4-4e4e-a3ec-551fa1c783be/27/shuffle_2_3_0.data (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:98)
	... 40 more

)
17/03/15 14:42:30 WARN TaskSetManager: Lost task 1.1 in stage 87.2 (TID 485, 172.21.15.173, executor 2): FetchFailed(BlockManagerId(2, 172.21.15.173, 45243, None), shuffleId=2, mapId=1, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-f4881aa3-8d2a-4277-9e2a-b4e770a955a7/blockmgr-df8265d8-47c4-4e4e-a3ec-551fa1c783be/29/shuffle_2_1_0.data, offset=27259, length=27259}
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:356)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-f4881aa3-8d2a-4277-9e2a-b4e770a955a7/blockmgr-df8265d8-47c4-4e4e-a3ec-551fa1c783be/29/shuffle_2_1_0.data, offset=27259, length=27259}
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:113)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:349)
	... 39 more
Caused by: java.io.FileNotFoundException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-f4881aa3-8d2a-4277-9e2a-b4e770a955a7/blockmgr-df8265d8-47c4-4e4e-a3ec-551fa1c783be/29/shuffle_2_1_0.data (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:98)
	... 40 more

)
17/03/15 14:42:30 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=90, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=508416
17/03/15 14:42:30 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=514848 lastFlushOffset=508733 createNewBlock=false
17/03/15 14:42:30 DEBUG DFSClient: Queued packet 90
17/03/15 14:42:30 DEBUG DFSClient: Waiting for ack for: 90
17/03/15 14:42:30 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 90 offsetInBlock: 508416 lastPacketInBlock: false lastByteOffsetInBlock: 514848
17/03/15 14:42:30 DEBUG DFSClient: DFSClient seqno: 90 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:42:30 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=91, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=514560
17/03/15 14:42:30 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=514848 lastFlushOffset=514848 createNewBlock=false
17/03/15 14:42:30 DEBUG DFSClient: Waiting for ack for: 90
17/03/15 14:42:30 WARN TaskSetManager: Lost task 2.1 in stage 87.2 (TID 486, 172.21.15.173, executor 2): FetchFailed(BlockManagerId(2, 172.21.15.173, 45243, None), shuffleId=2, mapId=1, reduceId=2, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-f4881aa3-8d2a-4277-9e2a-b4e770a955a7/blockmgr-df8265d8-47c4-4e4e-a3ec-551fa1c783be/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-f4881aa3-8d2a-4277-9e2a-b4e770a955a7/blockmgr-df8265d8-47c4-4e4e-a3ec-551fa1c783be/0d/shuffle_2_1_0.index (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:199)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:302)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:269)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:303)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:132)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	... 23 more

)
17/03/15 14:42:30 WARN TaskSetManager: Lost task 0.0 in stage 87.2 (TID 480, 172.21.15.173, executor 2): FetchFailed(BlockManagerId(2, 172.21.15.173, 45243, None), shuffleId=9, mapId=8, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-f4881aa3-8d2a-4277-9e2a-b4e770a955a7/blockmgr-df8265d8-47c4-4e4e-a3ec-551fa1c783be/25/shuffle_9_8_0.data, offset=0, length=7441677}
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:356)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-f4881aa3-8d2a-4277-9e2a-b4e770a955a7/blockmgr-df8265d8-47c4-4e4e-a3ec-551fa1c783be/25/shuffle_9_8_0.data, offset=0, length=7441677}
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:113)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:349)
	... 35 more
Caused by: java.io.FileNotFoundException: /tmp/spark-ffe82f9c-2b62-467f-8b23-0cca1c3108e3/executor-f4881aa3-8d2a-4277-9e2a-b4e770a955a7/blockmgr-df8265d8-47c4-4e4e-a3ec-551fa1c783be/25/shuffle_9_8_0.data (没有那个文件或目录)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:98)
	... 36 more

)
17/03/15 14:42:30 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=92, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=514560
17/03/15 14:42:30 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=519877 lastFlushOffset=514848 createNewBlock=false
17/03/15 14:42:30 DEBUG DFSClient: Queued packet 92
17/03/15 14:42:30 DEBUG DFSClient: Waiting for ack for: 92
17/03/15 14:42:30 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 92 offsetInBlock: 514560 lastPacketInBlock: false lastByteOffsetInBlock: 519877
17/03/15 14:42:30 DEBUG DFSClient: DFSClient seqno: 92 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:42:30 WARN TransportChannelHandler: Exception in connection from /172.21.15.173:38402
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:221)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:899)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:275)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)
17/03/15 14:42:30 ERROR TaskSchedulerImpl: Lost executor 2 on 172.21.15.173: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 14:42:30 WARN TaskSetManager: Lost task 3.0 in stage 87.2 (TID 483, 172.21.15.173, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 14:42:30 WARN TaskSetManager: Lost task 0.0 in stage 76.2 (TID 487, 172.21.15.173, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 14:42:30 WARN TaskSetManager: Lost task 2.0 in stage 76.2 (TID 489, 172.21.15.173, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 14:42:30 WARN TaskSetManager: Lost task 1.0 in stage 76.2 (TID 488, 172.21.15.173, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 14:42:30 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=93, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=519680
17/03/15 14:42:30 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=519877 lastFlushOffset=519877 createNewBlock=false
17/03/15 14:42:30 DEBUG DFSClient: Waiting for ack for: 92
17/03/15 14:42:30 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=94, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=519680
17/03/15 14:42:30 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=519877 lastFlushOffset=519877 createNewBlock=false
17/03/15 14:42:30 DEBUG DFSClient: Waiting for ack for: 92
[Stage 76:>                                                        (0 + 0) / 10]17/03/15 14:42:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=95, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=519680
17/03/15 14:42:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=519877 lastFlushOffset=519877 createNewBlock=false
17/03/15 14:42:35 DEBUG DFSClient: Waiting for ack for: 92
[Stage 76:>                                                        (0 + 4) / 10]17/03/15 14:42:35 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=96, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=519680
17/03/15 14:42:35 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=519877 lastFlushOffset=519877 createNewBlock=false
17/03/15 14:42:35 DEBUG DFSClient: Waiting for ack for: 92
17/03/15 14:42:40 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:42:40 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:42:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:42:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #33
17/03/15 14:42:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #33
17/03/15 14:42:40 DEBUG ProtobufRpcEngine: Call: renewLease took 8ms
17/03/15 14:42:40 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:42:40 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
[Stage 76:=================>                                       (3 + 4) / 10][Stage 76:============================>                            (5 + 4) / 10][Stage 76:=======================================>                 (7 + 3) / 10][Stage 76:=============================================>           (8 + 2) / 10][Stage 76:===================================================>     (9 + 1) / 10]17/03/15 14:42:47 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=97, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=519680
17/03/15 14:42:47 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=530024 lastFlushOffset=519877 createNewBlock=false
17/03/15 14:42:47 DEBUG DFSClient: Queued packet 97
17/03/15 14:42:47 DEBUG DFSClient: Waiting for ack for: 97
17/03/15 14:42:47 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 97 offsetInBlock: 519680 lastPacketInBlock: false lastByteOffsetInBlock: 530024
17/03/15 14:42:47 DEBUG DFSClient: DFSClient seqno: 97 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 77:>                                                        (0 + 0) / 10][Stage 77:>                                                        (0 + 4) / 10]17/03/15 14:42:50 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:42:50 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 77:======================>                                  (4 + 4) / 10][Stage 77:=============================================>           (8 + 2) / 10]17/03/15 14:42:58 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=98, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=529920
17/03/15 14:42:58 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=538634 lastFlushOffset=530024 createNewBlock=false
17/03/15 14:42:58 DEBUG DFSClient: Queued packet 98
17/03/15 14:42:58 DEBUG DFSClient: Waiting for ack for: 98
17/03/15 14:42:58 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 98 offsetInBlock: 529920 lastPacketInBlock: false lastByteOffsetInBlock: 538634
17/03/15 14:42:58 DEBUG DFSClient: DFSClient seqno: 98 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:42:59 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=99, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=538624
17/03/15 14:42:59 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=543817 lastFlushOffset=538634 createNewBlock=false
17/03/15 14:42:59 DEBUG DFSClient: Queued packet 99
17/03/15 14:42:59 DEBUG DFSClient: Waiting for ack for: 99
17/03/15 14:42:59 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 99 offsetInBlock: 538624 lastPacketInBlock: false lastByteOffsetInBlock: 543817
17/03/15 14:42:59 DEBUG DFSClient: DFSClient seqno: 99 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 79:>                                                        (0 + 4) / 10][Stage 79:======================>                                  (4 + 4) / 10][Stage 79:=============================================>           (8 + 2) / 10]17/03/15 14:43:00 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=100, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=543744
17/03/15 14:43:00 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=553408 lastFlushOffset=543817 createNewBlock=false
17/03/15 14:43:00 DEBUG DFSClient: Queued packet 100
17/03/15 14:43:00 DEBUG DFSClient: Waiting for ack for: 100
17/03/15 14:43:00 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 100 offsetInBlock: 543744 lastPacketInBlock: false lastByteOffsetInBlock: 553408
17/03/15 14:43:00 DEBUG DFSClient: DFSClient seqno: 100 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 80:======================>                                  (4 + 4) / 10][Stage 80:=============================================>           (8 + 2) / 10]17/03/15 14:43:01 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=101, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=552960
17/03/15 14:43:01 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=558920 lastFlushOffset=553408 createNewBlock=false
17/03/15 14:43:01 DEBUG DFSClient: Queued packet 101
17/03/15 14:43:01 DEBUG DFSClient: Waiting for ack for: 101
17/03/15 14:43:01 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 101 offsetInBlock: 552960 lastPacketInBlock: false lastByteOffsetInBlock: 558920
17/03/15 14:43:01 DEBUG DFSClient: DFSClient seqno: 101 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 81:>                                                        (0 + 4) / 10][Stage 81:======================>                                  (4 + 4) / 10][Stage 81:=============================================>           (8 + 2) / 10]17/03/15 14:43:04 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=102, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=558592
17/03/15 14:43:04 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=569861 lastFlushOffset=558920 createNewBlock=false
17/03/15 14:43:04 DEBUG DFSClient: Queued packet 102
17/03/15 14:43:04 DEBUG DFSClient: Waiting for ack for: 102
17/03/15 14:43:04 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 102 offsetInBlock: 558592 lastPacketInBlock: false lastByteOffsetInBlock: 569861
17/03/15 14:43:04 DEBUG DFSClient: DFSClient seqno: 102 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 82:>                                                        (0 + 4) / 10][Stage 82:=================>                                       (3 + 4) / 10][Stage 82:======================>                                  (4 + 4) / 10][Stage 82:=============================================>           (8 + 2) / 10]17/03/15 14:43:06 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=103, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=569856
17/03/15 14:43:06 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=575533 lastFlushOffset=569861 createNewBlock=false
17/03/15 14:43:06 DEBUG DFSClient: Queued packet 103
17/03/15 14:43:06 DEBUG DFSClient: Waiting for ack for: 103
17/03/15 14:43:06 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 103 offsetInBlock: 569856 lastPacketInBlock: false lastByteOffsetInBlock: 575533
17/03/15 14:43:06 DEBUG DFSClient: DFSClient seqno: 103 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 83:>                                                        (0 + 4) / 10]17/03/15 14:43:10 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:43:10 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:43:10 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:43:10 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #34
17/03/15 14:43:10 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #34
17/03/15 14:43:10 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 14:43:10 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:43:10 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
17/03/15 14:43:20 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:43:20 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/15 14:43:40 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:43:40 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:43:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:43:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #35
17/03/15 14:43:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #35
17/03/15 14:43:40 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 14:43:40 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:43:40 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
17/03/15 14:43:50 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:43:50 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 83:>                                                        (0 + 4) / 10]17/03/15 14:44:10 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:44:10 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:44:10 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:44:10 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #36
17/03/15 14:44:10 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #36
17/03/15 14:44:10 DEBUG ProtobufRpcEngine: Call: renewLease took 5ms
17/03/15 14:44:10 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:44:10 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
17/03/15 14:44:20 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:44:20 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 83:======================>                                  (4 + 4) / 10]17/03/15 14:44:40 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:44:40 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:44:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:44:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #37
17/03/15 14:44:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #37
17/03/15 14:44:40 DEBUG ProtobufRpcEngine: Call: renewLease took 5ms
17/03/15 14:44:40 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:44:40 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
17/03/15 14:44:50 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:44:50 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
17/03/15 14:45:10 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:45:10 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:45:10 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:45:10 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #38
17/03/15 14:45:10 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #38
17/03/15 14:45:10 DEBUG ProtobufRpcEngine: Call: renewLease took 5ms
17/03/15 14:45:10 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:45:10 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
17/03/15 14:45:20 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:45:20 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 83:======================>                                  (4 + 4) / 10]17/03/15 14:45:36 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: -1 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 0
17/03/15 14:45:36 DEBUG DFSClient: DFSClient seqno: -1 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:45:40 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:45:40 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:45:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:45:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #39
17/03/15 14:45:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #39
17/03/15 14:45:40 DEBUG ProtobufRpcEngine: Call: renewLease took 8ms
17/03/15 14:45:40 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:45:40 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
[Stage 83:=============================================>           (8 + 2) / 10]17/03/15 14:45:44 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=104, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=575488
17/03/15 14:45:44 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=586420 lastFlushOffset=575533 createNewBlock=false
17/03/15 14:45:44 DEBUG DFSClient: Queued packet 104
17/03/15 14:45:44 DEBUG DFSClient: Waiting for ack for: 104
17/03/15 14:45:44 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 104 offsetInBlock: 575488 lastPacketInBlock: false lastByteOffsetInBlock: 586420
17/03/15 14:45:44 DEBUG DFSClient: DFSClient seqno: 104 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 84:>                                                        (0 + 4) / 10][Stage 84:=================>                                       (3 + 4) / 10][Stage 84:======================>                                  (4 + 4) / 10]17/03/15 14:45:50 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:45:50 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 84:============================>                            (5 + 4) / 10][Stage 84:==================================>                      (6 + 4) / 10][Stage 84:===================================================>     (9 + 1) / 10]17/03/15 14:45:59 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=105, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=586240
17/03/15 14:45:59 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=592121 lastFlushOffset=586420 createNewBlock=false
17/03/15 14:45:59 DEBUG DFSClient: Queued packet 105
17/03/15 14:45:59 DEBUG DFSClient: Waiting for ack for: 105
17/03/15 14:45:59 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 105 offsetInBlock: 586240 lastPacketInBlock: false lastByteOffsetInBlock: 592121
17/03/15 14:46:00 DEBUG DFSClient: DFSClient seqno: 105 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 85:>                                                        (0 + 4) / 10][Stage 85:=================>                                       (3 + 4) / 10][Stage 85:======================>                                  (4 + 4) / 10][Stage 85:============================>                            (5 + 4) / 10][Stage 85:==================================>                      (6 + 4) / 10][Stage 85:=======================================>                 (7 + 3) / 10][Stage 85:=============================================>           (8 + 2) / 10][Stage 85:===================================================>     (9 + 1) / 10]17/03/15 14:46:09 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=106, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=591872
17/03/15 14:46:09 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=602705 lastFlushOffset=592121 createNewBlock=false
17/03/15 14:46:09 DEBUG DFSClient: Queued packet 106
17/03/15 14:46:09 DEBUG DFSClient: Waiting for ack for: 106
17/03/15 14:46:09 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 106 offsetInBlock: 591872 lastPacketInBlock: false lastByteOffsetInBlock: 602705
17/03/15 14:46:09 DEBUG DFSClient: DFSClient seqno: 106 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:46:10 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:46:10 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:46:10 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:46:10 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #40
17/03/15 14:46:10 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #40
17/03/15 14:46:10 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 14:46:10 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:46:10 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
[Stage 86:>                                                        (0 + 4) / 10][Stage 86:=====>                                                   (1 + 4) / 10][Stage 86:===========>                                             (2 + 4) / 10][Stage 86:=================>                                       (3 + 4) / 10][Stage 86:======================>                                  (4 + 4) / 10]17/03/15 14:46:20 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:46:20 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
[Stage 86:============================>                            (5 + 4) / 10][Stage 86:==================================>                      (6 + 4) / 10][Stage 86:=============================================>           (8 + 2) / 10][Stage 86:===================================================>     (9 + 1) / 10]17/03/15 14:46:29 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=107, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=602624
17/03/15 14:46:29 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=612654 lastFlushOffset=602705 createNewBlock=false
17/03/15 14:46:29 DEBUG DFSClient: Queued packet 107
17/03/15 14:46:29 DEBUG DFSClient: Waiting for ack for: 107
17/03/15 14:46:29 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 107 offsetInBlock: 602624 lastPacketInBlock: false lastByteOffsetInBlock: 612654
17/03/15 14:46:29 DEBUG DFSClient: DFSClient seqno: 107 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
[Stage 87:>                                                        (0 + 4) / 10]17/03/15 14:46:40 DEBUG Client: The ping interval is 60000 ms.
17/03/15 14:46:40 DEBUG Client: Connecting to spark1/172.21.15.90:9000
17/03/15 14:46:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: starting, having connections 1
17/03/15 14:46:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #41
17/03/15 14:46:40 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #41
17/03/15 14:46:40 DEBUG ProtobufRpcEngine: Call: renewLease took 7ms
17/03/15 14:46:40 DEBUG LeaseRenewer: Lease renewed for client DFSClient_NONMAPREDUCE_-2066807090_1
17/03/15 14:46:40 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-2066807090_1] with renew id 1 executed
17/03/15 14:46:40 WARN TaskSetManager: Lost task 0.0 in stage 87.3 (TID 600, 172.21.15.173, executor 3): java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:331)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:364)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:364)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:356)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:322)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:322)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1303)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:362)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:369)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:89)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:115)
	at org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$4$$anonfun$apply$5.apply(ReplicatedVertexView.scala:113)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)

17/03/15 14:46:41 ERROR TaskSchedulerImpl: Lost executor 3 on 172.21.15.173: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 14:46:41 WARN TaskSetManager: Lost task 3.0 in stage 87.3 (TID 603, 172.21.15.173, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 14:46:41 WARN TaskSetManager: Lost task 2.0 in stage 87.3 (TID 602, 172.21.15.173, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 14:46:41 WARN TaskSetManager: Lost task 0.1 in stage 87.3 (TID 605, 172.21.15.173, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 14:46:41 WARN TaskSetManager: Lost task 4.0 in stage 87.3 (TID 604, 172.21.15.173, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/03/15 14:46:41 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=108, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=612352
17/03/15 14:46:41 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=618407 lastFlushOffset=612654 createNewBlock=false
17/03/15 14:46:41 DEBUG DFSClient: Queued packet 108
17/03/15 14:46:41 DEBUG DFSClient: Waiting for ack for: 108
17/03/15 14:46:41 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 108 offsetInBlock: 612352 lastPacketInBlock: false lastByteOffsetInBlock: 618407
[Stage 87:>                                                        (0 + 0) / 10]17/03/15 14:46:41 DEBUG DFSClient: DFSClient seqno: 108 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:46:41 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=109, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=617984
17/03/15 14:46:41 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=618407 lastFlushOffset=618407 createNewBlock=false
17/03/15 14:46:41 DEBUG DFSClient: Waiting for ack for: 108
17/03/15 14:46:46 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=110, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=617984
17/03/15 14:46:46 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=618407 lastFlushOffset=618407 createNewBlock=false
17/03/15 14:46:46 DEBUG DFSClient: Waiting for ack for: 108
[Stage 87:>                                                        (0 + 4) / 10]17/03/15 14:46:46 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=111, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=617984
17/03/15 14:46:46 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=618407 lastFlushOffset=618407 createNewBlock=false
17/03/15 14:46:46 DEBUG DFSClient: Waiting for ack for: 108
17/03/15 14:46:47 WARN TaskSetManager: Lost task 4.1 in stage 87.3 (TID 606, 172.21.15.173, executor 4): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=4, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/15 14:46:47 WARN TaskSetManager: Lost task 2.1 in stage 87.3 (TID 608, 172.21.15.173, executor 4): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=2, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/15 14:46:47 WARN TaskSetManager: Lost task 0.2 in stage 87.3 (TID 607, 172.21.15.173, executor 4): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=0, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/15 14:46:47 WARN TaskSetManager: Lost task 3.1 in stage 87.3 (TID 609, 172.21.15.173, executor 4): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=3, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/15 14:46:47 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=112, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=617984
17/03/15 14:46:47 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=624049 lastFlushOffset=618407 createNewBlock=false
17/03/15 14:46:47 DEBUG DFSClient: Queued packet 112
17/03/15 14:46:47 DEBUG DFSClient: Waiting for ack for: 112
17/03/15 14:46:47 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 112 offsetInBlock: 617984 lastPacketInBlock: false lastByteOffsetInBlock: 624049
17/03/15 14:46:47 DEBUG DFSClient: DFSClient seqno: 112 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 87 (mapPartitions at VertexRDD.scala:356) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697) 	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693) 	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338) 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285) 	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338) 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338) 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285) 	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54) 	at org.apache.spark.scheduler.Task.run(Task.scala:114) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1456)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1444)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1443)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1443)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1273)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1668)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1626)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1615)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2016)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2037)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2056)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2081)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1159)
	at src.main.scala.SVDPlusPlusApp$.main(SVDPlusPlusApp.scala:105)
	at src.main.scala.SVDPlusPlusApp.main(SVDPlusPlusApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:728)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
17/03/15 14:46:47 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=113, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=623616
17/03/15 14:46:47 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=624049 lastFlushOffset=624049 createNewBlock=false
17/03/15 14:46:47 DEBUG DFSClient: Waiting for ack for: 112
17/03/15 14:46:47 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=114, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=623616
17/03/15 14:46:47 DEBUG DFSClient: DFSClient flush(): bytesCurBlock=628938 lastFlushOffset=624049 createNewBlock=false
17/03/15 14:46:47 DEBUG DFSClient: Queued packet 114
17/03/15 14:46:47 DEBUG DFSClient: Waiting for ack for: 114
17/03/15 14:46:47 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 114 offsetInBlock: 623616 lastPacketInBlock: false lastByteOffsetInBlock: 628938
17/03/15 14:46:47 DEBUG DFSClient: DFSClient seqno: 114 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:46:47 DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=115, src=/eventLogs/app-20170315142939-0020.lz4.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=628736
17/03/15 14:46:47 DEBUG DFSClient: Queued packet 115
17/03/15 14:46:47 DEBUG DFSClient: Queued packet 116
17/03/15 14:46:47 DEBUG DFSClient: Waiting for ack for: 116
17/03/15 14:46:47 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 115 offsetInBlock: 628736 lastPacketInBlock: false lastByteOffsetInBlock: 630490
17/03/15 14:46:47 DEBUG DFSClient: DFSClient seqno: 115 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:46:47 DEBUG DFSClient: DataStreamer block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261 sending packet packet seqno: 116 offsetInBlock: 630490 lastPacketInBlock: true lastByteOffsetInBlock: 630490
17/03/15 14:46:47 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(4, 172.21.15.173, 34150, None),rdd_14_1,StorageLevel(1 replicas),0,0))
17/03/15 14:46:47 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(4, 172.21.15.173, 34150, None),rdd_23_1,StorageLevel(1 replicas),0,0))
17/03/15 14:46:47 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(4, 172.21.15.173, 34150, None),rdd_90_1,StorageLevel(1 replicas),0,0))
17/03/15 14:46:47 WARN TaskSetManager: Lost task 1.1 in stage 87.3 (TID 610, 172.21.15.173, executor 4): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=1, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

)
17/03/15 14:46:47 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(87,3,ShuffleMapTask,FetchFailed(null,0,-1,1,org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:974)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:958)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:889)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:949)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:695)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:336)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.scheduler.Task.run(Task.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
),org.apache.spark.scheduler.TaskInfo@4dd5450a,null)
17/03/15 14:46:47 DEBUG DFSClient: DFSClient seqno: 116 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
17/03/15 14:46:47 DEBUG DFSClient: Closing old block BP-519507147-172.21.15.90-1479901973323:blk_1073809084_68261
17/03/15 14:46:47 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #42
17/03/15 14:46:47 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #42
17/03/15 14:46:47 DEBUG ProtobufRpcEngine: Call: complete took 12ms
17/03/15 14:46:47 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #43
17/03/15 14:46:47 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #43
17/03/15 14:46:47 DEBUG ProtobufRpcEngine: Call: getFileInfo took 3ms
17/03/15 14:46:47 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #44
17/03/15 14:46:47 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #44
17/03/15 14:46:47 DEBUG ProtobufRpcEngine: Call: rename took 13ms
17/03/15 14:46:47 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop sending #45
17/03/15 14:46:47 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop got value #45
17/03/15 14:46:47 DEBUG ProtobufRpcEngine: Call: setTimes took 3ms
17/03/15 14:46:47 DEBUG PoolThreadCache: Freed 19 thread-local buffer(s) from thread: shuffle-server-6-3
17/03/15 14:46:47 DEBUG PoolThreadCache: Freed 17 thread-local buffer(s) from thread: shuffle-server-6-4
17/03/15 14:46:47 DEBUG Client: stopping client from cache: org.apache.hadoop.ipc.Client@1ca04be
17/03/15 14:46:47 DEBUG Client: removing client from cache: org.apache.hadoop.ipc.Client@1ca04be
17/03/15 14:46:47 DEBUG Client: stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@1ca04be
17/03/15 14:46:47 DEBUG Client: Stopping client
17/03/15 14:46:47 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: closed
17/03/15 14:46:47 DEBUG Client: IPC Client (1707157467) connection to spark1/172.21.15.90:9000 from hadoop: stopped, remaining connections 0
